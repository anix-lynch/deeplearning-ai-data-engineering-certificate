{
 "cells": [
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Data Transformations with Apache Spark\n",
    "\n",
    "In this lab, you will perform transformations on the `classicmodels` database with Apache Spark. You will first practice the basics and then use `PySpark` to create a star schema model similar to the one done in the Week 1 assignment."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Table of Contents\n",
    "\n",
    "- [ 1 - Introduction](#1)\n",
    "- [ 2 - Environment Setup](#2)\n",
    "- [ 3 - Apache Spark 101](#3)\n",
    "  - [ 3.1 - Spark Classes](#3.1)\n",
    "  - [ 3.2 - Spark DataFrame](#3.2)\n",
    "  - [ 3.3 - Spark SQL](#3.3)\n",
    "  - [ 3.4 - UDFs and Data Types](#3.4)\n",
    "- [ 4 - Data Modeling with Spark](#4)\n",
    "  - [ 4.1 - Read the Tables](#4.1)\n",
    "  - [ 4.2 - Star Schema](#4.2)\n",
    "  - [ 4.3 - Customers Dimension](#4.3)\n",
    "  - [ 4.4 - Products Dimension](#4.4)\n",
    "  - [ 4.5 - Offices Dimension](#4.5)\n",
    "  - [ 4.6 - Employees Dimension](#4.6)\n",
    "  - [ 4.7 - Date Dimension](#4.7)\n",
    "  - [ 4.8 - Fact Table](#4.8)\n",
    "- [ 5 - Upload Files for Grading](#5)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "<a name='1'></a>\n",
    "## 1 - Introduction\n",
    "\n",
    "Apache Spark is an open-source unified analytics engine for large-scale data processing, it allows you to perform Data Engineering, Data Science, and Machine Learning jobs on single-node machines or clusters. In courses 1 and 3, you have seen some examples with AWS Glue jobs, a serverless service that allows you to run Spark jobs without setting up cloud resources. In this assignment, you are provided with a Spark cluster deployed using Amazon EMR. This service comes with a Studio and Workspace functions allowing you to run Spark jobs from this notebook directly.\n",
    "\n",
    "You will recreate the Star Schema data model from the Week 1 assignment using PySpark, the Python API for Spark."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "<a name='2'></a>\n",
    "## 2 - Environment Setup\n",
    "\n",
    "The `classicmodels` database is stored in an RDS instance running a Postgres engine, you will need to configure the connection to read the source data and then store the generated data models. Thankfully, the Studio functionality of Amazon EMR provides you with the necessary classes ready to use, but you will need to add a configuration to allow the environment to connect to a Postgres database. "
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "2.1. Run the following cell, this will point the Spark cluster to a JAR file with the necessary code to connect:"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 87,
   "metadata": {
    "execution": {
     "iopub.execute_input": "2025-11-29T18:43:36.762422Z",
     "iopub.status.busy": "2025-11-29T18:43:36.762184Z",
     "iopub.status.idle": "2025-11-29T18:44:24.449975Z",
     "shell.execute_reply": "2025-11-29T18:44:24.449227Z",
     "shell.execute_reply.started": "2025-11-29T18:43:36.762386Z"
    }
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Starting Spark application\n"
     ]
    },
    {
     "data": {
      "text/html": [
       "<table>\n",
       "<tbody><tr><th>ID</th><th>YARN Application ID</th><th>Kind</th><th>State</th><th>Spark UI</th><th>Driver log</th><th>User</th><th>Current session?</th></tr><tr><td>2</td><td>application_1764439453312_0003</td><td>pyspark</td><td>idle</td><td><a target=\"_blank\" href=\"http://ip-10-0-1-158.ec2.internal:20888/proxy/application_1764439453312_0003/\" class=\"emr-proxy-link j-197867KPE1ZQF application_1764439453312_0003\" emr-resource=\"j-197867KPE1ZQF\n",
       "\" application-id=\"application_1764439453312_0003\">Link</a></td><td><a target=\"_blank\" href=\"http://ip-10-0-1-33.ec2.internal:8042/node/containerlogs/container_1764439453312_0003_01_000001/livy\">Link</a></td><td>None</td><td>\u2714</td></tr></tbody></table>"
      ],
      "text/plain": [
       "<IPython.core.display.HTML object>"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "data": {
      "application/vnd.jupyter.widget-view+json": {
       "model_id": "",
       "version_major": 2,
       "version_minor": 0
      },
      "text/plain": [
       "FloatProgress(value=0.0, bar_style='info', description='Progress:', layout=Layout(height='25px', width='50%'),\u2026"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "SparkSession available as 'spark'.\n"
     ]
    },
    {
     "data": {
      "text/html": [
       "Current session configs: <tt>{'conf': {'spark.pyspark.python': 'python', 'spark.pyspark.virtualenv.enabled': 'true', 'spark.pyspark.virtualenv.type': 'native', 'spark.pyspark.virtualenv.bin.path': '/usr/bin/virtualenv', 'spark.jars.packages': 'org.postgresql:postgresql:42.2.5'}, 'proxyUser': 'assumed-role_voclabs_user4628236_iolivmkzjxod', 'kind': 'pyspark'}</tt><br>"
      ],
      "text/plain": [
       "<IPython.core.display.HTML object>"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "data": {
      "text/html": [
       "<table>\n",
       "<tbody><tr><th>ID</th><th>YARN Application ID</th><th>Kind</th><th>State</th><th>Spark UI</th><th>Driver log</th><th>User</th><th>Current session?</th></tr><tr><td>2</td><td>application_1764439453312_0003</td><td>pyspark</td><td>idle</td><td><a target=\"_blank\" href=\"http://ip-10-0-1-158.ec2.internal:20888/proxy/application_1764439453312_0003/\" class=\"emr-proxy-link j-197867KPE1ZQF application_1764439453312_0003\" emr-resource=\"j-197867KPE1ZQF\n",
       "\" application-id=\"application_1764439453312_0003\">Link</a></td><td><a target=\"_blank\" href=\"http://ip-10-0-1-33.ec2.internal:8042/node/containerlogs/container_1764439453312_0003_01_000001/livy\">Link</a></td><td>None</td><td>\u2714</td></tr></tbody></table>"
      ],
      "text/plain": [
       "<IPython.core.display.HTML object>"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    }
   ],
   "source": [
    "%%configure -f\n",
    "{\n",
    "    \"conf\": {\n",
    "        \"spark.pyspark.python\": \"python\",\n",
    "        \"spark.pyspark.virtualenv.enabled\": \"true\",\n",
    "        \"spark.pyspark.virtualenv.type\":\"native\",\n",
    "        \"spark.pyspark.virtualenv.bin.path\":\"/usr/bin/virtualenv\",\n",
    "        \"spark.jars.packages\": \"org.postgresql:postgresql:42.2.5\"\n",
    "    }\n",
    "}"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "2.2. Go to **CloudFormation** in the AWS console. You will see the stack with an alphanumeric ID. Click on it and search for the **Outputs** tab. You will see the key `PostgresEndpoint`, copy the corresponding **Value** (highlight and copy it as text, not as a link). Replace the placeholder `<RDS-ENDPOINT>` in the following cell and run it."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 88,
   "metadata": {
    "execution": {
     "iopub.execute_input": "2025-11-29T18:44:24.451404Z",
     "iopub.status.busy": "2025-11-29T18:44:24.451154Z",
     "iopub.status.idle": "2025-11-29T18:44:24.495378Z",
     "shell.execute_reply": "2025-11-29T18:44:24.494750Z",
     "shell.execute_reply.started": "2025-11-29T18:44:24.451368Z"
    }
   },
   "outputs": [
    {
     "data": {
      "application/vnd.jupyter.widget-view+json": {
       "model_id": "37fd46b0aabb4e10b55011049e6d968a",
       "version_major": 2,
       "version_minor": 0
      },
      "text/plain": [
       "VBox()"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "data": {
      "application/vnd.jupyter.widget-view+json": {
       "model_id": "",
       "version_major": 2,
       "version_minor": 0
      },
      "text/plain": [
       "FloatProgress(value=0.0, bar_style='info', description='Progress:', layout=Layout(height='25px', width='50%'),\u2026"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    }
   ],
   "source": [
    "RDS_ENDPOINT = \"de-c4w3a1-rds.cpgqia4kez4u.us-east-1.rds.amazonaws.com\"\n",
    "jdbc_url = f\"jdbc:postgresql://{RDS_ENDPOINT}:5432/postgres\"  # For PostgreSQL\n",
    "\n",
    "jdbc_properties = {\n",
    "    \"user\": \"postgresuser\",\n",
    "    \"password\": \"adminpwrd\",    \n",
    "    \"driver\": \"org.postgresql.Driver\"  # For PostgreSQL\n",
    "}"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "After running the previous cell you should wait while the Spark application starts. After it finishes, you should see a message like this:\n",
    "```bash\n",
    "SparkSession available as 'spark'\n",
    "```"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "2.3. Now use the `spark` object that is available to read from the database using Java Database Connectivity (JDBC), you will provide the JDBC url and the connection properties, and point to a table with their corresponding schema. In this case, you will call `information_schema.tables` to get the available tables; then you will select the schema and names."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 89,
   "metadata": {
    "execution": {
     "iopub.execute_input": "2025-11-29T18:44:24.497019Z",
     "iopub.status.busy": "2025-11-29T18:44:24.496544Z",
     "iopub.status.idle": "2025-11-29T18:44:25.238161Z",
     "shell.execute_reply": "2025-11-29T18:44:25.237519Z",
     "shell.execute_reply.started": "2025-11-29T18:44:24.496983Z"
    }
   },
   "outputs": [
    {
     "data": {
      "application/vnd.jupyter.widget-view+json": {
       "model_id": "",
       "version_major": 2,
       "version_minor": 0
      },
      "text/plain": [
       "FloatProgress(value=0.0, bar_style='info', description='Progress:', layout=Layout(height='25px', width='50%'),\u2026"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "An error was encountered:\n",
      "An error occurred while calling o98.jdbc.\n",
      ": org.postgresql.util.PSQLException: The connection attempt failed.\n",
      "\tat org.postgresql.core.v3.ConnectionFactoryImpl.openConnectionImpl(ConnectionFactoryImpl.java:292)\n",
      "\tat org.postgresql.core.ConnectionFactory.openConnection(ConnectionFactory.java:49)\n",
      "\tat org.postgresql.jdbc.PgConnection.<init>(PgConnection.java:195)\n",
      "\tat org.postgresql.Driver.makeConnection(Driver.java:454)\n",
      "\tat org.postgresql.Driver.connect(Driver.java:256)\n",
      "\tat org.apache.spark.sql.execution.datasources.jdbc.connection.BasicConnectionProvider.getConnection(BasicConnectionProvider.scala:49)\n",
      "\tat org.apache.spark.sql.execution.datasources.jdbc.connection.ConnectionProviderBase.create(ConnectionProvider.scala:102)\n",
      "\tat org.apache.spark.sql.jdbc.JdbcDialect.$anonfun$createConnectionFactory$1(JdbcDialects.scala:160)\n",
      "\tat org.apache.spark.sql.jdbc.JdbcDialect.$anonfun$createConnectionFactory$1$adapted(JdbcDialects.scala:156)\n",
      "\tat org.apache.spark.sql.execution.datasources.jdbc.JDBCRDD$.getQueryOutputSchema(JDBCRDD.scala:63)\n",
      "\tat org.apache.spark.sql.execution.datasources.jdbc.JDBCRDD$.resolveTable(JDBCRDD.scala:58)\n",
      "\tat org.apache.spark.sql.execution.datasources.jdbc.JDBCRelation$.getSchema(JDBCRelation.scala:241)\n",
      "\tat org.apache.spark.sql.execution.datasources.jdbc.JdbcRelationProvider.createRelation(JdbcRelationProvider.scala:37)\n",
      "\tat org.apache.spark.sql.execution.datasources.DataSource.resolveRelation(DataSource.scala:346)\n",
      "\tat org.apache.spark.sql.DataFrameReader.loadV1Source(DataFrameReader.scala:229)\n",
      "\tat org.apache.spark.sql.DataFrameReader.$anonfun$load$2(DataFrameReader.scala:211)\n",
      "\tat scala.Option.getOrElse(Option.scala:189)\n",
      "\tat org.apache.spark.sql.DataFrameReader.load(DataFrameReader.scala:211)\n",
      "\tat org.apache.spark.sql.DataFrameReader.load(DataFrameReader.scala:172)\n",
      "\tat org.apache.spark.sql.DataFrameReader.jdbc(DataFrameReader.scala:249)\n",
      "\tat java.base/jdk.internal.reflect.NativeMethodAccessorImpl.invoke0(Native Method)\n",
      "\tat java.base/jdk.internal.reflect.NativeMethodAccessorImpl.invoke(NativeMethodAccessorImpl.java:77)\n",
      "\tat java.base/jdk.internal.reflect.DelegatingMethodAccessorImpl.invoke(DelegatingMethodAccessorImpl.java:43)\n",
      "\tat java.base/java.lang.reflect.Method.invoke(Method.java:569)\n",
      "\tat py4j.reflection.MethodInvoker.invoke(MethodInvoker.java:244)\n",
      "\tat py4j.reflection.ReflectionEngine.invoke(ReflectionEngine.java:374)\n",
      "\tat py4j.Gateway.invoke(Gateway.java:282)\n",
      "\tat py4j.commands.AbstractCommand.invokeMethod(AbstractCommand.java:132)\n",
      "\tat py4j.commands.CallCommand.execute(CallCommand.java:79)\n",
      "\tat py4j.GatewayConnection.run(GatewayConnection.java:238)\n",
      "\tat java.base/java.lang.Thread.run(Thread.java:840)\n",
      "Caused by: java.net.UnknownHostException: <RDS-ENDPOINT>\n",
      "\tat java.base/sun.nio.ch.NioSocketImpl.connect(NioSocketImpl.java:572)\n",
      "\tat java.base/java.net.SocksSocketImpl.connect(SocksSocketImpl.java:327)\n",
      "\tat java.base/java.net.Socket.connect(Socket.java:633)\n",
      "\tat org.postgresql.core.PGStream.<init>(PGStream.java:70)\n",
      "\tat org.postgresql.core.v3.ConnectionFactoryImpl.tryConnect(ConnectionFactoryImpl.java:91)\n",
      "\tat org.postgresql.core.v3.ConnectionFactoryImpl.openConnectionImpl(ConnectionFactoryImpl.java:192)\n",
      "\t... 30 more\n",
      "\n",
      "Traceback (most recent call last):\n",
      "  File \"/mnt1/yarn/usercache/livy/appcache/application_1764439453312_0003/container_1764439453312_0003_01_000001/pyspark.zip/pyspark/sql/readwriter.py\", line 946, in jdbc\n",
      "    return self._df(self._jreader.jdbc(url, table, jprop))\n",
      "  File \"/mnt1/yarn/usercache/livy/appcache/application_1764439453312_0003/container_1764439453312_0003_01_000001/py4j-0.10.9.7-src.zip/py4j/java_gateway.py\", line 1322, in __call__\n",
      "    return_value = get_return_value(\n",
      "  File \"/mnt1/yarn/usercache/livy/appcache/application_1764439453312_0003/container_1764439453312_0003_01_000001/pyspark.zip/pyspark/errors/exceptions/captured.py\", line 179, in deco\n",
      "    return f(*a, **kw)\n",
      "  File \"/mnt1/yarn/usercache/livy/appcache/application_1764439453312_0003/container_1764439453312_0003_01_000001/py4j-0.10.9.7-src.zip/py4j/protocol.py\", line 326, in get_return_value\n",
      "    raise Py4JJavaError(\n",
      "py4j.protocol.Py4JJavaError: An error occurred while calling o98.jdbc.\n",
      ": org.postgresql.util.PSQLException: The connection attempt failed.\n",
      "\tat org.postgresql.core.v3.ConnectionFactoryImpl.openConnectionImpl(ConnectionFactoryImpl.java:292)\n",
      "\tat org.postgresql.core.ConnectionFactory.openConnection(ConnectionFactory.java:49)\n",
      "\tat org.postgresql.jdbc.PgConnection.<init>(PgConnection.java:195)\n",
      "\tat org.postgresql.Driver.makeConnection(Driver.java:454)\n",
      "\tat org.postgresql.Driver.connect(Driver.java:256)\n",
      "\tat org.apache.spark.sql.execution.datasources.jdbc.connection.BasicConnectionProvider.getConnection(BasicConnectionProvider.scala:49)\n",
      "\tat org.apache.spark.sql.execution.datasources.jdbc.connection.ConnectionProviderBase.create(ConnectionProvider.scala:102)\n",
      "\tat org.apache.spark.sql.jdbc.JdbcDialect.$anonfun$createConnectionFactory$1(JdbcDialects.scala:160)\n",
      "\tat org.apache.spark.sql.jdbc.JdbcDialect.$anonfun$createConnectionFactory$1$adapted(JdbcDialects.scala:156)\n",
      "\tat org.apache.spark.sql.execution.datasources.jdbc.JDBCRDD$.getQueryOutputSchema(JDBCRDD.scala:63)\n",
      "\tat org.apache.spark.sql.execution.datasources.jdbc.JDBCRDD$.resolveTable(JDBCRDD.scala:58)\n",
      "\tat org.apache.spark.sql.execution.datasources.jdbc.JDBCRelation$.getSchema(JDBCRelation.scala:241)\n",
      "\tat org.apache.spark.sql.execution.datasources.jdbc.JdbcRelationProvider.createRelation(JdbcRelationProvider.scala:37)\n",
      "\tat org.apache.spark.sql.execution.datasources.DataSource.resolveRelation(DataSource.scala:346)\n",
      "\tat org.apache.spark.sql.DataFrameReader.loadV1Source(DataFrameReader.scala:229)\n",
      "\tat org.apache.spark.sql.DataFrameReader.$anonfun$load$2(DataFrameReader.scala:211)\n",
      "\tat scala.Option.getOrElse(Option.scala:189)\n",
      "\tat org.apache.spark.sql.DataFrameReader.load(DataFrameReader.scala:211)\n",
      "\tat org.apache.spark.sql.DataFrameReader.load(DataFrameReader.scala:172)\n",
      "\tat org.apache.spark.sql.DataFrameReader.jdbc(DataFrameReader.scala:249)\n",
      "\tat java.base/jdk.internal.reflect.NativeMethodAccessorImpl.invoke0(Native Method)\n",
      "\tat java.base/jdk.internal.reflect.NativeMethodAccessorImpl.invoke(NativeMethodAccessorImpl.java:77)\n",
      "\tat java.base/jdk.internal.reflect.DelegatingMethodAccessorImpl.invoke(DelegatingMethodAccessorImpl.java:43)\n",
      "\tat java.base/java.lang.reflect.Method.invoke(Method.java:569)\n",
      "\tat py4j.reflection.MethodInvoker.invoke(MethodInvoker.java:244)\n",
      "\tat py4j.reflection.ReflectionEngine.invoke(ReflectionEngine.java:374)\n",
      "\tat py4j.Gateway.invoke(Gateway.java:282)\n",
      "\tat py4j.commands.AbstractCommand.invokeMethod(AbstractCommand.java:132)\n",
      "\tat py4j.commands.CallCommand.execute(CallCommand.java:79)\n",
      "\tat py4j.GatewayConnection.run(GatewayConnection.java:238)\n",
      "\tat java.base/java.lang.Thread.run(Thread.java:840)\n",
      "Caused by: java.net.UnknownHostException: <RDS-ENDPOINT>\n",
      "\tat java.base/sun.nio.ch.NioSocketImpl.connect(NioSocketImpl.java:572)\n",
      "\tat java.base/java.net.SocksSocketImpl.connect(SocksSocketImpl.java:327)\n",
      "\tat java.base/java.net.Socket.connect(Socket.java:633)\n",
      "\tat org.postgresql.core.PGStream.<init>(PGStream.java:70)\n",
      "\tat org.postgresql.core.v3.ConnectionFactoryImpl.tryConnect(ConnectionFactoryImpl.java:91)\n",
      "\tat org.postgresql.core.v3.ConnectionFactoryImpl.openConnectionImpl(ConnectionFactoryImpl.java:192)\n",
      "\t... 30 more\n",
      "\n",
      "\n"
     ]
    }
   ],
   "source": [
    "%%pyspark\n",
    "information_tables_df = spark.read.jdbc(jdbc_url, \"information_schema.tables\", properties=jdbc_properties)\n",
    "information_tables_df.select([\"table_schema\", \"table_name\"]).show()"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "2.4. Now that you listed the available schemas, call the `information_schema.schemata` table. Select the `schema_name` and `schema_owner`, and then show the resulting DataFrame."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 90,
   "metadata": {
    "execution": {
     "iopub.execute_input": "2025-11-29T18:44:25.239264Z",
     "iopub.status.busy": "2025-11-29T18:44:25.239092Z",
     "iopub.status.idle": "2025-11-29T18:44:25.495570Z",
     "shell.execute_reply": "2025-11-29T18:44:25.494861Z",
     "shell.execute_reply.started": "2025-11-29T18:44:25.239239Z"
    }
   },
   "outputs": [
    {
     "data": {
      "application/vnd.jupyter.widget-view+json": {
       "model_id": "04716a94ae3443308bc0e5499a6a0aa7",
       "version_major": 2,
       "version_minor": 0
      },
      "text/plain": [
       "VBox()"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "data": {
      "application/vnd.jupyter.widget-view+json": {
       "model_id": "",
       "version_major": 2,
       "version_minor": 0
      },
      "text/plain": [
       "FloatProgress(value=0.0, bar_style='info', description='Progress:', layout=Layout(height='25px', width='50%'),\u2026"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "An error was encountered:\n",
      "An error occurred while calling o102.jdbc.\n",
      ": org.postgresql.util.PSQLException: The connection attempt failed.\n",
      "\tat org.postgresql.core.v3.ConnectionFactoryImpl.openConnectionImpl(ConnectionFactoryImpl.java:292)\n",
      "\tat org.postgresql.core.ConnectionFactory.openConnection(ConnectionFactory.java:49)\n",
      "\tat org.postgresql.jdbc.PgConnection.<init>(PgConnection.java:195)\n",
      "\tat org.postgresql.Driver.makeConnection(Driver.java:454)\n",
      "\tat org.postgresql.Driver.connect(Driver.java:256)\n",
      "\tat org.apache.spark.sql.execution.datasources.jdbc.connection.BasicConnectionProvider.getConnection(BasicConnectionProvider.scala:49)\n",
      "\tat org.apache.spark.sql.execution.datasources.jdbc.connection.ConnectionProviderBase.create(ConnectionProvider.scala:102)\n",
      "\tat org.apache.spark.sql.jdbc.JdbcDialect.$anonfun$createConnectionFactory$1(JdbcDialects.scala:160)\n",
      "\tat org.apache.spark.sql.jdbc.JdbcDialect.$anonfun$createConnectionFactory$1$adapted(JdbcDialects.scala:156)\n",
      "\tat org.apache.spark.sql.execution.datasources.jdbc.JDBCRDD$.getQueryOutputSchema(JDBCRDD.scala:63)\n",
      "\tat org.apache.spark.sql.execution.datasources.jdbc.JDBCRDD$.resolveTable(JDBCRDD.scala:58)\n",
      "\tat org.apache.spark.sql.execution.datasources.jdbc.JDBCRelation$.getSchema(JDBCRelation.scala:241)\n",
      "\tat org.apache.spark.sql.execution.datasources.jdbc.JdbcRelationProvider.createRelation(JdbcRelationProvider.scala:37)\n",
      "\tat org.apache.spark.sql.execution.datasources.DataSource.resolveRelation(DataSource.scala:346)\n",
      "\tat org.apache.spark.sql.DataFrameReader.loadV1Source(DataFrameReader.scala:229)\n",
      "\tat org.apache.spark.sql.DataFrameReader.$anonfun$load$2(DataFrameReader.scala:211)\n",
      "\tat scala.Option.getOrElse(Option.scala:189)\n",
      "\tat org.apache.spark.sql.DataFrameReader.load(DataFrameReader.scala:211)\n",
      "\tat org.apache.spark.sql.DataFrameReader.load(DataFrameReader.scala:172)\n",
      "\tat org.apache.spark.sql.DataFrameReader.jdbc(DataFrameReader.scala:249)\n",
      "\tat java.base/jdk.internal.reflect.NativeMethodAccessorImpl.invoke0(Native Method)\n",
      "\tat java.base/jdk.internal.reflect.NativeMethodAccessorImpl.invoke(NativeMethodAccessorImpl.java:77)\n",
      "\tat java.base/jdk.internal.reflect.DelegatingMethodAccessorImpl.invoke(DelegatingMethodAccessorImpl.java:43)\n",
      "\tat java.base/java.lang.reflect.Method.invoke(Method.java:569)\n",
      "\tat py4j.reflection.MethodInvoker.invoke(MethodInvoker.java:244)\n",
      "\tat py4j.reflection.ReflectionEngine.invoke(ReflectionEngine.java:374)\n",
      "\tat py4j.Gateway.invoke(Gateway.java:282)\n",
      "\tat py4j.commands.AbstractCommand.invokeMethod(AbstractCommand.java:132)\n",
      "\tat py4j.commands.CallCommand.execute(CallCommand.java:79)\n",
      "\tat py4j.GatewayConnection.run(GatewayConnection.java:238)\n",
      "\tat java.base/java.lang.Thread.run(Thread.java:840)\n",
      "Caused by: java.net.UnknownHostException: <RDS-ENDPOINT>\n",
      "\tat java.base/sun.nio.ch.NioSocketImpl.connect(NioSocketImpl.java:572)\n",
      "\tat java.base/java.net.SocksSocketImpl.connect(SocksSocketImpl.java:327)\n",
      "\tat java.base/java.net.Socket.connect(Socket.java:633)\n",
      "\tat org.postgresql.core.PGStream.<init>(PGStream.java:70)\n",
      "\tat org.postgresql.core.v3.ConnectionFactoryImpl.tryConnect(ConnectionFactoryImpl.java:91)\n",
      "\tat org.postgresql.core.v3.ConnectionFactoryImpl.openConnectionImpl(ConnectionFactoryImpl.java:192)\n",
      "\t... 30 more\n",
      "\n",
      "Traceback (most recent call last):\n",
      "  File \"/mnt1/yarn/usercache/livy/appcache/application_1764439453312_0003/container_1764439453312_0003_01_000001/pyspark.zip/pyspark/sql/readwriter.py\", line 946, in jdbc\n",
      "    return self._df(self._jreader.jdbc(url, table, jprop))\n",
      "  File \"/mnt1/yarn/usercache/livy/appcache/application_1764439453312_0003/container_1764439453312_0003_01_000001/py4j-0.10.9.7-src.zip/py4j/java_gateway.py\", line 1322, in __call__\n",
      "    return_value = get_return_value(\n",
      "  File \"/mnt1/yarn/usercache/livy/appcache/application_1764439453312_0003/container_1764439453312_0003_01_000001/pyspark.zip/pyspark/errors/exceptions/captured.py\", line 179, in deco\n",
      "    return f(*a, **kw)\n",
      "  File \"/mnt1/yarn/usercache/livy/appcache/application_1764439453312_0003/container_1764439453312_0003_01_000001/py4j-0.10.9.7-src.zip/py4j/protocol.py\", line 326, in get_return_value\n",
      "    raise Py4JJavaError(\n",
      "py4j.protocol.Py4JJavaError: An error occurred while calling o102.jdbc.\n",
      ": org.postgresql.util.PSQLException: The connection attempt failed.\n",
      "\tat org.postgresql.core.v3.ConnectionFactoryImpl.openConnectionImpl(ConnectionFactoryImpl.java:292)\n",
      "\tat org.postgresql.core.ConnectionFactory.openConnection(ConnectionFactory.java:49)\n",
      "\tat org.postgresql.jdbc.PgConnection.<init>(PgConnection.java:195)\n",
      "\tat org.postgresql.Driver.makeConnection(Driver.java:454)\n",
      "\tat org.postgresql.Driver.connect(Driver.java:256)\n",
      "\tat org.apache.spark.sql.execution.datasources.jdbc.connection.BasicConnectionProvider.getConnection(BasicConnectionProvider.scala:49)\n",
      "\tat org.apache.spark.sql.execution.datasources.jdbc.connection.ConnectionProviderBase.create(ConnectionProvider.scala:102)\n",
      "\tat org.apache.spark.sql.jdbc.JdbcDialect.$anonfun$createConnectionFactory$1(JdbcDialects.scala:160)\n",
      "\tat org.apache.spark.sql.jdbc.JdbcDialect.$anonfun$createConnectionFactory$1$adapted(JdbcDialects.scala:156)\n",
      "\tat org.apache.spark.sql.execution.datasources.jdbc.JDBCRDD$.getQueryOutputSchema(JDBCRDD.scala:63)\n",
      "\tat org.apache.spark.sql.execution.datasources.jdbc.JDBCRDD$.resolveTable(JDBCRDD.scala:58)\n",
      "\tat org.apache.spark.sql.execution.datasources.jdbc.JDBCRelation$.getSchema(JDBCRelation.scala:241)\n",
      "\tat org.apache.spark.sql.execution.datasources.jdbc.JdbcRelationProvider.createRelation(JdbcRelationProvider.scala:37)\n",
      "\tat org.apache.spark.sql.execution.datasources.DataSource.resolveRelation(DataSource.scala:346)\n",
      "\tat org.apache.spark.sql.DataFrameReader.loadV1Source(DataFrameReader.scala:229)\n",
      "\tat org.apache.spark.sql.DataFrameReader.$anonfun$load$2(DataFrameReader.scala:211)\n",
      "\tat scala.Option.getOrElse(Option.scala:189)\n",
      "\tat org.apache.spark.sql.DataFrameReader.load(DataFrameReader.scala:211)\n",
      "\tat org.apache.spark.sql.DataFrameReader.load(DataFrameReader.scala:172)\n",
      "\tat org.apache.spark.sql.DataFrameReader.jdbc(DataFrameReader.scala:249)\n",
      "\tat java.base/jdk.internal.reflect.NativeMethodAccessorImpl.invoke0(Native Method)\n",
      "\tat java.base/jdk.internal.reflect.NativeMethodAccessorImpl.invoke(NativeMethodAccessorImpl.java:77)\n",
      "\tat java.base/jdk.internal.reflect.DelegatingMethodAccessorImpl.invoke(DelegatingMethodAccessorImpl.java:43)\n",
      "\tat java.base/java.lang.reflect.Method.invoke(Method.java:569)\n",
      "\tat py4j.reflection.MethodInvoker.invoke(MethodInvoker.java:244)\n",
      "\tat py4j.reflection.ReflectionEngine.invoke(ReflectionEngine.java:374)\n",
      "\tat py4j.Gateway.invoke(Gateway.java:282)\n",
      "\tat py4j.commands.AbstractCommand.invokeMethod(AbstractCommand.java:132)\n",
      "\tat py4j.commands.CallCommand.execute(CallCommand.java:79)\n",
      "\tat py4j.GatewayConnection.run(GatewayConnection.java:238)\n",
      "\tat java.base/java.lang.Thread.run(Thread.java:840)\n",
      "Caused by: java.net.UnknownHostException: <RDS-ENDPOINT>\n",
      "\tat java.base/sun.nio.ch.NioSocketImpl.connect(NioSocketImpl.java:572)\n",
      "\tat java.base/java.net.SocksSocketImpl.connect(SocksSocketImpl.java:327)\n",
      "\tat java.base/java.net.Socket.connect(Socket.java:633)\n",
      "\tat org.postgresql.core.PGStream.<init>(PGStream.java:70)\n",
      "\tat org.postgresql.core.v3.ConnectionFactoryImpl.tryConnect(ConnectionFactoryImpl.java:91)\n",
      "\tat org.postgresql.core.v3.ConnectionFactoryImpl.openConnectionImpl(ConnectionFactoryImpl.java:192)\n",
      "\t... 30 more\n",
      "\n",
      "\n"
     ]
    }
   ],
   "source": [
    "information_schemas_df = spark.read.jdbc(jdbc_url, \"information_schema.schemata\", properties=jdbc_properties)\n",
    "information_schemas_df.select([\"schema_name\", \"schema_owner\"]).show()"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "<a name='3'></a>\n",
    "## 3 - Apache Spark 101\n",
    "\n",
    "As mentioned before, Apache Spark is a workload engine that can be used with high-level APIs in programming languages such as Java, Scala and Python. The core abstraction of Spark is a Resilient Distributed Dataset (RDD), which is a collection of elements that can be partitioned across the nodes of the cluster; this enables running intensive workloads on the data in parallel. Spark also has some high-level toolsets like Spark SQL for structured data processing using SQL, pandas API for Spark to run pandas workloads, and MLlib for machine learning workloads. \n",
    "\n",
    "You will focus on PySpark, the Python API. However, you will skip the details on how to connect to a Spark cluster and most of the initial setup required as the necessary configuration to run this notebook has already been provided. \n",
    "\n",
    "In this section, you will get a brief overview of the required classes to access Spark with PySpark and run your workloads."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "<a name='3.1'></a>\n",
    "### 3.1 - Spark Classes\n",
    "\n",
    "To start a Spark program you must create a `SparkConf` object, that contains information about your application, and a `SparkContext` object, which tells Spark how to access a cluster.\n",
    "\n",
    "```python\n",
    "from pyspark import SparkContext, SparkConf\n",
    "conf = SparkConf().setAppName(appName).setMaster(master)\n",
    "sc = SparkContext(conf=conf)\n",
    "```\n",
    "\n",
    "The `appName` parameter is a string with the name of your application, it shows on the Spark cluster UI. `master` is the connection string to a Spark cluster or a `\"local\"` string to run in local mode."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "<a name='3.2'></a>\n",
    "### 3.2 - Spark DataFrame\n",
    "\n",
    "In this lab, you will be using the PySpark DataFrame API, which enables the use of Spark DataFrames, an abstraction on top of RDDs. For PySpark applications running this API, you can start by initializing a `SparkSession` object which is the entry point of PySpark.\n",
    "\n",
    "```python\n",
    "from pyspark.sql import SparkSession\n",
    "\n",
    "spark = SparkSession.builder.getOrCreate()\n",
    "```\n",
    "\n",
    "In this notebook, you can access a preconfigured `SparkSession` object using the `spark` variable, which you have used before to read the available tables. Now, you will start looking into the Spark DataFrame. Let's read the `orders` table from the `classicmodels` schema."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 91,
   "metadata": {
    "execution": {
     "iopub.execute_input": "2025-11-29T18:44:25.496719Z",
     "iopub.status.busy": "2025-11-29T18:44:25.496549Z",
     "iopub.status.idle": "2025-11-29T18:44:25.724506Z",
     "shell.execute_reply": "2025-11-29T18:44:25.723786Z",
     "shell.execute_reply.started": "2025-11-29T18:44:25.496697Z"
    }
   },
   "outputs": [
    {
     "data": {
      "application/vnd.jupyter.widget-view+json": {
       "model_id": "",
       "version_major": 2,
       "version_minor": 0
      },
      "text/plain": [
       "FloatProgress(value=0.0, bar_style='info', description='Progress:', layout=Layout(height='25px', width='50%'),\u2026"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "An error was encountered:\n",
      "An error occurred while calling o106.jdbc.\n",
      ": org.postgresql.util.PSQLException: The connection attempt failed.\n",
      "\tat org.postgresql.core.v3.ConnectionFactoryImpl.openConnectionImpl(ConnectionFactoryImpl.java:292)\n",
      "\tat org.postgresql.core.ConnectionFactory.openConnection(ConnectionFactory.java:49)\n",
      "\tat org.postgresql.jdbc.PgConnection.<init>(PgConnection.java:195)\n",
      "\tat org.postgresql.Driver.makeConnection(Driver.java:454)\n",
      "\tat org.postgresql.Driver.connect(Driver.java:256)\n",
      "\tat org.apache.spark.sql.execution.datasources.jdbc.connection.BasicConnectionProvider.getConnection(BasicConnectionProvider.scala:49)\n",
      "\tat org.apache.spark.sql.execution.datasources.jdbc.connection.ConnectionProviderBase.create(ConnectionProvider.scala:102)\n",
      "\tat org.apache.spark.sql.jdbc.JdbcDialect.$anonfun$createConnectionFactory$1(JdbcDialects.scala:160)\n",
      "\tat org.apache.spark.sql.jdbc.JdbcDialect.$anonfun$createConnectionFactory$1$adapted(JdbcDialects.scala:156)\n",
      "\tat org.apache.spark.sql.execution.datasources.jdbc.JDBCRDD$.getQueryOutputSchema(JDBCRDD.scala:63)\n",
      "\tat org.apache.spark.sql.execution.datasources.jdbc.JDBCRDD$.resolveTable(JDBCRDD.scala:58)\n",
      "\tat org.apache.spark.sql.execution.datasources.jdbc.JDBCRelation$.getSchema(JDBCRelation.scala:241)\n",
      "\tat org.apache.spark.sql.execution.datasources.jdbc.JdbcRelationProvider.createRelation(JdbcRelationProvider.scala:37)\n",
      "\tat org.apache.spark.sql.execution.datasources.DataSource.resolveRelation(DataSource.scala:346)\n",
      "\tat org.apache.spark.sql.DataFrameReader.loadV1Source(DataFrameReader.scala:229)\n",
      "\tat org.apache.spark.sql.DataFrameReader.$anonfun$load$2(DataFrameReader.scala:211)\n",
      "\tat scala.Option.getOrElse(Option.scala:189)\n",
      "\tat org.apache.spark.sql.DataFrameReader.load(DataFrameReader.scala:211)\n",
      "\tat org.apache.spark.sql.DataFrameReader.load(DataFrameReader.scala:172)\n",
      "\tat org.apache.spark.sql.DataFrameReader.jdbc(DataFrameReader.scala:249)\n",
      "\tat java.base/jdk.internal.reflect.NativeMethodAccessorImpl.invoke0(Native Method)\n",
      "\tat java.base/jdk.internal.reflect.NativeMethodAccessorImpl.invoke(NativeMethodAccessorImpl.java:77)\n",
      "\tat java.base/jdk.internal.reflect.DelegatingMethodAccessorImpl.invoke(DelegatingMethodAccessorImpl.java:43)\n",
      "\tat java.base/java.lang.reflect.Method.invoke(Method.java:569)\n",
      "\tat py4j.reflection.MethodInvoker.invoke(MethodInvoker.java:244)\n",
      "\tat py4j.reflection.ReflectionEngine.invoke(ReflectionEngine.java:374)\n",
      "\tat py4j.Gateway.invoke(Gateway.java:282)\n",
      "\tat py4j.commands.AbstractCommand.invokeMethod(AbstractCommand.java:132)\n",
      "\tat py4j.commands.CallCommand.execute(CallCommand.java:79)\n",
      "\tat py4j.GatewayConnection.run(GatewayConnection.java:238)\n",
      "\tat java.base/java.lang.Thread.run(Thread.java:840)\n",
      "Caused by: java.net.UnknownHostException: <RDS-ENDPOINT>\n",
      "\tat java.base/sun.nio.ch.NioSocketImpl.connect(NioSocketImpl.java:572)\n",
      "\tat java.base/java.net.SocksSocketImpl.connect(SocksSocketImpl.java:327)\n",
      "\tat java.base/java.net.Socket.connect(Socket.java:633)\n",
      "\tat org.postgresql.core.PGStream.<init>(PGStream.java:70)\n",
      "\tat org.postgresql.core.v3.ConnectionFactoryImpl.tryConnect(ConnectionFactoryImpl.java:91)\n",
      "\tat org.postgresql.core.v3.ConnectionFactoryImpl.openConnectionImpl(ConnectionFactoryImpl.java:192)\n",
      "\t... 30 more\n",
      "\n",
      "Traceback (most recent call last):\n",
      "  File \"/mnt1/yarn/usercache/livy/appcache/application_1764439453312_0003/container_1764439453312_0003_01_000001/pyspark.zip/pyspark/sql/readwriter.py\", line 946, in jdbc\n",
      "    return self._df(self._jreader.jdbc(url, table, jprop))\n",
      "  File \"/mnt1/yarn/usercache/livy/appcache/application_1764439453312_0003/container_1764439453312_0003_01_000001/py4j-0.10.9.7-src.zip/py4j/java_gateway.py\", line 1322, in __call__\n",
      "    return_value = get_return_value(\n",
      "  File \"/mnt1/yarn/usercache/livy/appcache/application_1764439453312_0003/container_1764439453312_0003_01_000001/pyspark.zip/pyspark/errors/exceptions/captured.py\", line 179, in deco\n",
      "    return f(*a, **kw)\n",
      "  File \"/mnt1/yarn/usercache/livy/appcache/application_1764439453312_0003/container_1764439453312_0003_01_000001/py4j-0.10.9.7-src.zip/py4j/protocol.py\", line 326, in get_return_value\n",
      "    raise Py4JJavaError(\n",
      "py4j.protocol.Py4JJavaError: An error occurred while calling o106.jdbc.\n",
      ": org.postgresql.util.PSQLException: The connection attempt failed.\n",
      "\tat org.postgresql.core.v3.ConnectionFactoryImpl.openConnectionImpl(ConnectionFactoryImpl.java:292)\n",
      "\tat org.postgresql.core.ConnectionFactory.openConnection(ConnectionFactory.java:49)\n",
      "\tat org.postgresql.jdbc.PgConnection.<init>(PgConnection.java:195)\n",
      "\tat org.postgresql.Driver.makeConnection(Driver.java:454)\n",
      "\tat org.postgresql.Driver.connect(Driver.java:256)\n",
      "\tat org.apache.spark.sql.execution.datasources.jdbc.connection.BasicConnectionProvider.getConnection(BasicConnectionProvider.scala:49)\n",
      "\tat org.apache.spark.sql.execution.datasources.jdbc.connection.ConnectionProviderBase.create(ConnectionProvider.scala:102)\n",
      "\tat org.apache.spark.sql.jdbc.JdbcDialect.$anonfun$createConnectionFactory$1(JdbcDialects.scala:160)\n",
      "\tat org.apache.spark.sql.jdbc.JdbcDialect.$anonfun$createConnectionFactory$1$adapted(JdbcDialects.scala:156)\n",
      "\tat org.apache.spark.sql.execution.datasources.jdbc.JDBCRDD$.getQueryOutputSchema(JDBCRDD.scala:63)\n",
      "\tat org.apache.spark.sql.execution.datasources.jdbc.JDBCRDD$.resolveTable(JDBCRDD.scala:58)\n",
      "\tat org.apache.spark.sql.execution.datasources.jdbc.JDBCRelation$.getSchema(JDBCRelation.scala:241)\n",
      "\tat org.apache.spark.sql.execution.datasources.jdbc.JdbcRelationProvider.createRelation(JdbcRelationProvider.scala:37)\n",
      "\tat org.apache.spark.sql.execution.datasources.DataSource.resolveRelation(DataSource.scala:346)\n",
      "\tat org.apache.spark.sql.DataFrameReader.loadV1Source(DataFrameReader.scala:229)\n",
      "\tat org.apache.spark.sql.DataFrameReader.$anonfun$load$2(DataFrameReader.scala:211)\n",
      "\tat scala.Option.getOrElse(Option.scala:189)\n",
      "\tat org.apache.spark.sql.DataFrameReader.load(DataFrameReader.scala:211)\n",
      "\tat org.apache.spark.sql.DataFrameReader.load(DataFrameReader.scala:172)\n",
      "\tat org.apache.spark.sql.DataFrameReader.jdbc(DataFrameReader.scala:249)\n",
      "\tat java.base/jdk.internal.reflect.NativeMethodAccessorImpl.invoke0(Native Method)\n",
      "\tat java.base/jdk.internal.reflect.NativeMethodAccessorImpl.invoke(NativeMethodAccessorImpl.java:77)\n",
      "\tat java.base/jdk.internal.reflect.DelegatingMethodAccessorImpl.invoke(DelegatingMethodAccessorImpl.java:43)\n",
      "\tat java.base/java.lang.reflect.Method.invoke(Method.java:569)\n",
      "\tat py4j.reflection.MethodInvoker.invoke(MethodInvoker.java:244)\n",
      "\tat py4j.reflection.ReflectionEngine.invoke(ReflectionEngine.java:374)\n",
      "\tat py4j.Gateway.invoke(Gateway.java:282)\n",
      "\tat py4j.commands.AbstractCommand.invokeMethod(AbstractCommand.java:132)\n",
      "\tat py4j.commands.CallCommand.execute(CallCommand.java:79)\n",
      "\tat py4j.GatewayConnection.run(GatewayConnection.java:238)\n",
      "\tat java.base/java.lang.Thread.run(Thread.java:840)\n",
      "Caused by: java.net.UnknownHostException: <RDS-ENDPOINT>\n",
      "\tat java.base/sun.nio.ch.NioSocketImpl.connect(NioSocketImpl.java:572)\n",
      "\tat java.base/java.net.SocksSocketImpl.connect(SocksSocketImpl.java:327)\n",
      "\tat java.base/java.net.Socket.connect(Socket.java:633)\n",
      "\tat org.postgresql.core.PGStream.<init>(PGStream.java:70)\n",
      "\tat org.postgresql.core.v3.ConnectionFactoryImpl.tryConnect(ConnectionFactoryImpl.java:91)\n",
      "\tat org.postgresql.core.v3.ConnectionFactoryImpl.openConnectionImpl(ConnectionFactoryImpl.java:192)\n",
      "\t... 30 more\n",
      "\n",
      "\n"
     ]
    }
   ],
   "source": [
    "%%pyspark\n",
    "\n",
    "# Read data from RDS into a Spark DataFrame\n",
    "orders_df = spark.read.jdbc(url=jdbc_url, table=\"classicmodels.orders\", properties=jdbc_properties)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Let's explore the DataFrame operations available, here is a list of some of the most important ones:\n",
    "\n",
    "* `df.printSchema()`: Prints the schema of the DataFrame.\n",
    "* `df.select(\"col\")`: Select the column of the name `col` from the DataFrame.\n",
    "* `df.show()`: Prints the content of the DataFrame.\n",
    "* `df.filter(df[\"col\"] > value)`: Filters the DataFrame based on a logical condition.\n",
    "* `df.groupBy(\"col\").agg()`: Perform an aggregation based on a column of name `col`. The aggregation can be `count`, `max`, `min`, `avg`.\n",
    "* `df.withColumn(\"new_col\",col_values)`: Adds a new column to the DataFrame with the `new_col` name and `col_values` as values for the column."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Let's start by printing the content of the `orders` DataFrame."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 92,
   "metadata": {
    "execution": {
     "iopub.execute_input": "2025-11-29T18:44:25.725630Z",
     "iopub.status.busy": "2025-11-29T18:44:25.725449Z",
     "iopub.status.idle": "2025-11-29T18:44:25.779762Z",
     "shell.execute_reply": "2025-11-29T18:44:25.776258Z",
     "shell.execute_reply.started": "2025-11-29T18:44:25.725606Z"
    }
   },
   "outputs": [
    {
     "data": {
      "application/vnd.jupyter.widget-view+json": {
       "model_id": "b0d4c9153c88403ab22efe645d67deab",
       "version_major": 2,
       "version_minor": 0
      },
      "text/plain": [
       "VBox()"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "data": {
      "application/vnd.jupyter.widget-view+json": {
       "model_id": "",
       "version_major": 2,
       "version_minor": 0
      },
      "text/plain": [
       "FloatProgress(value=0.0, bar_style='info', description='Progress:', layout=Layout(height='25px', width='50%'),\u2026"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "An error was encountered:\n",
      "name 'orders_df' is not defined\n",
      "Traceback (most recent call last):\n",
      "NameError: name 'orders_df' is not defined\n",
      "\n"
     ]
    }
   ],
   "source": [
    "orders_df.show()"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "You can create a Spark DataFrame from collections such as a list of tuples, a list of dictionaries, an RDD, and a `pandas` DataFrame. This is an example:"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 50,
   "metadata": {
    "execution": {
     "iopub.execute_input": "2025-11-29T18:43:25.743213Z",
     "iopub.status.busy": "2025-11-29T18:43:25.742982Z",
     "iopub.status.idle": "2025-11-29T18:43:31.053389Z",
     "shell.execute_reply": "2025-11-29T18:43:31.052686Z",
     "shell.execute_reply.started": "2025-11-29T18:43:25.743180Z"
    }
   },
   "outputs": [
    {
     "data": {
      "application/vnd.jupyter.widget-view+json": {
       "model_id": "84744d1b6aae488dadb7dec792a4cd1d",
       "version_major": 2,
       "version_minor": 0
      },
      "text/plain": [
       "VBox()"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "data": {
      "application/vnd.jupyter.widget-view+json": {
       "model_id": "",
       "version_major": 2,
       "version_minor": 0
      },
      "text/plain": [
       "FloatProgress(value=0.0, bar_style='info', description='Progress:', layout=Layout(height='25px', width='50%'),\u2026"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "+-----+---+\n",
      "|   _1| _2|\n",
      "+-----+---+\n",
      "|Alice|  1|\n",
      "|  Bob|  2|\n",
      "|Carla|  3|\n",
      "+-----+---+"
     ]
    }
   ],
   "source": [
    "list_of_tuples = [(\"Alice\", 1),(\"Bob\", 2),(\"Carla\", 3)]\n",
    "spark.createDataFrame(list_of_tuples).show()"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "You can define the schema as a second parameter, either by passing a list of column names or a `StructType`, the later one uses an array of `StructField` for each column with the corresponding name, type and if the column accepts nulls."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 51,
   "metadata": {
    "execution": {
     "iopub.execute_input": "2025-11-29T18:43:31.054773Z",
     "iopub.status.busy": "2025-11-29T18:43:31.054524Z",
     "iopub.status.idle": "2025-11-29T18:43:31.821459Z",
     "shell.execute_reply": "2025-11-29T18:43:31.820687Z",
     "shell.execute_reply.started": "2025-11-29T18:43:31.054738Z"
    }
   },
   "outputs": [
    {
     "data": {
      "application/vnd.jupyter.widget-view+json": {
       "model_id": "c0e6867dc6994d1c805ea16de1534720",
       "version_major": 2,
       "version_minor": 0
      },
      "text/plain": [
       "VBox()"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "data": {
      "application/vnd.jupyter.widget-view+json": {
       "model_id": "",
       "version_major": 2,
       "version_minor": 0
      },
      "text/plain": [
       "FloatProgress(value=0.0, bar_style='info', description='Progress:', layout=Layout(height='25px', width='50%'),\u2026"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "+-----+---+\n",
      "| name|age|\n",
      "+-----+---+\n",
      "|Alice|  1|\n",
      "|  Bob|  2|\n",
      "|Carla|  3|\n",
      "+-----+---+"
     ]
    }
   ],
   "source": [
    "from pyspark.sql.types import *\n",
    "schema = StructType([\n",
    "   StructField(\"name\", StringType(), True),\n",
    "   StructField(\"age\", IntegerType(), True)])\n",
    "test_df = spark.createDataFrame(list_of_tuples, schema)\n",
    "test_df.show()"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Finally, you can also write Spark DataFrames in the same formats that you can read. During this lab, you will save DataFrames to the same database. Here is an example of how to store a DataFrame to Postgres to a pre-created `test_schema`:"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 52,
   "metadata": {
    "execution": {
     "iopub.execute_input": "2025-11-29T18:43:31.822951Z",
     "iopub.status.busy": "2025-11-29T18:43:31.822688Z",
     "iopub.status.idle": "2025-11-29T18:43:32.079188Z",
     "shell.execute_reply": "2025-11-29T18:43:32.078483Z",
     "shell.execute_reply.started": "2025-11-29T18:43:31.822904Z"
    }
   },
   "outputs": [
    {
     "data": {
      "application/vnd.jupyter.widget-view+json": {
       "model_id": "75cd5b5257aa41e7a876f5cf0d6db8b6",
       "version_major": 2,
       "version_minor": 0
      },
      "text/plain": [
       "VBox()"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "data": {
      "application/vnd.jupyter.widget-view+json": {
       "model_id": "",
       "version_major": 2,
       "version_minor": 0
      },
      "text/plain": [
       "FloatProgress(value=0.0, bar_style='info', description='Progress:', layout=Layout(height='25px', width='50%'),\u2026"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "An error was encountered:\n",
      "An error occurred while calling o144.jdbc.\n",
      ": org.postgresql.util.PSQLException: The connection attempt failed.\n",
      "\tat org.postgresql.core.v3.ConnectionFactoryImpl.openConnectionImpl(ConnectionFactoryImpl.java:292)\n",
      "\tat org.postgresql.core.ConnectionFactory.openConnection(ConnectionFactory.java:49)\n",
      "\tat org.postgresql.jdbc.PgConnection.<init>(PgConnection.java:195)\n",
      "\tat org.postgresql.Driver.makeConnection(Driver.java:454)\n",
      "\tat org.postgresql.Driver.connect(Driver.java:256)\n",
      "\tat org.apache.spark.sql.execution.datasources.jdbc.connection.BasicConnectionProvider.getConnection(BasicConnectionProvider.scala:49)\n",
      "\tat org.apache.spark.sql.execution.datasources.jdbc.connection.ConnectionProviderBase.create(ConnectionProvider.scala:102)\n",
      "\tat org.apache.spark.sql.jdbc.JdbcDialect.$anonfun$createConnectionFactory$1(JdbcDialects.scala:160)\n",
      "\tat org.apache.spark.sql.jdbc.JdbcDialect.$anonfun$createConnectionFactory$1$adapted(JdbcDialects.scala:156)\n",
      "\tat org.apache.spark.sql.execution.datasources.jdbc.JdbcRelationProvider.createRelation(JdbcRelationProvider.scala:50)\n",
      "\tat org.apache.spark.sql.execution.datasources.SaveIntoDataSourceCommand.run(SaveIntoDataSourceCommand.scala:48)\n",
      "\tat org.apache.spark.sql.execution.command.ExecutedCommandExec.sideEffectResult$lzycompute(commands.scala:75)\n",
      "\tat org.apache.spark.sql.execution.command.ExecutedCommandExec.sideEffectResult(commands.scala:73)\n",
      "\tat org.apache.spark.sql.execution.command.ExecutedCommandExec.executeCollect(commands.scala:84)\n",
      "\tat org.apache.spark.sql.execution.QueryExecution$$anonfun$eagerlyExecuteCommands$1.$anonfun$applyOrElse$1(QueryExecution.scala:113)\n",
      "\tat org.apache.spark.sql.catalyst.QueryPlanningTracker$.withTracker(QueryPlanningTracker.scala:108)\n",
      "\tat org.apache.spark.sql.execution.SQLExecution$.withTracker(SQLExecution.scala:255)\n",
      "\tat org.apache.spark.sql.execution.SQLExecution$.executeQuery$1(SQLExecution.scala:129)\n",
      "\tat org.apache.spark.sql.execution.SQLExecution$.$anonfun$withNewExecutionId$9(SQLExecution.scala:165)\n",
      "\tat org.apache.spark.sql.catalyst.QueryPlanningTracker$.withTracker(QueryPlanningTracker.scala:108)\n",
      "\tat org.apache.spark.sql.execution.SQLExecution$.withTracker(SQLExecution.scala:255)\n",
      "\tat org.apache.spark.sql.execution.SQLExecution$.$anonfun$withNewExecutionId$8(SQLExecution.scala:165)\n",
      "\tat org.apache.spark.sql.execution.SQLExecution$.withSQLConfPropagated(SQLExecution.scala:276)\n",
      "\tat org.apache.spark.sql.execution.SQLExecution$.$anonfun$withNewExecutionId$1(SQLExecution.scala:164)\n",
      "\tat org.apache.spark.sql.SparkSession.withActive(SparkSession.scala:900)\n",
      "\tat org.apache.spark.sql.execution.SQLExecution$.withNewExecutionId(SQLExecution.scala:70)\n",
      "\tat org.apache.spark.sql.execution.QueryExecution$$anonfun$eagerlyExecuteCommands$1.applyOrElse(QueryExecution.scala:110)\n",
      "\tat org.apache.spark.sql.execution.QueryExecution$$anonfun$eagerlyExecuteCommands$1.applyOrElse(QueryExecution.scala:101)\n",
      "\tat org.apache.spark.sql.catalyst.trees.TreeNode.$anonfun$transformDownWithPruning$1(TreeNode.scala:503)\n",
      "\tat org.apache.spark.sql.catalyst.trees.CurrentOrigin$.withOrigin(origin.scala:76)\n",
      "\tat org.apache.spark.sql.catalyst.trees.TreeNode.transformDownWithPruning(TreeNode.scala:503)\n",
      "\tat org.apache.spark.sql.catalyst.plans.logical.LogicalPlan.org$apache$spark$sql$catalyst$plans$logical$AnalysisHelper$$super$transformDownWithPruning(LogicalPlan.scala:33)\n",
      "\tat org.apache.spark.sql.catalyst.plans.logical.AnalysisHelper.transformDownWithPruning(AnalysisHelper.scala:267)\n",
      "\tat org.apache.spark.sql.catalyst.plans.logical.AnalysisHelper.transformDownWithPruning$(AnalysisHelper.scala:263)\n",
      "\tat org.apache.spark.sql.catalyst.plans.logical.LogicalPlan.transformDownWithPruning(LogicalPlan.scala:33)\n",
      "\tat org.apache.spark.sql.catalyst.plans.logical.LogicalPlan.transformDownWithPruning(LogicalPlan.scala:33)\n",
      "\tat org.apache.spark.sql.catalyst.trees.TreeNode.transformDown(TreeNode.scala:479)\n",
      "\tat org.apache.spark.sql.execution.QueryExecution.eagerlyExecuteCommands(QueryExecution.scala:101)\n",
      "\tat org.apache.spark.sql.execution.QueryExecution.commandExecuted$lzycompute(QueryExecution.scala:88)\n",
      "\tat org.apache.spark.sql.execution.QueryExecution.commandExecuted(QueryExecution.scala:86)\n",
      "\tat org.apache.spark.sql.execution.QueryExecution.assertCommandExecuted(QueryExecution.scala:151)\n",
      "\tat org.apache.spark.sql.DataFrameWriter.runCommand(DataFrameWriter.scala:859)\n",
      "\tat org.apache.spark.sql.DataFrameWriter.saveToV1Source(DataFrameWriter.scala:388)\n",
      "\tat org.apache.spark.sql.DataFrameWriter.saveInternal(DataFrameWriter.scala:361)\n",
      "\tat org.apache.spark.sql.DataFrameWriter.save(DataFrameWriter.scala:248)\n",
      "\tat org.apache.spark.sql.DataFrameWriter.jdbc(DataFrameWriter.scala:756)\n",
      "\tat java.base/jdk.internal.reflect.NativeMethodAccessorImpl.invoke0(Native Method)\n",
      "\tat java.base/jdk.internal.reflect.NativeMethodAccessorImpl.invoke(NativeMethodAccessorImpl.java:77)\n",
      "\tat java.base/jdk.internal.reflect.DelegatingMethodAccessorImpl.invoke(DelegatingMethodAccessorImpl.java:43)\n",
      "\tat java.base/java.lang.reflect.Method.invoke(Method.java:569)\n",
      "\tat py4j.reflection.MethodInvoker.invoke(MethodInvoker.java:244)\n",
      "\tat py4j.reflection.ReflectionEngine.invoke(ReflectionEngine.java:374)\n",
      "\tat py4j.Gateway.invoke(Gateway.java:282)\n",
      "\tat py4j.commands.AbstractCommand.invokeMethod(AbstractCommand.java:132)\n",
      "\tat py4j.commands.CallCommand.execute(CallCommand.java:79)\n",
      "\tat py4j.GatewayConnection.run(GatewayConnection.java:238)\n",
      "\tat java.base/java.lang.Thread.run(Thread.java:840)\n",
      "Caused by: java.net.UnknownHostException: <RDS-ENDPOINT>\n",
      "\tat java.base/sun.nio.ch.NioSocketImpl.connect(NioSocketImpl.java:572)\n",
      "\tat java.base/java.net.SocksSocketImpl.connect(SocksSocketImpl.java:327)\n",
      "\tat java.base/java.net.Socket.connect(Socket.java:633)\n",
      "\tat org.postgresql.core.PGStream.<init>(PGStream.java:70)\n",
      "\tat org.postgresql.core.v3.ConnectionFactoryImpl.tryConnect(ConnectionFactoryImpl.java:91)\n",
      "\tat org.postgresql.core.v3.ConnectionFactoryImpl.openConnectionImpl(ConnectionFactoryImpl.java:192)\n",
      "\t... 56 more\n",
      "\n",
      "Traceback (most recent call last):\n",
      "  File \"/mnt1/yarn/usercache/livy/appcache/application_1764439453312_0002/container_1764439453312_0002_01_000001/pyspark.zip/pyspark/sql/readwriter.py\", line 1984, in jdbc\n",
      "    self.mode(mode)._jwrite.jdbc(url, table, jprop)\n",
      "  File \"/mnt1/yarn/usercache/livy/appcache/application_1764439453312_0002/container_1764439453312_0002_01_000001/py4j-0.10.9.7-src.zip/py4j/java_gateway.py\", line 1322, in __call__\n",
      "    return_value = get_return_value(\n",
      "  File \"/mnt1/yarn/usercache/livy/appcache/application_1764439453312_0002/container_1764439453312_0002_01_000001/pyspark.zip/pyspark/errors/exceptions/captured.py\", line 179, in deco\n",
      "    return f(*a, **kw)\n",
      "  File \"/mnt1/yarn/usercache/livy/appcache/application_1764439453312_0002/container_1764439453312_0002_01_000001/py4j-0.10.9.7-src.zip/py4j/protocol.py\", line 326, in get_return_value\n",
      "    raise Py4JJavaError(\n",
      "py4j.protocol.Py4JJavaError: An error occurred while calling o144.jdbc.\n",
      ": org.postgresql.util.PSQLException: The connection attempt failed.\n",
      "\tat org.postgresql.core.v3.ConnectionFactoryImpl.openConnectionImpl(ConnectionFactoryImpl.java:292)\n",
      "\tat org.postgresql.core.ConnectionFactory.openConnection(ConnectionFactory.java:49)\n",
      "\tat org.postgresql.jdbc.PgConnection.<init>(PgConnection.java:195)\n",
      "\tat org.postgresql.Driver.makeConnection(Driver.java:454)\n",
      "\tat org.postgresql.Driver.connect(Driver.java:256)\n",
      "\tat org.apache.spark.sql.execution.datasources.jdbc.connection.BasicConnectionProvider.getConnection(BasicConnectionProvider.scala:49)\n",
      "\tat org.apache.spark.sql.execution.datasources.jdbc.connection.ConnectionProviderBase.create(ConnectionProvider.scala:102)\n",
      "\tat org.apache.spark.sql.jdbc.JdbcDialect.$anonfun$createConnectionFactory$1(JdbcDialects.scala:160)\n",
      "\tat org.apache.spark.sql.jdbc.JdbcDialect.$anonfun$createConnectionFactory$1$adapted(JdbcDialects.scala:156)\n",
      "\tat org.apache.spark.sql.execution.datasources.jdbc.JdbcRelationProvider.createRelation(JdbcRelationProvider.scala:50)\n",
      "\tat org.apache.spark.sql.execution.datasources.SaveIntoDataSourceCommand.run(SaveIntoDataSourceCommand.scala:48)\n",
      "\tat org.apache.spark.sql.execution.command.ExecutedCommandExec.sideEffectResult$lzycompute(commands.scala:75)\n",
      "\tat org.apache.spark.sql.execution.command.ExecutedCommandExec.sideEffectResult(commands.scala:73)\n",
      "\tat org.apache.spark.sql.execution.command.ExecutedCommandExec.executeCollect(commands.scala:84)\n",
      "\tat org.apache.spark.sql.execution.QueryExecution$$anonfun$eagerlyExecuteCommands$1.$anonfun$applyOrElse$1(QueryExecution.scala:113)\n",
      "\tat org.apache.spark.sql.catalyst.QueryPlanningTracker$.withTracker(QueryPlanningTracker.scala:108)\n",
      "\tat org.apache.spark.sql.execution.SQLExecution$.withTracker(SQLExecution.scala:255)\n",
      "\tat org.apache.spark.sql.execution.SQLExecution$.executeQuery$1(SQLExecution.scala:129)\n",
      "\tat org.apache.spark.sql.execution.SQLExecution$.$anonfun$withNewExecutionId$9(SQLExecution.scala:165)\n",
      "\tat org.apache.spark.sql.catalyst.QueryPlanningTracker$.withTracker(QueryPlanningTracker.scala:108)\n",
      "\tat org.apache.spark.sql.execution.SQLExecution$.withTracker(SQLExecution.scala:255)\n",
      "\tat org.apache.spark.sql.execution.SQLExecution$.$anonfun$withNewExecutionId$8(SQLExecution.scala:165)\n",
      "\tat org.apache.spark.sql.execution.SQLExecution$.withSQLConfPropagated(SQLExecution.scala:276)\n",
      "\tat org.apache.spark.sql.execution.SQLExecution$.$anonfun$withNewExecutionId$1(SQLExecution.scala:164)\n",
      "\tat org.apache.spark.sql.SparkSession.withActive(SparkSession.scala:900)\n",
      "\tat org.apache.spark.sql.execution.SQLExecution$.withNewExecutionId(SQLExecution.scala:70)\n",
      "\tat org.apache.spark.sql.execution.QueryExecution$$anonfun$eagerlyExecuteCommands$1.applyOrElse(QueryExecution.scala:110)\n",
      "\tat org.apache.spark.sql.execution.QueryExecution$$anonfun$eagerlyExecuteCommands$1.applyOrElse(QueryExecution.scala:101)\n",
      "\tat org.apache.spark.sql.catalyst.trees.TreeNode.$anonfun$transformDownWithPruning$1(TreeNode.scala:503)\n",
      "\tat org.apache.spark.sql.catalyst.trees.CurrentOrigin$.withOrigin(origin.scala:76)\n",
      "\tat org.apache.spark.sql.catalyst.trees.TreeNode.transformDownWithPruning(TreeNode.scala:503)\n",
      "\tat org.apache.spark.sql.catalyst.plans.logical.LogicalPlan.org$apache$spark$sql$catalyst$plans$logical$AnalysisHelper$$super$transformDownWithPruning(LogicalPlan.scala:33)\n",
      "\tat org.apache.spark.sql.catalyst.plans.logical.AnalysisHelper.transformDownWithPruning(AnalysisHelper.scala:267)\n",
      "\tat org.apache.spark.sql.catalyst.plans.logical.AnalysisHelper.transformDownWithPruning$(AnalysisHelper.scala:263)\n",
      "\tat org.apache.spark.sql.catalyst.plans.logical.LogicalPlan.transformDownWithPruning(LogicalPlan.scala:33)\n",
      "\tat org.apache.spark.sql.catalyst.plans.logical.LogicalPlan.transformDownWithPruning(LogicalPlan.scala:33)\n",
      "\tat org.apache.spark.sql.catalyst.trees.TreeNode.transformDown(TreeNode.scala:479)\n",
      "\tat org.apache.spark.sql.execution.QueryExecution.eagerlyExecuteCommands(QueryExecution.scala:101)\n",
      "\tat org.apache.spark.sql.execution.QueryExecution.commandExecuted$lzycompute(QueryExecution.scala:88)\n",
      "\tat org.apache.spark.sql.execution.QueryExecution.commandExecuted(QueryExecution.scala:86)\n",
      "\tat org.apache.spark.sql.execution.QueryExecution.assertCommandExecuted(QueryExecution.scala:151)\n",
      "\tat org.apache.spark.sql.DataFrameWriter.runCommand(DataFrameWriter.scala:859)\n",
      "\tat org.apache.spark.sql.DataFrameWriter.saveToV1Source(DataFrameWriter.scala:388)\n",
      "\tat org.apache.spark.sql.DataFrameWriter.saveInternal(DataFrameWriter.scala:361)\n",
      "\tat org.apache.spark.sql.DataFrameWriter.save(DataFrameWriter.scala:248)\n",
      "\tat org.apache.spark.sql.DataFrameWriter.jdbc(DataFrameWriter.scala:756)\n",
      "\tat java.base/jdk.internal.reflect.NativeMethodAccessorImpl.invoke0(Native Method)\n",
      "\tat java.base/jdk.internal.reflect.NativeMethodAccessorImpl.invoke(NativeMethodAccessorImpl.java:77)\n",
      "\tat java.base/jdk.internal.reflect.DelegatingMethodAccessorImpl.invoke(DelegatingMethodAccessorImpl.java:43)\n",
      "\tat java.base/java.lang.reflect.Method.invoke(Method.java:569)\n",
      "\tat py4j.reflection.MethodInvoker.invoke(MethodInvoker.java:244)\n",
      "\tat py4j.reflection.ReflectionEngine.invoke(ReflectionEngine.java:374)\n",
      "\tat py4j.Gateway.invoke(Gateway.java:282)\n",
      "\tat py4j.commands.AbstractCommand.invokeMethod(AbstractCommand.java:132)\n",
      "\tat py4j.commands.CallCommand.execute(CallCommand.java:79)\n",
      "\tat py4j.GatewayConnection.run(GatewayConnection.java:238)\n",
      "\tat java.base/java.lang.Thread.run(Thread.java:840)\n",
      "Caused by: java.net.UnknownHostException: <RDS-ENDPOINT>\n",
      "\tat java.base/sun.nio.ch.NioSocketImpl.connect(NioSocketImpl.java:572)\n",
      "\tat java.base/java.net.SocksSocketImpl.connect(SocksSocketImpl.java:327)\n",
      "\tat java.base/java.net.Socket.connect(Socket.java:633)\n",
      "\tat org.postgresql.core.PGStream.<init>(PGStream.java:70)\n",
      "\tat org.postgresql.core.v3.ConnectionFactoryImpl.tryConnect(ConnectionFactoryImpl.java:91)\n",
      "\tat org.postgresql.core.v3.ConnectionFactoryImpl.openConnectionImpl(ConnectionFactoryImpl.java:192)\n",
      "\t... 56 more\n",
      "\n",
      "\n"
     ]
    }
   ],
   "source": [
    "test_df.write.jdbc(url=jdbc_url, table=\"test_schema.test_table\", mode=\"overwrite\", properties=jdbc_properties)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "<a name='3.3'></a>\n",
    "### 3.3 - Spark SQL\n",
    "\n",
    "As mentioned before, one of the high-level tools offered by Spark is Spark SQL, this will be one of the main tools you will use during the lab. You can perform SQL queries using the available DataFrames, first, you have to register each DataFrame as a temporary view and then call the `sql` function from the `SparkSession` object."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 53,
   "metadata": {
    "execution": {
     "iopub.execute_input": "2025-11-29T18:43:32.080508Z",
     "iopub.status.busy": "2025-11-29T18:43:32.080245Z",
     "iopub.status.idle": "2025-11-29T18:43:32.143886Z",
     "shell.execute_reply": "2025-11-29T18:43:32.143176Z",
     "shell.execute_reply.started": "2025-11-29T18:43:32.080473Z"
    }
   },
   "outputs": [
    {
     "data": {
      "application/vnd.jupyter.widget-view+json": {
       "model_id": "c70d89d01c66489babd2bffadc082050",
       "version_major": 2,
       "version_minor": 0
      },
      "text/plain": [
       "VBox()"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "data": {
      "application/vnd.jupyter.widget-view+json": {
       "model_id": "",
       "version_major": 2,
       "version_minor": 0
      },
      "text/plain": [
       "FloatProgress(value=0.0, bar_style='info', description='Progress:', layout=Layout(height='25px', width='50%'),\u2026"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "An error was encountered:\n",
      "name 'orders_df' is not defined\n",
      "Traceback (most recent call last):\n",
      "NameError: name 'orders_df' is not defined\n",
      "\n"
     ]
    }
   ],
   "source": [
    "orders_df.createOrReplaceTempView(\"orders\")\n",
    "\n",
    "sqlDF = spark.sql(\"SELECT * FROM orders\")\n",
    "sqlDF.show()"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "<a name='3.4'></a>\n",
    "### 3.4 - UDFs and Data Types\n",
    "\n",
    "Another advantage of Spark SQL is the definition of Python functions as SQL User Defined Functions (UDF). UDFs help us store custom logic and use it with multiple DataFrames. For example, if you want a text column to be title case (the first letter of each word is capitalized), you can define a function for it and then store it as a UDF.\n",
    "\n",
    "```python\n",
    "from pyspark.sql.types import StringType\n",
    "\n",
    "def titleCase(text: str):\n",
    "    output = ' '.join(word[0].upper() + word[1:] for word in text.split())\n",
    "    return output\n",
    "\n",
    "spark.udf.register(\"titleUDF\", titleCase, StringType())\n",
    "\n",
    "spark.sql(\"select book_id, titleUDF(book_name) as title from books\")\n",
    "```"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "In the previous example, you registered the UDF using the `spark.udf.register` function, which takes the name of the function to use in SQL, the Python function and the return type. You use Spark SQL Data Types in this case, you will work more with them later, as they can be used to describe the schema of a Spark DataFrame. It's also worth mentioning that you can also use UDF directly on DataFrames; in this case, we use a `lambda` function.\n",
    "\n",
    "```python\n",
    "from pyspark.sql.functions import col, udf\n",
    "\n",
    "titleUDF = udf(lambda z: titleCase(z),StringType())\n",
    "\n",
    "books_df.select(col(\"book_id\"), titleUDF(col(\"book_name\")).alias(\"title\"))\n",
    "```"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "You will create a UDF using this method to generate surrogate keys. You will use the `hashlib` library to generate a hash based on a list of column values. "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 54,
   "metadata": {
    "execution": {
     "iopub.execute_input": "2025-11-29T18:43:32.144851Z",
     "iopub.status.busy": "2025-11-29T18:43:32.144692Z",
     "iopub.status.idle": "2025-11-29T18:43:32.196901Z",
     "shell.execute_reply": "2025-11-29T18:43:32.196148Z",
     "shell.execute_reply.started": "2025-11-29T18:43:32.144830Z"
    }
   },
   "outputs": [
    {
     "data": {
      "application/vnd.jupyter.widget-view+json": {
       "model_id": "627b8120bb404e24b59f73d7bb35a152",
       "version_major": 2,
       "version_minor": 0
      },
      "text/plain": [
       "VBox()"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "data": {
      "application/vnd.jupyter.widget-view+json": {
       "model_id": "",
       "version_major": 2,
       "version_minor": 0
      },
      "text/plain": [
       "FloatProgress(value=0.0, bar_style='info', description='Progress:', layout=Layout(height='25px', width='50%'),\u2026"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    }
   ],
   "source": [
    "import hashlib\n",
    "from typing import List\n",
    "from pyspark.sql.types import StringType\n",
    "from pyspark.sql.functions import col, udf, array\n",
    "\n",
    "def surrogateKey(text_values: List[str]):\n",
    "    sha256 = hashlib.sha256()\n",
    "    data = ''.join(text_values)\n",
    "    sha256.update(data.encode())\n",
    "    return sha256.hexdigest()"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Let's test the function, create an array of strings, and call the function with it. "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 55,
   "metadata": {
    "execution": {
     "iopub.execute_input": "2025-11-29T18:43:32.198871Z",
     "iopub.status.busy": "2025-11-29T18:43:32.198035Z",
     "iopub.status.idle": "2025-11-29T18:43:32.256752Z",
     "shell.execute_reply": "2025-11-29T18:43:32.256131Z",
     "shell.execute_reply.started": "2025-11-29T18:43:32.198830Z"
    }
   },
   "outputs": [
    {
     "data": {
      "application/vnd.jupyter.widget-view+json": {
       "model_id": "4d9f6900e867428598993f3e3001dc60",
       "version_major": 2,
       "version_minor": 0
      },
      "text/plain": [
       "VBox()"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "data": {
      "application/vnd.jupyter.widget-view+json": {
       "model_id": "",
       "version_major": 2,
       "version_minor": 0
      },
      "text/plain": [
       "FloatProgress(value=0.0, bar_style='info', description='Progress:', layout=Layout(height='25px', width='50%'),\u2026"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "'ba6de314675567f28b142ba42bab0ab026f777507ab8ca885397dd9494d2a855'"
     ]
    }
   ],
   "source": [
    "surrogateKey([\"01221212\",\"123123123\",\"Hello World\"])"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Now let's create the UDF with the `surrogateKey` function and a lambda function, the return type is `StringType()`."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 56,
   "metadata": {
    "execution": {
     "iopub.execute_input": "2025-11-29T18:43:32.258565Z",
     "iopub.status.busy": "2025-11-29T18:43:32.258022Z",
     "iopub.status.idle": "2025-11-29T18:43:32.316938Z",
     "shell.execute_reply": "2025-11-29T18:43:32.316201Z",
     "shell.execute_reply.started": "2025-11-29T18:43:32.258527Z"
    }
   },
   "outputs": [
    {
     "data": {
      "application/vnd.jupyter.widget-view+json": {
       "model_id": "d10738cb53c74cf4af6c40ad27afbd40",
       "version_major": 2,
       "version_minor": 0
      },
      "text/plain": [
       "VBox()"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "data": {
      "application/vnd.jupyter.widget-view+json": {
       "model_id": "",
       "version_major": 2,
       "version_minor": 0
      },
      "text/plain": [
       "FloatProgress(value=0.0, bar_style='info', description='Progress:', layout=Layout(height='25px', width='50%'),\u2026"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    }
   ],
   "source": [
    "surrogateUDF = udf(lambda z: surrogateKey(z),StringType())"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Let's test this new UDF with the `orders_df`, use the `withColumn` function to generate a new column `order_key`, and pass to the UDF the `ordernumber` and `status` column as an `array`."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 57,
   "metadata": {
    "execution": {
     "iopub.execute_input": "2025-11-29T18:43:32.318749Z",
     "iopub.status.busy": "2025-11-29T18:43:32.318506Z",
     "iopub.status.idle": "2025-11-29T18:43:32.380982Z",
     "shell.execute_reply": "2025-11-29T18:43:32.380080Z",
     "shell.execute_reply.started": "2025-11-29T18:43:32.318715Z"
    }
   },
   "outputs": [
    {
     "data": {
      "application/vnd.jupyter.widget-view+json": {
       "model_id": "e7f2d09409e545a292c8891c6c158057",
       "version_major": 2,
       "version_minor": 0
      },
      "text/plain": [
       "VBox()"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "data": {
      "application/vnd.jupyter.widget-view+json": {
       "model_id": "",
       "version_major": 2,
       "version_minor": 0
      },
      "text/plain": [
       "FloatProgress(value=0.0, bar_style='info', description='Progress:', layout=Layout(height='25px', width='50%'),\u2026"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "An error was encountered:\n",
      "name 'orders_df' is not defined\n",
      "Traceback (most recent call last):\n",
      "NameError: name 'orders_df' is not defined\n",
      "\n"
     ]
    }
   ],
   "source": [
    "orders_df.withColumn(\"order_key\",surrogateUDF(array(orders_df.ordernumber,orders_df.status))).show()"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "<a name='4'></a>\n",
    "## 4 - Data Modeling with Spark\n",
    "\n",
    "As mentioned before, you will recreate the star schema from the Week 1 assignment of this course. "
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "<a name='4.1'></a>\n",
    "### 4.1 - Read the Tables\n",
    "\n",
    "For the first step, you will read the `classicmodels` tables with Spark."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 58,
   "metadata": {
    "execution": {
     "iopub.execute_input": "2025-11-29T18:43:32.382556Z",
     "iopub.status.busy": "2025-11-29T18:43:32.382304Z",
     "iopub.status.idle": "2025-11-29T18:43:32.645485Z",
     "shell.execute_reply": "2025-11-29T18:43:32.644810Z",
     "shell.execute_reply.started": "2025-11-29T18:43:32.382520Z"
    }
   },
   "outputs": [
    {
     "data": {
      "application/vnd.jupyter.widget-view+json": {
       "model_id": "ea068b495ad4463db1331f40c496916c",
       "version_major": 2,
       "version_minor": 0
      },
      "text/plain": [
       "VBox()"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "data": {
      "application/vnd.jupyter.widget-view+json": {
       "model_id": "",
       "version_major": 2,
       "version_minor": 0
      },
      "text/plain": [
       "FloatProgress(value=0.0, bar_style='info', description='Progress:', layout=Layout(height='25px', width='50%'),\u2026"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "An error was encountered:\n",
      "An error occurred while calling o148.jdbc.\n",
      ": org.postgresql.util.PSQLException: The connection attempt failed.\n",
      "\tat org.postgresql.core.v3.ConnectionFactoryImpl.openConnectionImpl(ConnectionFactoryImpl.java:292)\n",
      "\tat org.postgresql.core.ConnectionFactory.openConnection(ConnectionFactory.java:49)\n",
      "\tat org.postgresql.jdbc.PgConnection.<init>(PgConnection.java:195)\n",
      "\tat org.postgresql.Driver.makeConnection(Driver.java:454)\n",
      "\tat org.postgresql.Driver.connect(Driver.java:256)\n",
      "\tat org.apache.spark.sql.execution.datasources.jdbc.connection.BasicConnectionProvider.getConnection(BasicConnectionProvider.scala:49)\n",
      "\tat org.apache.spark.sql.execution.datasources.jdbc.connection.ConnectionProviderBase.create(ConnectionProvider.scala:102)\n",
      "\tat org.apache.spark.sql.jdbc.JdbcDialect.$anonfun$createConnectionFactory$1(JdbcDialects.scala:160)\n",
      "\tat org.apache.spark.sql.jdbc.JdbcDialect.$anonfun$createConnectionFactory$1$adapted(JdbcDialects.scala:156)\n",
      "\tat org.apache.spark.sql.execution.datasources.jdbc.JDBCRDD$.getQueryOutputSchema(JDBCRDD.scala:63)\n",
      "\tat org.apache.spark.sql.execution.datasources.jdbc.JDBCRDD$.resolveTable(JDBCRDD.scala:58)\n",
      "\tat org.apache.spark.sql.execution.datasources.jdbc.JDBCRelation$.getSchema(JDBCRelation.scala:241)\n",
      "\tat org.apache.spark.sql.execution.datasources.jdbc.JdbcRelationProvider.createRelation(JdbcRelationProvider.scala:37)\n",
      "\tat org.apache.spark.sql.execution.datasources.DataSource.resolveRelation(DataSource.scala:346)\n",
      "\tat org.apache.spark.sql.DataFrameReader.loadV1Source(DataFrameReader.scala:229)\n",
      "\tat org.apache.spark.sql.DataFrameReader.$anonfun$load$2(DataFrameReader.scala:211)\n",
      "\tat scala.Option.getOrElse(Option.scala:189)\n",
      "\tat org.apache.spark.sql.DataFrameReader.load(DataFrameReader.scala:211)\n",
      "\tat org.apache.spark.sql.DataFrameReader.load(DataFrameReader.scala:172)\n",
      "\tat org.apache.spark.sql.DataFrameReader.jdbc(DataFrameReader.scala:249)\n",
      "\tat java.base/jdk.internal.reflect.NativeMethodAccessorImpl.invoke0(Native Method)\n",
      "\tat java.base/jdk.internal.reflect.NativeMethodAccessorImpl.invoke(NativeMethodAccessorImpl.java:77)\n",
      "\tat java.base/jdk.internal.reflect.DelegatingMethodAccessorImpl.invoke(DelegatingMethodAccessorImpl.java:43)\n",
      "\tat java.base/java.lang.reflect.Method.invoke(Method.java:569)\n",
      "\tat py4j.reflection.MethodInvoker.invoke(MethodInvoker.java:244)\n",
      "\tat py4j.reflection.ReflectionEngine.invoke(ReflectionEngine.java:374)\n",
      "\tat py4j.Gateway.invoke(Gateway.java:282)\n",
      "\tat py4j.commands.AbstractCommand.invokeMethod(AbstractCommand.java:132)\n",
      "\tat py4j.commands.CallCommand.execute(CallCommand.java:79)\n",
      "\tat py4j.GatewayConnection.run(GatewayConnection.java:238)\n",
      "\tat java.base/java.lang.Thread.run(Thread.java:840)\n",
      "Caused by: java.net.UnknownHostException: <RDS-ENDPOINT>\n",
      "\tat java.base/sun.nio.ch.NioSocketImpl.connect(NioSocketImpl.java:572)\n",
      "\tat java.base/java.net.SocksSocketImpl.connect(SocksSocketImpl.java:327)\n",
      "\tat java.base/java.net.Socket.connect(Socket.java:633)\n",
      "\tat org.postgresql.core.PGStream.<init>(PGStream.java:70)\n",
      "\tat org.postgresql.core.v3.ConnectionFactoryImpl.tryConnect(ConnectionFactoryImpl.java:91)\n",
      "\tat org.postgresql.core.v3.ConnectionFactoryImpl.openConnectionImpl(ConnectionFactoryImpl.java:192)\n",
      "\t... 30 more\n",
      "\n",
      "Traceback (most recent call last):\n",
      "  File \"/mnt1/yarn/usercache/livy/appcache/application_1764439453312_0002/container_1764439453312_0002_01_000001/pyspark.zip/pyspark/sql/readwriter.py\", line 946, in jdbc\n",
      "    return self._df(self._jreader.jdbc(url, table, jprop))\n",
      "  File \"/mnt1/yarn/usercache/livy/appcache/application_1764439453312_0002/container_1764439453312_0002_01_000001/py4j-0.10.9.7-src.zip/py4j/java_gateway.py\", line 1322, in __call__\n",
      "    return_value = get_return_value(\n",
      "  File \"/mnt1/yarn/usercache/livy/appcache/application_1764439453312_0002/container_1764439453312_0002_01_000001/pyspark.zip/pyspark/errors/exceptions/captured.py\", line 179, in deco\n",
      "    return f(*a, **kw)\n",
      "  File \"/mnt1/yarn/usercache/livy/appcache/application_1764439453312_0002/container_1764439453312_0002_01_000001/py4j-0.10.9.7-src.zip/py4j/protocol.py\", line 326, in get_return_value\n",
      "    raise Py4JJavaError(\n",
      "py4j.protocol.Py4JJavaError: An error occurred while calling o148.jdbc.\n",
      ": org.postgresql.util.PSQLException: The connection attempt failed.\n",
      "\tat org.postgresql.core.v3.ConnectionFactoryImpl.openConnectionImpl(ConnectionFactoryImpl.java:292)\n",
      "\tat org.postgresql.core.ConnectionFactory.openConnection(ConnectionFactory.java:49)\n",
      "\tat org.postgresql.jdbc.PgConnection.<init>(PgConnection.java:195)\n",
      "\tat org.postgresql.Driver.makeConnection(Driver.java:454)\n",
      "\tat org.postgresql.Driver.connect(Driver.java:256)\n",
      "\tat org.apache.spark.sql.execution.datasources.jdbc.connection.BasicConnectionProvider.getConnection(BasicConnectionProvider.scala:49)\n",
      "\tat org.apache.spark.sql.execution.datasources.jdbc.connection.ConnectionProviderBase.create(ConnectionProvider.scala:102)\n",
      "\tat org.apache.spark.sql.jdbc.JdbcDialect.$anonfun$createConnectionFactory$1(JdbcDialects.scala:160)\n",
      "\tat org.apache.spark.sql.jdbc.JdbcDialect.$anonfun$createConnectionFactory$1$adapted(JdbcDialects.scala:156)\n",
      "\tat org.apache.spark.sql.execution.datasources.jdbc.JDBCRDD$.getQueryOutputSchema(JDBCRDD.scala:63)\n",
      "\tat org.apache.spark.sql.execution.datasources.jdbc.JDBCRDD$.resolveTable(JDBCRDD.scala:58)\n",
      "\tat org.apache.spark.sql.execution.datasources.jdbc.JDBCRelation$.getSchema(JDBCRelation.scala:241)\n",
      "\tat org.apache.spark.sql.execution.datasources.jdbc.JdbcRelationProvider.createRelation(JdbcRelationProvider.scala:37)\n",
      "\tat org.apache.spark.sql.execution.datasources.DataSource.resolveRelation(DataSource.scala:346)\n",
      "\tat org.apache.spark.sql.DataFrameReader.loadV1Source(DataFrameReader.scala:229)\n",
      "\tat org.apache.spark.sql.DataFrameReader.$anonfun$load$2(DataFrameReader.scala:211)\n",
      "\tat scala.Option.getOrElse(Option.scala:189)\n",
      "\tat org.apache.spark.sql.DataFrameReader.load(DataFrameReader.scala:211)\n",
      "\tat org.apache.spark.sql.DataFrameReader.load(DataFrameReader.scala:172)\n",
      "\tat org.apache.spark.sql.DataFrameReader.jdbc(DataFrameReader.scala:249)\n",
      "\tat java.base/jdk.internal.reflect.NativeMethodAccessorImpl.invoke0(Native Method)\n",
      "\tat java.base/jdk.internal.reflect.NativeMethodAccessorImpl.invoke(NativeMethodAccessorImpl.java:77)\n",
      "\tat java.base/jdk.internal.reflect.DelegatingMethodAccessorImpl.invoke(DelegatingMethodAccessorImpl.java:43)\n",
      "\tat java.base/java.lang.reflect.Method.invoke(Method.java:569)\n",
      "\tat py4j.reflection.MethodInvoker.invoke(MethodInvoker.java:244)\n",
      "\tat py4j.reflection.ReflectionEngine.invoke(ReflectionEngine.java:374)\n",
      "\tat py4j.Gateway.invoke(Gateway.java:282)\n",
      "\tat py4j.commands.AbstractCommand.invokeMethod(AbstractCommand.java:132)\n",
      "\tat py4j.commands.CallCommand.execute(CallCommand.java:79)\n",
      "\tat py4j.GatewayConnection.run(GatewayConnection.java:238)\n",
      "\tat java.base/java.lang.Thread.run(Thread.java:840)\n",
      "Caused by: java.net.UnknownHostException: <RDS-ENDPOINT>\n",
      "\tat java.base/sun.nio.ch.NioSocketImpl.connect(NioSocketImpl.java:572)\n",
      "\tat java.base/java.net.SocksSocketImpl.connect(SocksSocketImpl.java:327)\n",
      "\tat java.base/java.net.Socket.connect(Socket.java:633)\n",
      "\tat org.postgresql.core.PGStream.<init>(PGStream.java:70)\n",
      "\tat org.postgresql.core.v3.ConnectionFactoryImpl.tryConnect(ConnectionFactoryImpl.java:91)\n",
      "\tat org.postgresql.core.v3.ConnectionFactoryImpl.openConnectionImpl(ConnectionFactoryImpl.java:192)\n",
      "\t... 30 more\n",
      "\n",
      "\n"
     ]
    }
   ],
   "source": [
    "employees_df = spark.read.jdbc(url=jdbc_url, table=\"classicmodels.employees\", properties=jdbc_properties)\n",
    "offices_df = spark.read.jdbc(url=jdbc_url, table=\"classicmodels.offices\", properties=jdbc_properties)\n",
    "customers_df = spark.read.jdbc(url=jdbc_url, table=\"classicmodels.customers\", properties=jdbc_properties)\n",
    "orderdetails_df = spark.read.jdbc(url=jdbc_url, table=\"classicmodels.orderdetails\", properties=jdbc_properties)\n",
    "productlines_df = spark.read.jdbc(url=jdbc_url, table=\"classicmodels.productlines\", properties=jdbc_properties)\n",
    "products_df = spark.read.jdbc(url=jdbc_url, table=\"classicmodels.products\", properties=jdbc_properties)\n",
    "payments_df = spark.read.jdbc(url=jdbc_url, table=\"classicmodels.payments\", properties=jdbc_properties)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Register the Spark DataFrames as temporary views, and call them with the same name as their Postgres RDS counterpart."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 59,
   "metadata": {
    "execution": {
     "iopub.execute_input": "2025-11-29T18:43:32.646611Z",
     "iopub.status.busy": "2025-11-29T18:43:32.646447Z",
     "iopub.status.idle": "2025-11-29T18:43:32.694695Z",
     "shell.execute_reply": "2025-11-29T18:43:32.693850Z",
     "shell.execute_reply.started": "2025-11-29T18:43:32.646590Z"
    }
   },
   "outputs": [
    {
     "data": {
      "application/vnd.jupyter.widget-view+json": {
       "model_id": "71012f28a6eb4378bda04a330a1a16b4",
       "version_major": 2,
       "version_minor": 0
      },
      "text/plain": [
       "VBox()"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "data": {
      "application/vnd.jupyter.widget-view+json": {
       "model_id": "",
       "version_major": 2,
       "version_minor": 0
      },
      "text/plain": [
       "FloatProgress(value=0.0, bar_style='info', description='Progress:', layout=Layout(height='25px', width='50%'),\u2026"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "An error was encountered:\n",
      "name 'employees_df' is not defined\n",
      "Traceback (most recent call last):\n",
      "NameError: name 'employees_df' is not defined\n",
      "\n"
     ]
    }
   ],
   "source": [
    "employees_df.createOrReplaceTempView(\"employees\")\n",
    "offices_df.createOrReplaceTempView(\"offices\")\n",
    "customers_df.createOrReplaceTempView(\"customers\")\n",
    "orderdetails_df.createOrReplaceTempView(\"orderdetails\")\n",
    "productlines_df.createOrReplaceTempView(\"productlines\")\n",
    "products_df.createOrReplaceTempView(\"products\")\n",
    "payments_df.createOrReplaceTempView(\"payments\")"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "You can verify the schema of any of the tables with the `printSchema` function."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 60,
   "metadata": {
    "execution": {
     "iopub.execute_input": "2025-11-29T18:43:32.696438Z",
     "iopub.status.busy": "2025-11-29T18:43:32.695969Z",
     "iopub.status.idle": "2025-11-29T18:43:32.764302Z",
     "shell.execute_reply": "2025-11-29T18:43:32.761612Z",
     "shell.execute_reply.started": "2025-11-29T18:43:32.696400Z"
    }
   },
   "outputs": [
    {
     "data": {
      "application/vnd.jupyter.widget-view+json": {
       "model_id": "ad42c83b571941e8b87a6b81a395f36e",
       "version_major": 2,
       "version_minor": 0
      },
      "text/plain": [
       "VBox()"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "data": {
      "application/vnd.jupyter.widget-view+json": {
       "model_id": "",
       "version_major": 2,
       "version_minor": 0
      },
      "text/plain": [
       "FloatProgress(value=0.0, bar_style='info', description='Progress:', layout=Layout(height='25px', width='50%'),\u2026"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "An error was encountered:\n",
      "name 'products_df' is not defined\n",
      "Traceback (most recent call last):\n",
      "NameError: name 'products_df' is not defined\n",
      "\n"
     ]
    }
   ],
   "source": [
    "products_df.printSchema()"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "<a name='4.2'></a>\n",
    "### 4.2 - Star Schema\n",
    "\n",
    "Here is the new ERM diagram for the star schema, which was modified to include some transformations. You will use Spark SQL to bring the necessary columns for each table. Then you will perform additional operations to the resulting DataFrame and store the resulting data in the `classicmodels_star_schema` schema in the Postgres RDS.\n",
    "\n",
    "![img](https://dlai-data-engineering.s3.amazonaws.com/labs/c4w3a1-177787/images/star_schema.png)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "<a name='4.3'></a>\n",
    "### 4.3 - Customers Dimension\n",
    "\n",
    "Let's start with the dimensional tables, first with the `customers` dimension. "
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "4.3.1. Create a SQL query that brings the relevant columns from the `customers` temporal view and stores the query result in a Spark DataFrame. Follow the instructions to prepare the query:\n",
    "- You need to create a column `customer_number` based on the `customerNumber`, which will be a surrogate key `customer_key` later. This function requires an array of text columns so you will need to `cast()` the `customerNumber` to string.\n",
    "- For this new data model you are required to create the field `contact_name` which is a combination of the `contactFirstName` and `contactLastName` fields. You can use the function `concat()`."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 61,
   "metadata": {
    "execution": {
     "iopub.execute_input": "2025-11-29T18:43:32.766080Z",
     "iopub.status.busy": "2025-11-29T18:43:32.765659Z",
     "iopub.status.idle": "2025-11-29T18:43:33.024354Z",
     "shell.execute_reply": "2025-11-29T18:43:33.023705Z",
     "shell.execute_reply.started": "2025-11-29T18:43:32.766010Z"
    },
    "exercise": [
     "ex01"
    ],
    "tags": [
     "graded"
    ]
   },
   "outputs": [
    {
     "data": {
      "application/vnd.jupyter.widget-view+json": {
       "model_id": "b874c6c5be754d9fb80e683c66e47b2e",
       "version_major": 2,
       "version_minor": 0
      },
      "text/plain": [
       "VBox()"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "data": {
      "application/vnd.jupyter.widget-view+json": {
       "model_id": "",
       "version_major": 2,
       "version_minor": 0
      },
      "text/plain": [
       "FloatProgress(value=0.0, bar_style='info', description='Progress:', layout=Layout(height='25px', width='50%'),\u2026"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "An error was encountered:\n",
      "\n",
      "[PARSE_SYNTAX_ERROR] Syntax error at or near 'as'.(line 3, pos 14)\n",
      "\n",
      "== SQL ==\n",
      "\n",
      "SELECT \n",
      "    None(None as string) as customer_number, \n",
      "--------------^^^\n",
      "    customerName as customer_name,\n",
      "    None(None, None) as contact_name, \n",
      "    phone as phone, \n",
      "    addressLine1 as address_line_1, \n",
      "    addressLine2 as address_line_2, \n",
      "    postalCode as postal_code, \n",
      "    city as city, \n",
      "    state as state, \n",
      "    country as country,\n",
      "    creditLimit as credit_limit\n",
      "FROM customers\n",
      "\n",
      "Traceback (most recent call last):\n",
      "  File \"/mnt1/yarn/usercache/livy/appcache/application_1764439453312_0002/container_1764439453312_0002_01_000001/pyspark.zip/pyspark/sql/session.py\", line 1631, in sql\n",
      "    return DataFrame(self._jsparkSession.sql(sqlQuery, litArgs), self)\n",
      "  File \"/mnt1/yarn/usercache/livy/appcache/application_1764439453312_0002/container_1764439453312_0002_01_000001/py4j-0.10.9.7-src.zip/py4j/java_gateway.py\", line 1322, in __call__\n",
      "    return_value = get_return_value(\n",
      "  File \"/mnt1/yarn/usercache/livy/appcache/application_1764439453312_0002/container_1764439453312_0002_01_000001/pyspark.zip/pyspark/errors/exceptions/captured.py\", line 185, in deco\n",
      "    raise converted from None\n",
      "pyspark.errors.exceptions.captured.ParseException: \n",
      "[PARSE_SYNTAX_ERROR] Syntax error at or near 'as'.(line 3, pos 14)\n",
      "\n",
      "== SQL ==\n",
      "\n",
      "SELECT \n",
      "    None(None as string) as customer_number, \n",
      "--------------^^^\n",
      "    customerName as customer_name,\n",
      "    None(None, None) as contact_name, \n",
      "    phone as phone, \n",
      "    addressLine1 as address_line_1, \n",
      "    addressLine2 as address_line_2, \n",
      "    postalCode as postal_code, \n",
      "    city as city, \n",
      "    state as state, \n",
      "    country as country,\n",
      "    creditLimit as credit_limit\n",
      "FROM customers\n",
      "\n",
      "\n"
     ]
    }
   ],
   "source": [
    "select_query_customers = \"\"\"\n",
    "SELECT \n",
    "    cast(customerNumber as string) as customer_number, \n",
    "    customerName as customer_name,\n",
    "    concat(contactFirstName, contactLastName) as contact_name, \n",
    "    phone as phone, \n",
    "    addressLine1 as address_line_1, \n",
    "    addressLine2 as address_line_2, \n",
    "    postalCode as postal_code, \n",
    "    city as city, \n",
    "    state as state, \n",
    "    country as country,\n",
    "    creditLimit as credit_limit\n",
    "FROM customers\n",
    "\"\"\"\n",
    "\n",
    "dim_customers_df = spark.sql(select_query_customers)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "4.3.2. Now, with the resulting `dim_customers_df` DataFrame:\n",
    "- Call the `surrogateUDF` to generate a surrogate key based on the `customer_number`. You will need to use `array()` function to convert it to an array. \n",
    "- Add the surrogate key using the `withColumn()` function, call the new column `customer_key`. \n",
    "\n",
    "Then perform a select to grab the columns related to the ERM diagram."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 62,
   "metadata": {
    "execution": {
     "iopub.execute_input": "2025-11-29T18:43:33.025431Z",
     "iopub.status.busy": "2025-11-29T18:43:33.025248Z",
     "iopub.status.idle": "2025-11-29T18:43:33.075442Z",
     "shell.execute_reply": "2025-11-29T18:43:33.074477Z",
     "shell.execute_reply.started": "2025-11-29T18:43:33.025409Z"
    },
    "exercise": [
     "ex02"
    ],
    "tags": [
     "graded"
    ]
   },
   "outputs": [
    {
     "data": {
      "application/vnd.jupyter.widget-view+json": {
       "model_id": "2ae9f3a502b544a4b5d6b32bb1305a99",
       "version_major": 2,
       "version_minor": 0
      },
      "text/plain": [
       "VBox()"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "data": {
      "application/vnd.jupyter.widget-view+json": {
       "model_id": "",
       "version_major": 2,
       "version_minor": 0
      },
      "text/plain": [
       "FloatProgress(value=0.0, bar_style='info', description='Progress:', layout=Layout(height='25px', width='50%'),\u2026"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "An error was encountered:\n",
      "name 'dim_customers_df' is not defined\n",
      "Traceback (most recent call last):\n",
      "NameError: name 'dim_customers_df' is not defined\n",
      "\n"
     ]
    }
   ],
   "source": [
    "dim_customers_df = dim_customers_df.withColumn(\"customer_key\", surrogateUDF(array(\"customer_number\")))\\\n",
    ".select([\"customer_key\",\"customer_name\",\"contact_name\",\"phone\",\"address_line_1\",\"address_line_2\",\"postal_code\",\"city\",\"state\",\"country\",\"credit_limit\"])"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "4.3.3. Now, store the final DataFrame into the `classicmodels_star_schema` schema, creating a new table called `dim_customers`."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 63,
   "metadata": {
    "execution": {
     "iopub.execute_input": "2025-11-29T18:43:33.077052Z",
     "iopub.status.busy": "2025-11-29T18:43:33.076788Z",
     "iopub.status.idle": "2025-11-29T18:43:33.134368Z",
     "shell.execute_reply": "2025-11-29T18:43:33.131208Z",
     "shell.execute_reply.started": "2025-11-29T18:43:33.077015Z"
    },
    "exercise": [
     "ex03"
    ],
    "tags": [
     "graded"
    ]
   },
   "outputs": [
    {
     "data": {
      "application/vnd.jupyter.widget-view+json": {
       "model_id": "5743e5f4ac02440ea15c4315a690a607",
       "version_major": 2,
       "version_minor": 0
      },
      "text/plain": [
       "VBox()"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "data": {
      "application/vnd.jupyter.widget-view+json": {
       "model_id": "",
       "version_major": 2,
       "version_minor": 0
      },
      "text/plain": [
       "FloatProgress(value=0.0, bar_style='info', description='Progress:', layout=Layout(height='25px', width='50%'),\u2026"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "An error was encountered:\n",
      "name 'dim_customers_df' is not defined\n",
      "Traceback (most recent call last):\n",
      "NameError: name 'dim_customers_df' is not defined\n",
      "\n"
     ]
    }
   ],
   "source": [
    "dim_customers_df.write.jdbc(url=jdbc_url, table=\"classicmodels_star_schema.dim_customers\", mode=\"overwrite\", properties=jdbc_properties)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "4.3.4. Check that the table is stored in the schema:"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 64,
   "metadata": {
    "execution": {
     "iopub.execute_input": "2025-11-29T18:43:33.135810Z",
     "iopub.status.busy": "2025-11-29T18:43:33.135560Z",
     "iopub.status.idle": "2025-11-29T18:43:33.389532Z",
     "shell.execute_reply": "2025-11-29T18:43:33.388789Z",
     "shell.execute_reply.started": "2025-11-29T18:43:33.135774Z"
    },
    "exercise": [
     "ex04"
    ],
    "tags": [
     "graded"
    ]
   },
   "outputs": [
    {
     "data": {
      "application/vnd.jupyter.widget-view+json": {
       "model_id": "f793c54b9d60472480ce4eff504eefb4",
       "version_major": 2,
       "version_minor": 0
      },
      "text/plain": [
       "VBox()"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "data": {
      "application/vnd.jupyter.widget-view+json": {
       "model_id": "",
       "version_major": 2,
       "version_minor": 0
      },
      "text/plain": [
       "FloatProgress(value=0.0, bar_style='info', description='Progress:', layout=Layout(height='25px', width='50%'),\u2026"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "An error was encountered:\n",
      "An error occurred while calling o158.jdbc.\n",
      ": org.postgresql.util.PSQLException: The connection attempt failed.\n",
      "\tat org.postgresql.core.v3.ConnectionFactoryImpl.openConnectionImpl(ConnectionFactoryImpl.java:292)\n",
      "\tat org.postgresql.core.ConnectionFactory.openConnection(ConnectionFactory.java:49)\n",
      "\tat org.postgresql.jdbc.PgConnection.<init>(PgConnection.java:195)\n",
      "\tat org.postgresql.Driver.makeConnection(Driver.java:454)\n",
      "\tat org.postgresql.Driver.connect(Driver.java:256)\n",
      "\tat org.apache.spark.sql.execution.datasources.jdbc.connection.BasicConnectionProvider.getConnection(BasicConnectionProvider.scala:49)\n",
      "\tat org.apache.spark.sql.execution.datasources.jdbc.connection.ConnectionProviderBase.create(ConnectionProvider.scala:102)\n",
      "\tat org.apache.spark.sql.jdbc.JdbcDialect.$anonfun$createConnectionFactory$1(JdbcDialects.scala:160)\n",
      "\tat org.apache.spark.sql.jdbc.JdbcDialect.$anonfun$createConnectionFactory$1$adapted(JdbcDialects.scala:156)\n",
      "\tat org.apache.spark.sql.execution.datasources.jdbc.JDBCRDD$.getQueryOutputSchema(JDBCRDD.scala:63)\n",
      "\tat org.apache.spark.sql.execution.datasources.jdbc.JDBCRDD$.resolveTable(JDBCRDD.scala:58)\n",
      "\tat org.apache.spark.sql.execution.datasources.jdbc.JDBCRelation$.getSchema(JDBCRelation.scala:241)\n",
      "\tat org.apache.spark.sql.execution.datasources.jdbc.JdbcRelationProvider.createRelation(JdbcRelationProvider.scala:37)\n",
      "\tat org.apache.spark.sql.execution.datasources.DataSource.resolveRelation(DataSource.scala:346)\n",
      "\tat org.apache.spark.sql.DataFrameReader.loadV1Source(DataFrameReader.scala:229)\n",
      "\tat org.apache.spark.sql.DataFrameReader.$anonfun$load$2(DataFrameReader.scala:211)\n",
      "\tat scala.Option.getOrElse(Option.scala:189)\n",
      "\tat org.apache.spark.sql.DataFrameReader.load(DataFrameReader.scala:211)\n",
      "\tat org.apache.spark.sql.DataFrameReader.load(DataFrameReader.scala:172)\n",
      "\tat org.apache.spark.sql.DataFrameReader.jdbc(DataFrameReader.scala:249)\n",
      "\tat java.base/jdk.internal.reflect.NativeMethodAccessorImpl.invoke0(Native Method)\n",
      "\tat java.base/jdk.internal.reflect.NativeMethodAccessorImpl.invoke(NativeMethodAccessorImpl.java:77)\n",
      "\tat java.base/jdk.internal.reflect.DelegatingMethodAccessorImpl.invoke(DelegatingMethodAccessorImpl.java:43)\n",
      "\tat java.base/java.lang.reflect.Method.invoke(Method.java:569)\n",
      "\tat py4j.reflection.MethodInvoker.invoke(MethodInvoker.java:244)\n",
      "\tat py4j.reflection.ReflectionEngine.invoke(ReflectionEngine.java:374)\n",
      "\tat py4j.Gateway.invoke(Gateway.java:282)\n",
      "\tat py4j.commands.AbstractCommand.invokeMethod(AbstractCommand.java:132)\n",
      "\tat py4j.commands.CallCommand.execute(CallCommand.java:79)\n",
      "\tat py4j.GatewayConnection.run(GatewayConnection.java:238)\n",
      "\tat java.base/java.lang.Thread.run(Thread.java:840)\n",
      "Caused by: java.net.UnknownHostException: <RDS-ENDPOINT>\n",
      "\tat java.base/sun.nio.ch.NioSocketImpl.connect(NioSocketImpl.java:572)\n",
      "\tat java.base/java.net.SocksSocketImpl.connect(SocksSocketImpl.java:327)\n",
      "\tat java.base/java.net.Socket.connect(Socket.java:633)\n",
      "\tat org.postgresql.core.PGStream.<init>(PGStream.java:70)\n",
      "\tat org.postgresql.core.v3.ConnectionFactoryImpl.tryConnect(ConnectionFactoryImpl.java:91)\n",
      "\tat org.postgresql.core.v3.ConnectionFactoryImpl.openConnectionImpl(ConnectionFactoryImpl.java:192)\n",
      "\t... 30 more\n",
      "\n",
      "Traceback (most recent call last):\n",
      "  File \"/mnt1/yarn/usercache/livy/appcache/application_1764439453312_0002/container_1764439453312_0002_01_000001/pyspark.zip/pyspark/sql/readwriter.py\", line 946, in jdbc\n",
      "    return self._df(self._jreader.jdbc(url, table, jprop))\n",
      "  File \"/mnt1/yarn/usercache/livy/appcache/application_1764439453312_0002/container_1764439453312_0002_01_000001/py4j-0.10.9.7-src.zip/py4j/java_gateway.py\", line 1322, in __call__\n",
      "    return_value = get_return_value(\n",
      "  File \"/mnt1/yarn/usercache/livy/appcache/application_1764439453312_0002/container_1764439453312_0002_01_000001/pyspark.zip/pyspark/errors/exceptions/captured.py\", line 179, in deco\n",
      "    return f(*a, **kw)\n",
      "  File \"/mnt1/yarn/usercache/livy/appcache/application_1764439453312_0002/container_1764439453312_0002_01_000001/py4j-0.10.9.7-src.zip/py4j/protocol.py\", line 326, in get_return_value\n",
      "    raise Py4JJavaError(\n",
      "py4j.protocol.Py4JJavaError: An error occurred while calling o158.jdbc.\n",
      ": org.postgresql.util.PSQLException: The connection attempt failed.\n",
      "\tat org.postgresql.core.v3.ConnectionFactoryImpl.openConnectionImpl(ConnectionFactoryImpl.java:292)\n",
      "\tat org.postgresql.core.ConnectionFactory.openConnection(ConnectionFactory.java:49)\n",
      "\tat org.postgresql.jdbc.PgConnection.<init>(PgConnection.java:195)\n",
      "\tat org.postgresql.Driver.makeConnection(Driver.java:454)\n",
      "\tat org.postgresql.Driver.connect(Driver.java:256)\n",
      "\tat org.apache.spark.sql.execution.datasources.jdbc.connection.BasicConnectionProvider.getConnection(BasicConnectionProvider.scala:49)\n",
      "\tat org.apache.spark.sql.execution.datasources.jdbc.connection.ConnectionProviderBase.create(ConnectionProvider.scala:102)\n",
      "\tat org.apache.spark.sql.jdbc.JdbcDialect.$anonfun$createConnectionFactory$1(JdbcDialects.scala:160)\n",
      "\tat org.apache.spark.sql.jdbc.JdbcDialect.$anonfun$createConnectionFactory$1$adapted(JdbcDialects.scala:156)\n",
      "\tat org.apache.spark.sql.execution.datasources.jdbc.JDBCRDD$.getQueryOutputSchema(JDBCRDD.scala:63)\n",
      "\tat org.apache.spark.sql.execution.datasources.jdbc.JDBCRDD$.resolveTable(JDBCRDD.scala:58)\n",
      "\tat org.apache.spark.sql.execution.datasources.jdbc.JDBCRelation$.getSchema(JDBCRelation.scala:241)\n",
      "\tat org.apache.spark.sql.execution.datasources.jdbc.JdbcRelationProvider.createRelation(JdbcRelationProvider.scala:37)\n",
      "\tat org.apache.spark.sql.execution.datasources.DataSource.resolveRelation(DataSource.scala:346)\n",
      "\tat org.apache.spark.sql.DataFrameReader.loadV1Source(DataFrameReader.scala:229)\n",
      "\tat org.apache.spark.sql.DataFrameReader.$anonfun$load$2(DataFrameReader.scala:211)\n",
      "\tat scala.Option.getOrElse(Option.scala:189)\n",
      "\tat org.apache.spark.sql.DataFrameReader.load(DataFrameReader.scala:211)\n",
      "\tat org.apache.spark.sql.DataFrameReader.load(DataFrameReader.scala:172)\n",
      "\tat org.apache.spark.sql.DataFrameReader.jdbc(DataFrameReader.scala:249)\n",
      "\tat java.base/jdk.internal.reflect.NativeMethodAccessorImpl.invoke0(Native Method)\n",
      "\tat java.base/jdk.internal.reflect.NativeMethodAccessorImpl.invoke(NativeMethodAccessorImpl.java:77)\n",
      "\tat java.base/jdk.internal.reflect.DelegatingMethodAccessorImpl.invoke(DelegatingMethodAccessorImpl.java:43)\n",
      "\tat java.base/java.lang.reflect.Method.invoke(Method.java:569)\n",
      "\tat py4j.reflection.MethodInvoker.invoke(MethodInvoker.java:244)\n",
      "\tat py4j.reflection.ReflectionEngine.invoke(ReflectionEngine.java:374)\n",
      "\tat py4j.Gateway.invoke(Gateway.java:282)\n",
      "\tat py4j.commands.AbstractCommand.invokeMethod(AbstractCommand.java:132)\n",
      "\tat py4j.commands.CallCommand.execute(CallCommand.java:79)\n",
      "\tat py4j.GatewayConnection.run(GatewayConnection.java:238)\n",
      "\tat java.base/java.lang.Thread.run(Thread.java:840)\n",
      "Caused by: java.net.UnknownHostException: <RDS-ENDPOINT>\n",
      "\tat java.base/sun.nio.ch.NioSocketImpl.connect(NioSocketImpl.java:572)\n",
      "\tat java.base/java.net.SocksSocketImpl.connect(SocksSocketImpl.java:327)\n",
      "\tat java.base/java.net.Socket.connect(Socket.java:633)\n",
      "\tat org.postgresql.core.PGStream.<init>(PGStream.java:70)\n",
      "\tat org.postgresql.core.v3.ConnectionFactoryImpl.tryConnect(ConnectionFactoryImpl.java:91)\n",
      "\tat org.postgresql.core.v3.ConnectionFactoryImpl.openConnectionImpl(ConnectionFactoryImpl.java:192)\n",
      "\t... 30 more\n",
      "\n",
      "\n"
     ]
    }
   ],
   "source": [
    "dim_customers_df_check = spark.read.jdbc(url=jdbc_url, table=\"classicmodels_star_schema.dim_customers\", properties=jdbc_properties)\n",
    "\n",
    "print(\"dim_customers column names: \", dim_customers_df_check.columns)\n",
    "\n",
    "dim_customers_row_count = dim_customers_df_check.count()\n",
    "print(\"dim_customers number of rows: \", dim_customers_row_count)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "##### __Expected Output__\n",
    "\n",
    "```\n",
    "dim_customers column names:  ['customer_key', 'customer_name', 'contact_name', 'phone', 'address_line_1', 'address_line_2', 'postal_code', 'city', 'state', 'country', 'credit_limit']\n",
    "dim_customers number of rows:  122\n",
    "```"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "<a name='4.4'></a>\n",
    "### 4.4 - Products Dimension\n",
    "\n",
    "Continue with the `products` dimension. "
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "4.4.1. Create a SQL query that brings the relevant columns from the `products` temporal view and stores the query result in a Spark DataFrame. Later you will create a surrogate key `product_key` based on the `productCode`. The `productCode` is already a string, so you don't have to cast it - just select it for now and name as `product_code` for consistency."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 65,
   "metadata": {
    "execution": {
     "iopub.execute_input": "2025-11-29T18:43:33.390680Z",
     "iopub.status.busy": "2025-11-29T18:43:33.390449Z",
     "iopub.status.idle": "2025-11-29T18:43:33.438269Z",
     "shell.execute_reply": "2025-11-29T18:43:33.437492Z",
     "shell.execute_reply.started": "2025-11-29T18:43:33.390656Z"
    },
    "exercise": [
     "ex05"
    ],
    "tags": [
     "graded"
    ]
   },
   "outputs": [
    {
     "data": {
      "application/vnd.jupyter.widget-view+json": {
       "model_id": "2f5088f6ebaf4c2bba204d6363a45447",
       "version_major": 2,
       "version_minor": 0
      },
      "text/plain": [
       "VBox()"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "data": {
      "application/vnd.jupyter.widget-view+json": {
       "model_id": "",
       "version_major": 2,
       "version_minor": 0
      },
      "text/plain": [
       "FloatProgress(value=0.0, bar_style='info', description='Progress:', layout=Layout(height='25px', width='50%'),\u2026"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "An error was encountered:\n",
      "invalid syntax (<stdin>, line 14)\n",
      "  File \"<stdin>\", line 14\n",
      "    dim_products_df = None.None(None)\n",
      "                           ^\n",
      "SyntaxError: invalid syntax\n",
      "\n"
     ]
    }
   ],
   "source": [
    "select_query_products = \"\"\"\n",
    "SELECT \n",
    "    products.productCode as product_code, \n",
    "    productName as product_name, \n",
    "    products.productLine as product_line, \n",
    "    productScale as product_scale, \n",
    "    productVendor as product_vendor,\n",
    "    productDescription as product_description, \n",
    "    textDescription as product_line_description\n",
    "FROM products\n",
    "JOIN productlines ON products.productLine=productlines.productLine\n",
    "\"\"\"\n",
    "\n",
    "dim_products_df = spark.sql(select_query_products)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "4.4.2. With the resulting `dim_products_df` DataFrame:\n",
    "- Call the `surrogateUDF` to generate a surrogate key based on the `product_code`. You will need to use `array()` function to convert it to an array. \n",
    "- Add the surrogate key using the `withColumn()` function, call the new column `product_key`. \n",
    "\n",
    "Then perform a select to grab the columns related to the ERM diagram."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 66,
   "metadata": {
    "execution": {
     "iopub.execute_input": "2025-11-29T18:43:33.440135Z",
     "iopub.status.busy": "2025-11-29T18:43:33.439699Z",
     "iopub.status.idle": "2025-11-29T18:43:33.497578Z",
     "shell.execute_reply": "2025-11-29T18:43:33.494967Z",
     "shell.execute_reply.started": "2025-11-29T18:43:33.440097Z"
    },
    "exercise": [
     "ex06"
    ],
    "tags": [
     "graded"
    ]
   },
   "outputs": [
    {
     "data": {
      "application/vnd.jupyter.widget-view+json": {
       "model_id": "c36804c921b94f4589892ba523194634",
       "version_major": 2,
       "version_minor": 0
      },
      "text/plain": [
       "VBox()"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "data": {
      "application/vnd.jupyter.widget-view+json": {
       "model_id": "",
       "version_major": 2,
       "version_minor": 0
      },
      "text/plain": [
       "FloatProgress(value=0.0, bar_style='info', description='Progress:', layout=Layout(height='25px', width='50%'),\u2026"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "An error was encountered:\n",
      "name 'dim_products_df' is not defined\n",
      "Traceback (most recent call last):\n",
      "NameError: name 'dim_products_df' is not defined\n",
      "\n"
     ]
    }
   ],
   "source": [
    "dim_products_df = dim_products_df.withColumn(\"product_key\", surrogateUDF(array(\"product_code\")))\\\n",
    ".select([\"product_key\",\"product_name\",\"product_line\",\"product_scale\",\"product_vendor\",\"product_description\",\"product_line_description\"])"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "4.4.3. Store the `dim_products_df` DataFrame into the `classicmodels_star_schema` schema, table `dim_products` (see how it was done in the step 4.3.3)."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 67,
   "metadata": {
    "execution": {
     "iopub.execute_input": "2025-11-29T18:43:33.504208Z",
     "iopub.status.busy": "2025-11-29T18:43:33.501247Z",
     "iopub.status.idle": "2025-11-29T18:43:33.557249Z",
     "shell.execute_reply": "2025-11-29T18:43:33.556240Z",
     "shell.execute_reply.started": "2025-11-29T18:43:33.504167Z"
    },
    "exercise": [
     "ex07"
    ],
    "tags": [
     "graded"
    ]
   },
   "outputs": [
    {
     "data": {
      "application/vnd.jupyter.widget-view+json": {
       "model_id": "0083dbafc9ab4556aae0523e43c52049",
       "version_major": 2,
       "version_minor": 0
      },
      "text/plain": [
       "VBox()"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "data": {
      "application/vnd.jupyter.widget-view+json": {
       "model_id": "",
       "version_major": 2,
       "version_minor": 0
      },
      "text/plain": [
       "FloatProgress(value=0.0, bar_style='info', description='Progress:', layout=Layout(height='25px', width='50%'),\u2026"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "An error was encountered:\n",
      "name 'dim_products_df' is not defined\n",
      "Traceback (most recent call last):\n",
      "NameError: name 'dim_products_df' is not defined\n",
      "\n"
     ]
    }
   ],
   "source": [
    "dim_products_df.write.jdbc(url=jdbc_url, table=\"classicmodels_star_schema.dim_products\", mode=\"overwrite\", properties=jdbc_properties)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "4.4.4. Check your work:"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 68,
   "metadata": {
    "execution": {
     "iopub.execute_input": "2025-11-29T18:43:33.569107Z",
     "iopub.status.busy": "2025-11-29T18:43:33.568304Z",
     "iopub.status.idle": "2025-11-29T18:43:33.827630Z",
     "shell.execute_reply": "2025-11-29T18:43:33.826901Z",
     "shell.execute_reply.started": "2025-11-29T18:43:33.569060Z"
    },
    "exercise": [
     "ex08"
    ],
    "tags": [
     "graded"
    ]
   },
   "outputs": [
    {
     "data": {
      "application/vnd.jupyter.widget-view+json": {
       "model_id": "064b339668ab4036873822087ea77b34",
       "version_major": 2,
       "version_minor": 0
      },
      "text/plain": [
       "VBox()"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "data": {
      "application/vnd.jupyter.widget-view+json": {
       "model_id": "",
       "version_major": 2,
       "version_minor": 0
      },
      "text/plain": [
       "FloatProgress(value=0.0, bar_style='info', description='Progress:', layout=Layout(height='25px', width='50%'),\u2026"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "An error was encountered:\n",
      "An error occurred while calling o162.jdbc.\n",
      ": org.postgresql.util.PSQLException: The connection attempt failed.\n",
      "\tat org.postgresql.core.v3.ConnectionFactoryImpl.openConnectionImpl(ConnectionFactoryImpl.java:292)\n",
      "\tat org.postgresql.core.ConnectionFactory.openConnection(ConnectionFactory.java:49)\n",
      "\tat org.postgresql.jdbc.PgConnection.<init>(PgConnection.java:195)\n",
      "\tat org.postgresql.Driver.makeConnection(Driver.java:454)\n",
      "\tat org.postgresql.Driver.connect(Driver.java:256)\n",
      "\tat org.apache.spark.sql.execution.datasources.jdbc.connection.BasicConnectionProvider.getConnection(BasicConnectionProvider.scala:49)\n",
      "\tat org.apache.spark.sql.execution.datasources.jdbc.connection.ConnectionProviderBase.create(ConnectionProvider.scala:102)\n",
      "\tat org.apache.spark.sql.jdbc.JdbcDialect.$anonfun$createConnectionFactory$1(JdbcDialects.scala:160)\n",
      "\tat org.apache.spark.sql.jdbc.JdbcDialect.$anonfun$createConnectionFactory$1$adapted(JdbcDialects.scala:156)\n",
      "\tat org.apache.spark.sql.execution.datasources.jdbc.JDBCRDD$.getQueryOutputSchema(JDBCRDD.scala:63)\n",
      "\tat org.apache.spark.sql.execution.datasources.jdbc.JDBCRDD$.resolveTable(JDBCRDD.scala:58)\n",
      "\tat org.apache.spark.sql.execution.datasources.jdbc.JDBCRelation$.getSchema(JDBCRelation.scala:241)\n",
      "\tat org.apache.spark.sql.execution.datasources.jdbc.JdbcRelationProvider.createRelation(JdbcRelationProvider.scala:37)\n",
      "\tat org.apache.spark.sql.execution.datasources.DataSource.resolveRelation(DataSource.scala:346)\n",
      "\tat org.apache.spark.sql.DataFrameReader.loadV1Source(DataFrameReader.scala:229)\n",
      "\tat org.apache.spark.sql.DataFrameReader.$anonfun$load$2(DataFrameReader.scala:211)\n",
      "\tat scala.Option.getOrElse(Option.scala:189)\n",
      "\tat org.apache.spark.sql.DataFrameReader.load(DataFrameReader.scala:211)\n",
      "\tat org.apache.spark.sql.DataFrameReader.load(DataFrameReader.scala:172)\n",
      "\tat org.apache.spark.sql.DataFrameReader.jdbc(DataFrameReader.scala:249)\n",
      "\tat java.base/jdk.internal.reflect.NativeMethodAccessorImpl.invoke0(Native Method)\n",
      "\tat java.base/jdk.internal.reflect.NativeMethodAccessorImpl.invoke(NativeMethodAccessorImpl.java:77)\n",
      "\tat java.base/jdk.internal.reflect.DelegatingMethodAccessorImpl.invoke(DelegatingMethodAccessorImpl.java:43)\n",
      "\tat java.base/java.lang.reflect.Method.invoke(Method.java:569)\n",
      "\tat py4j.reflection.MethodInvoker.invoke(MethodInvoker.java:244)\n",
      "\tat py4j.reflection.ReflectionEngine.invoke(ReflectionEngine.java:374)\n",
      "\tat py4j.Gateway.invoke(Gateway.java:282)\n",
      "\tat py4j.commands.AbstractCommand.invokeMethod(AbstractCommand.java:132)\n",
      "\tat py4j.commands.CallCommand.execute(CallCommand.java:79)\n",
      "\tat py4j.GatewayConnection.run(GatewayConnection.java:238)\n",
      "\tat java.base/java.lang.Thread.run(Thread.java:840)\n",
      "Caused by: java.net.UnknownHostException: <RDS-ENDPOINT>\n",
      "\tat java.base/sun.nio.ch.NioSocketImpl.connect(NioSocketImpl.java:572)\n",
      "\tat java.base/java.net.SocksSocketImpl.connect(SocksSocketImpl.java:327)\n",
      "\tat java.base/java.net.Socket.connect(Socket.java:633)\n",
      "\tat org.postgresql.core.PGStream.<init>(PGStream.java:70)\n",
      "\tat org.postgresql.core.v3.ConnectionFactoryImpl.tryConnect(ConnectionFactoryImpl.java:91)\n",
      "\tat org.postgresql.core.v3.ConnectionFactoryImpl.openConnectionImpl(ConnectionFactoryImpl.java:192)\n",
      "\t... 30 more\n",
      "\n",
      "Traceback (most recent call last):\n",
      "  File \"/mnt1/yarn/usercache/livy/appcache/application_1764439453312_0002/container_1764439453312_0002_01_000001/pyspark.zip/pyspark/sql/readwriter.py\", line 946, in jdbc\n",
      "    return self._df(self._jreader.jdbc(url, table, jprop))\n",
      "  File \"/mnt1/yarn/usercache/livy/appcache/application_1764439453312_0002/container_1764439453312_0002_01_000001/py4j-0.10.9.7-src.zip/py4j/java_gateway.py\", line 1322, in __call__\n",
      "    return_value = get_return_value(\n",
      "  File \"/mnt1/yarn/usercache/livy/appcache/application_1764439453312_0002/container_1764439453312_0002_01_000001/pyspark.zip/pyspark/errors/exceptions/captured.py\", line 179, in deco\n",
      "    return f(*a, **kw)\n",
      "  File \"/mnt1/yarn/usercache/livy/appcache/application_1764439453312_0002/container_1764439453312_0002_01_000001/py4j-0.10.9.7-src.zip/py4j/protocol.py\", line 326, in get_return_value\n",
      "    raise Py4JJavaError(\n",
      "py4j.protocol.Py4JJavaError: An error occurred while calling o162.jdbc.\n",
      ": org.postgresql.util.PSQLException: The connection attempt failed.\n",
      "\tat org.postgresql.core.v3.ConnectionFactoryImpl.openConnectionImpl(ConnectionFactoryImpl.java:292)\n",
      "\tat org.postgresql.core.ConnectionFactory.openConnection(ConnectionFactory.java:49)\n",
      "\tat org.postgresql.jdbc.PgConnection.<init>(PgConnection.java:195)\n",
      "\tat org.postgresql.Driver.makeConnection(Driver.java:454)\n",
      "\tat org.postgresql.Driver.connect(Driver.java:256)\n",
      "\tat org.apache.spark.sql.execution.datasources.jdbc.connection.BasicConnectionProvider.getConnection(BasicConnectionProvider.scala:49)\n",
      "\tat org.apache.spark.sql.execution.datasources.jdbc.connection.ConnectionProviderBase.create(ConnectionProvider.scala:102)\n",
      "\tat org.apache.spark.sql.jdbc.JdbcDialect.$anonfun$createConnectionFactory$1(JdbcDialects.scala:160)\n",
      "\tat org.apache.spark.sql.jdbc.JdbcDialect.$anonfun$createConnectionFactory$1$adapted(JdbcDialects.scala:156)\n",
      "\tat org.apache.spark.sql.execution.datasources.jdbc.JDBCRDD$.getQueryOutputSchema(JDBCRDD.scala:63)\n",
      "\tat org.apache.spark.sql.execution.datasources.jdbc.JDBCRDD$.resolveTable(JDBCRDD.scala:58)\n",
      "\tat org.apache.spark.sql.execution.datasources.jdbc.JDBCRelation$.getSchema(JDBCRelation.scala:241)\n",
      "\tat org.apache.spark.sql.execution.datasources.jdbc.JdbcRelationProvider.createRelation(JdbcRelationProvider.scala:37)\n",
      "\tat org.apache.spark.sql.execution.datasources.DataSource.resolveRelation(DataSource.scala:346)\n",
      "\tat org.apache.spark.sql.DataFrameReader.loadV1Source(DataFrameReader.scala:229)\n",
      "\tat org.apache.spark.sql.DataFrameReader.$anonfun$load$2(DataFrameReader.scala:211)\n",
      "\tat scala.Option.getOrElse(Option.scala:189)\n",
      "\tat org.apache.spark.sql.DataFrameReader.load(DataFrameReader.scala:211)\n",
      "\tat org.apache.spark.sql.DataFrameReader.load(DataFrameReader.scala:172)\n",
      "\tat org.apache.spark.sql.DataFrameReader.jdbc(DataFrameReader.scala:249)\n",
      "\tat java.base/jdk.internal.reflect.NativeMethodAccessorImpl.invoke0(Native Method)\n",
      "\tat java.base/jdk.internal.reflect.NativeMethodAccessorImpl.invoke(NativeMethodAccessorImpl.java:77)\n",
      "\tat java.base/jdk.internal.reflect.DelegatingMethodAccessorImpl.invoke(DelegatingMethodAccessorImpl.java:43)\n",
      "\tat java.base/java.lang.reflect.Method.invoke(Method.java:569)\n",
      "\tat py4j.reflection.MethodInvoker.invoke(MethodInvoker.java:244)\n",
      "\tat py4j.reflection.ReflectionEngine.invoke(ReflectionEngine.java:374)\n",
      "\tat py4j.Gateway.invoke(Gateway.java:282)\n",
      "\tat py4j.commands.AbstractCommand.invokeMethod(AbstractCommand.java:132)\n",
      "\tat py4j.commands.CallCommand.execute(CallCommand.java:79)\n",
      "\tat py4j.GatewayConnection.run(GatewayConnection.java:238)\n",
      "\tat java.base/java.lang.Thread.run(Thread.java:840)\n",
      "Caused by: java.net.UnknownHostException: <RDS-ENDPOINT>\n",
      "\tat java.base/sun.nio.ch.NioSocketImpl.connect(NioSocketImpl.java:572)\n",
      "\tat java.base/java.net.SocksSocketImpl.connect(SocksSocketImpl.java:327)\n",
      "\tat java.base/java.net.Socket.connect(Socket.java:633)\n",
      "\tat org.postgresql.core.PGStream.<init>(PGStream.java:70)\n",
      "\tat org.postgresql.core.v3.ConnectionFactoryImpl.tryConnect(ConnectionFactoryImpl.java:91)\n",
      "\tat org.postgresql.core.v3.ConnectionFactoryImpl.openConnectionImpl(ConnectionFactoryImpl.java:192)\n",
      "\t... 30 more\n",
      "\n",
      "\n"
     ]
    }
   ],
   "source": [
    "dim_products_df_check = spark.read.jdbc(url=jdbc_url, table=\"classicmodels_star_schema.dim_products\", properties=jdbc_properties)\n",
    "\n",
    "print(\"dim_products column names: \", dim_products_df_check.columns)\n",
    "\n",
    "dim_products_row_count = dim_products_df_check.count()\n",
    "print(\"dim_products number of rows: \", dim_products_row_count)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "##### __Expected Output__\n",
    "\n",
    "```\n",
    "dim_products column names:  ['product_key', 'product_name', 'product_line', 'product_scale', 'product_vendor', 'product_description', 'product_line_description']\n",
    "dim_products number of rows:  110\n",
    "```"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "<a name='4.5'></a>\n",
    "### 4.5 - Offices Dimension\n",
    "\n",
    "Now, let's proceed with the `offices` dimension."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "4.5.1. Create a SQL query that brings the relevant columns from the `offices` temporal table and stores the query result in a Spark dataframe. Later you will create a surrogate key `office_key` based on the `officeCode`. The `officeCode` is already a string, so you don't have to cast it - just select it for now and name as `office_code` for consistency."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 69,
   "metadata": {
    "execution": {
     "iopub.execute_input": "2025-11-29T18:43:33.829316Z",
     "iopub.status.busy": "2025-11-29T18:43:33.829142Z",
     "iopub.status.idle": "2025-11-29T18:43:33.876069Z",
     "shell.execute_reply": "2025-11-29T18:43:33.875466Z",
     "shell.execute_reply.started": "2025-11-29T18:43:33.829293Z"
    },
    "exercise": [
     "ex09"
    ],
    "tags": [
     "graded"
    ]
   },
   "outputs": [
    {
     "data": {
      "application/vnd.jupyter.widget-view+json": {
       "model_id": "629404a1c63948bfb24fa814dc1b2962",
       "version_major": 2,
       "version_minor": 0
      },
      "text/plain": [
       "VBox()"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "data": {
      "application/vnd.jupyter.widget-view+json": {
       "model_id": "",
       "version_major": 2,
       "version_minor": 0
      },
      "text/plain": [
       "FloatProgress(value=0.0, bar_style='info', description='Progress:', layout=Layout(height='25px', width='50%'),\u2026"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "An error was encountered:\n",
      "invalid syntax (<stdin>, line 12)\n",
      "  File \"<stdin>\", line 12\n",
      "    dim_offices_df = None.None(None)\n",
      "                          ^\n",
      "SyntaxError: invalid syntax\n",
      "\n"
     ]
    }
   ],
   "source": [
    "select_query_offices = \"\"\"\n",
    "SELECT \n",
    "    offices.officeCode as office_code, \n",
    "    postalCode as postal_code, \n",
    "    city as city, \n",
    "    state as state, \n",
    "    country as country, \n",
    "    territory as territory\n",
    "FROM offices\n",
    "\"\"\"\n",
    "\n",
    "dim_offices_df = spark.sql(select_query_offices)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "4.5.2. With the resulting `dim_offices_df` DataFrame:\n",
    "- Call the `surrogateUDF` to generate a surrogate key based on the `office_code`. You will need to use `array()` function to convert it to an array. \n",
    "- Add the surrogate key using the `withColumn()` function, call the new column `office_key`. \n",
    "\n",
    "Then perform a select to grab the columns related to the ERM diagram."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 70,
   "metadata": {
    "execution": {
     "iopub.execute_input": "2025-11-29T18:43:33.877655Z",
     "iopub.status.busy": "2025-11-29T18:43:33.876920Z",
     "iopub.status.idle": "2025-11-29T18:43:33.931135Z",
     "shell.execute_reply": "2025-11-29T18:43:33.930519Z",
     "shell.execute_reply.started": "2025-11-29T18:43:33.877626Z"
    },
    "exercise": [
     "ex10"
    ],
    "tags": [
     "graded"
    ]
   },
   "outputs": [
    {
     "data": {
      "application/vnd.jupyter.widget-view+json": {
       "model_id": "296a13103fe246cb89a133c99735b014",
       "version_major": 2,
       "version_minor": 0
      },
      "text/plain": [
       "VBox()"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "data": {
      "application/vnd.jupyter.widget-view+json": {
       "model_id": "",
       "version_major": 2,
       "version_minor": 0
      },
      "text/plain": [
       "FloatProgress(value=0.0, bar_style='info', description='Progress:', layout=Layout(height='25px', width='50%'),\u2026"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "An error was encountered:\n",
      "name 'dim_offices_df' is not defined\n",
      "Traceback (most recent call last):\n",
      "NameError: name 'dim_offices_df' is not defined\n",
      "\n"
     ]
    }
   ],
   "source": [
    "dim_offices_df = dim_offices_df.withColumn(\"office_key\", surrogateUDF(array(\"office_code\")))\\\n",
    ".select([\"office_key\",\"postal_code\",\"city\",\"state\",\"country\",\"territory\"])"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "4.5.3. Store the `dim_offices_df` DataFrame into the `classicmodels_star_schema` schema, table `dim_offices`."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 71,
   "metadata": {
    "execution": {
     "iopub.execute_input": "2025-11-29T18:43:33.932459Z",
     "iopub.status.busy": "2025-11-29T18:43:33.932218Z",
     "iopub.status.idle": "2025-11-29T18:43:33.987079Z",
     "shell.execute_reply": "2025-11-29T18:43:33.986344Z",
     "shell.execute_reply.started": "2025-11-29T18:43:33.932423Z"
    },
    "exercise": [
     "ex11"
    ],
    "tags": [
     "graded"
    ]
   },
   "outputs": [
    {
     "data": {
      "application/vnd.jupyter.widget-view+json": {
       "model_id": "f9078ad16ab243c9adfd592e9c50bfbe",
       "version_major": 2,
       "version_minor": 0
      },
      "text/plain": [
       "VBox()"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "data": {
      "application/vnd.jupyter.widget-view+json": {
       "model_id": "",
       "version_major": 2,
       "version_minor": 0
      },
      "text/plain": [
       "FloatProgress(value=0.0, bar_style='info', description='Progress:', layout=Layout(height='25px', width='50%'),\u2026"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "An error was encountered:\n",
      "name 'dim_offices_df' is not defined\n",
      "Traceback (most recent call last):\n",
      "NameError: name 'dim_offices_df' is not defined\n",
      "\n"
     ]
    }
   ],
   "source": [
    "dim_offices_df.write.jdbc(url=jdbc_url, table=\"classicmodels_star_schema.dim_offices\", mode=\"overwrite\", properties=jdbc_properties)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "4.5.4. Check your work:"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 72,
   "metadata": {
    "execution": {
     "iopub.execute_input": "2025-11-29T18:43:33.988713Z",
     "iopub.status.busy": "2025-11-29T18:43:33.988224Z",
     "iopub.status.idle": "2025-11-29T18:43:34.248343Z",
     "shell.execute_reply": "2025-11-29T18:43:34.247637Z",
     "shell.execute_reply.started": "2025-11-29T18:43:33.988673Z"
    },
    "exercise": [
     "ex12"
    ],
    "tags": [
     "graded"
    ]
   },
   "outputs": [
    {
     "data": {
      "application/vnd.jupyter.widget-view+json": {
       "model_id": "30171232b5454c5eaa1adb5023b01a43",
       "version_major": 2,
       "version_minor": 0
      },
      "text/plain": [
       "VBox()"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "data": {
      "application/vnd.jupyter.widget-view+json": {
       "model_id": "",
       "version_major": 2,
       "version_minor": 0
      },
      "text/plain": [
       "FloatProgress(value=0.0, bar_style='info', description='Progress:', layout=Layout(height='25px', width='50%'),\u2026"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "An error was encountered:\n",
      "An error occurred while calling o166.jdbc.\n",
      ": org.postgresql.util.PSQLException: The connection attempt failed.\n",
      "\tat org.postgresql.core.v3.ConnectionFactoryImpl.openConnectionImpl(ConnectionFactoryImpl.java:292)\n",
      "\tat org.postgresql.core.ConnectionFactory.openConnection(ConnectionFactory.java:49)\n",
      "\tat org.postgresql.jdbc.PgConnection.<init>(PgConnection.java:195)\n",
      "\tat org.postgresql.Driver.makeConnection(Driver.java:454)\n",
      "\tat org.postgresql.Driver.connect(Driver.java:256)\n",
      "\tat org.apache.spark.sql.execution.datasources.jdbc.connection.BasicConnectionProvider.getConnection(BasicConnectionProvider.scala:49)\n",
      "\tat org.apache.spark.sql.execution.datasources.jdbc.connection.ConnectionProviderBase.create(ConnectionProvider.scala:102)\n",
      "\tat org.apache.spark.sql.jdbc.JdbcDialect.$anonfun$createConnectionFactory$1(JdbcDialects.scala:160)\n",
      "\tat org.apache.spark.sql.jdbc.JdbcDialect.$anonfun$createConnectionFactory$1$adapted(JdbcDialects.scala:156)\n",
      "\tat org.apache.spark.sql.execution.datasources.jdbc.JDBCRDD$.getQueryOutputSchema(JDBCRDD.scala:63)\n",
      "\tat org.apache.spark.sql.execution.datasources.jdbc.JDBCRDD$.resolveTable(JDBCRDD.scala:58)\n",
      "\tat org.apache.spark.sql.execution.datasources.jdbc.JDBCRelation$.getSchema(JDBCRelation.scala:241)\n",
      "\tat org.apache.spark.sql.execution.datasources.jdbc.JdbcRelationProvider.createRelation(JdbcRelationProvider.scala:37)\n",
      "\tat org.apache.spark.sql.execution.datasources.DataSource.resolveRelation(DataSource.scala:346)\n",
      "\tat org.apache.spark.sql.DataFrameReader.loadV1Source(DataFrameReader.scala:229)\n",
      "\tat org.apache.spark.sql.DataFrameReader.$anonfun$load$2(DataFrameReader.scala:211)\n",
      "\tat scala.Option.getOrElse(Option.scala:189)\n",
      "\tat org.apache.spark.sql.DataFrameReader.load(DataFrameReader.scala:211)\n",
      "\tat org.apache.spark.sql.DataFrameReader.load(DataFrameReader.scala:172)\n",
      "\tat org.apache.spark.sql.DataFrameReader.jdbc(DataFrameReader.scala:249)\n",
      "\tat java.base/jdk.internal.reflect.NativeMethodAccessorImpl.invoke0(Native Method)\n",
      "\tat java.base/jdk.internal.reflect.NativeMethodAccessorImpl.invoke(NativeMethodAccessorImpl.java:77)\n",
      "\tat java.base/jdk.internal.reflect.DelegatingMethodAccessorImpl.invoke(DelegatingMethodAccessorImpl.java:43)\n",
      "\tat java.base/java.lang.reflect.Method.invoke(Method.java:569)\n",
      "\tat py4j.reflection.MethodInvoker.invoke(MethodInvoker.java:244)\n",
      "\tat py4j.reflection.ReflectionEngine.invoke(ReflectionEngine.java:374)\n",
      "\tat py4j.Gateway.invoke(Gateway.java:282)\n",
      "\tat py4j.commands.AbstractCommand.invokeMethod(AbstractCommand.java:132)\n",
      "\tat py4j.commands.CallCommand.execute(CallCommand.java:79)\n",
      "\tat py4j.GatewayConnection.run(GatewayConnection.java:238)\n",
      "\tat java.base/java.lang.Thread.run(Thread.java:840)\n",
      "Caused by: java.net.UnknownHostException: <RDS-ENDPOINT>\n",
      "\tat java.base/sun.nio.ch.NioSocketImpl.connect(NioSocketImpl.java:572)\n",
      "\tat java.base/java.net.SocksSocketImpl.connect(SocksSocketImpl.java:327)\n",
      "\tat java.base/java.net.Socket.connect(Socket.java:633)\n",
      "\tat org.postgresql.core.PGStream.<init>(PGStream.java:70)\n",
      "\tat org.postgresql.core.v3.ConnectionFactoryImpl.tryConnect(ConnectionFactoryImpl.java:91)\n",
      "\tat org.postgresql.core.v3.ConnectionFactoryImpl.openConnectionImpl(ConnectionFactoryImpl.java:192)\n",
      "\t... 30 more\n",
      "\n",
      "Traceback (most recent call last):\n",
      "  File \"/mnt1/yarn/usercache/livy/appcache/application_1764439453312_0002/container_1764439453312_0002_01_000001/pyspark.zip/pyspark/sql/readwriter.py\", line 946, in jdbc\n",
      "    return self._df(self._jreader.jdbc(url, table, jprop))\n",
      "  File \"/mnt1/yarn/usercache/livy/appcache/application_1764439453312_0002/container_1764439453312_0002_01_000001/py4j-0.10.9.7-src.zip/py4j/java_gateway.py\", line 1322, in __call__\n",
      "    return_value = get_return_value(\n",
      "  File \"/mnt1/yarn/usercache/livy/appcache/application_1764439453312_0002/container_1764439453312_0002_01_000001/pyspark.zip/pyspark/errors/exceptions/captured.py\", line 179, in deco\n",
      "    return f(*a, **kw)\n",
      "  File \"/mnt1/yarn/usercache/livy/appcache/application_1764439453312_0002/container_1764439453312_0002_01_000001/py4j-0.10.9.7-src.zip/py4j/protocol.py\", line 326, in get_return_value\n",
      "    raise Py4JJavaError(\n",
      "py4j.protocol.Py4JJavaError: An error occurred while calling o166.jdbc.\n",
      ": org.postgresql.util.PSQLException: The connection attempt failed.\n",
      "\tat org.postgresql.core.v3.ConnectionFactoryImpl.openConnectionImpl(ConnectionFactoryImpl.java:292)\n",
      "\tat org.postgresql.core.ConnectionFactory.openConnection(ConnectionFactory.java:49)\n",
      "\tat org.postgresql.jdbc.PgConnection.<init>(PgConnection.java:195)\n",
      "\tat org.postgresql.Driver.makeConnection(Driver.java:454)\n",
      "\tat org.postgresql.Driver.connect(Driver.java:256)\n",
      "\tat org.apache.spark.sql.execution.datasources.jdbc.connection.BasicConnectionProvider.getConnection(BasicConnectionProvider.scala:49)\n",
      "\tat org.apache.spark.sql.execution.datasources.jdbc.connection.ConnectionProviderBase.create(ConnectionProvider.scala:102)\n",
      "\tat org.apache.spark.sql.jdbc.JdbcDialect.$anonfun$createConnectionFactory$1(JdbcDialects.scala:160)\n",
      "\tat org.apache.spark.sql.jdbc.JdbcDialect.$anonfun$createConnectionFactory$1$adapted(JdbcDialects.scala:156)\n",
      "\tat org.apache.spark.sql.execution.datasources.jdbc.JDBCRDD$.getQueryOutputSchema(JDBCRDD.scala:63)\n",
      "\tat org.apache.spark.sql.execution.datasources.jdbc.JDBCRDD$.resolveTable(JDBCRDD.scala:58)\n",
      "\tat org.apache.spark.sql.execution.datasources.jdbc.JDBCRelation$.getSchema(JDBCRelation.scala:241)\n",
      "\tat org.apache.spark.sql.execution.datasources.jdbc.JdbcRelationProvider.createRelation(JdbcRelationProvider.scala:37)\n",
      "\tat org.apache.spark.sql.execution.datasources.DataSource.resolveRelation(DataSource.scala:346)\n",
      "\tat org.apache.spark.sql.DataFrameReader.loadV1Source(DataFrameReader.scala:229)\n",
      "\tat org.apache.spark.sql.DataFrameReader.$anonfun$load$2(DataFrameReader.scala:211)\n",
      "\tat scala.Option.getOrElse(Option.scala:189)\n",
      "\tat org.apache.spark.sql.DataFrameReader.load(DataFrameReader.scala:211)\n",
      "\tat org.apache.spark.sql.DataFrameReader.load(DataFrameReader.scala:172)\n",
      "\tat org.apache.spark.sql.DataFrameReader.jdbc(DataFrameReader.scala:249)\n",
      "\tat java.base/jdk.internal.reflect.NativeMethodAccessorImpl.invoke0(Native Method)\n",
      "\tat java.base/jdk.internal.reflect.NativeMethodAccessorImpl.invoke(NativeMethodAccessorImpl.java:77)\n",
      "\tat java.base/jdk.internal.reflect.DelegatingMethodAccessorImpl.invoke(DelegatingMethodAccessorImpl.java:43)\n",
      "\tat java.base/java.lang.reflect.Method.invoke(Method.java:569)\n",
      "\tat py4j.reflection.MethodInvoker.invoke(MethodInvoker.java:244)\n",
      "\tat py4j.reflection.ReflectionEngine.invoke(ReflectionEngine.java:374)\n",
      "\tat py4j.Gateway.invoke(Gateway.java:282)\n",
      "\tat py4j.commands.AbstractCommand.invokeMethod(AbstractCommand.java:132)\n",
      "\tat py4j.commands.CallCommand.execute(CallCommand.java:79)\n",
      "\tat py4j.GatewayConnection.run(GatewayConnection.java:238)\n",
      "\tat java.base/java.lang.Thread.run(Thread.java:840)\n",
      "Caused by: java.net.UnknownHostException: <RDS-ENDPOINT>\n",
      "\tat java.base/sun.nio.ch.NioSocketImpl.connect(NioSocketImpl.java:572)\n",
      "\tat java.base/java.net.SocksSocketImpl.connect(SocksSocketImpl.java:327)\n",
      "\tat java.base/java.net.Socket.connect(Socket.java:633)\n",
      "\tat org.postgresql.core.PGStream.<init>(PGStream.java:70)\n",
      "\tat org.postgresql.core.v3.ConnectionFactoryImpl.tryConnect(ConnectionFactoryImpl.java:91)\n",
      "\tat org.postgresql.core.v3.ConnectionFactoryImpl.openConnectionImpl(ConnectionFactoryImpl.java:192)\n",
      "\t... 30 more\n",
      "\n",
      "\n"
     ]
    }
   ],
   "source": [
    "dim_offices_df_check = spark.read.jdbc(url=jdbc_url, table=\"classicmodels_star_schema.dim_offices\", properties=jdbc_properties)\n",
    "\n",
    "print(\"dim_offices column names: \", dim_offices_df_check.columns)\n",
    "\n",
    "dim_offices_row_count = dim_offices_df_check.count()\n",
    "print(\"dim_offices number of rows: \", dim_offices_row_count)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "##### __Expected Output__\n",
    "\n",
    "```\n",
    "dim_offices column names:  ['office_key', 'postal_code', 'city', 'state', 'country', 'territory']\n",
    "dim_offices number of rows:  7\n",
    "```"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "<a name='4.6'></a>\n",
    "### 4.6 - Employees Dimension\n",
    "\n",
    "Let's continue with the `employees` dimension."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "4.6.1. Follow similar steps to create `employees` dimension. There will be a surrogate key `employee_key` based on the `employeeNumber`. You'll need to create a column `employee_number` based on the `employeeNumber`. Cast to string with the function `cast()`."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 73,
   "metadata": {
    "execution": {
     "iopub.execute_input": "2025-11-29T18:43:34.249805Z",
     "iopub.status.busy": "2025-11-29T18:43:34.249547Z",
     "iopub.status.idle": "2025-11-29T18:43:34.309655Z",
     "shell.execute_reply": "2025-11-29T18:43:34.306792Z",
     "shell.execute_reply.started": "2025-11-29T18:43:34.249769Z"
    },
    "exercise": [
     "ex13"
    ],
    "tags": [
     "graded"
    ]
   },
   "outputs": [
    {
     "data": {
      "application/vnd.jupyter.widget-view+json": {
       "model_id": "0d784007da434770ac52ebc9d82cd020",
       "version_major": 2,
       "version_minor": 0
      },
      "text/plain": [
       "VBox()"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "data": {
      "application/vnd.jupyter.widget-view+json": {
       "model_id": "",
       "version_major": 2,
       "version_minor": 0
      },
      "text/plain": [
       "FloatProgress(value=0.0, bar_style='info', description='Progress:', layout=Layout(height='25px', width='50%'),\u2026"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "An error was encountered:\n",
      "invalid syntax (<stdin>, line 11)\n",
      "  File \"<stdin>\", line 11\n",
      "    dim_employees_df = None.None(None)\n",
      "                            ^\n",
      "SyntaxError: invalid syntax\n",
      "\n"
     ]
    }
   ],
   "source": [
    "select_query_employees = \"\"\"\n",
    "SELECT \n",
    "    cast(employeeNumber as string) as employee_number,\n",
    "    lastName as employee_last_name, \n",
    "    firstName as employee_first_name, \n",
    "    jobTitle as job_title, \n",
    "    email as email\n",
    "FROM employees\n",
    "\"\"\"\n",
    "\n",
    "dim_employees_df = spark.sql(select_query_employees)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "4.6.2. With the resulting `dim_employees_df` DataFrame:\n",
    "- Call the `surrogateUDF` to generate a surrogate key based on the `employee_number`. You will need to use `array()` function to convert it to an array.\n",
    "- Add the surrogate key using the `withColumn()` function, call the new column `employee_key`. \n",
    "\n",
    "Then perform a select to grab the columns related to the ERM diagram."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 74,
   "metadata": {
    "execution": {
     "iopub.execute_input": "2025-11-29T18:43:34.312607Z",
     "iopub.status.busy": "2025-11-29T18:43:34.312351Z",
     "iopub.status.idle": "2025-11-29T18:43:34.371273Z",
     "shell.execute_reply": "2025-11-29T18:43:34.368523Z",
     "shell.execute_reply.started": "2025-11-29T18:43:34.312561Z"
    },
    "exercise": [
     "ex14"
    ],
    "tags": [
     "graded"
    ]
   },
   "outputs": [
    {
     "data": {
      "application/vnd.jupyter.widget-view+json": {
       "model_id": "1c1429b0d6d940839e6796c7b671e075",
       "version_major": 2,
       "version_minor": 0
      },
      "text/plain": [
       "VBox()"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "data": {
      "application/vnd.jupyter.widget-view+json": {
       "model_id": "",
       "version_major": 2,
       "version_minor": 0
      },
      "text/plain": [
       "FloatProgress(value=0.0, bar_style='info', description='Progress:', layout=Layout(height='25px', width='50%'),\u2026"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "An error was encountered:\n",
      "name 'dim_employees_df' is not defined\n",
      "Traceback (most recent call last):\n",
      "NameError: name 'dim_employees_df' is not defined\n",
      "\n"
     ]
    }
   ],
   "source": [
    "dim_employees_df = dim_employees_df.withColumn(\"employee_key\", surrogateUDF(array(\"employee_number\")))\\\n",
    ".select([\"employee_key\",\"employee_last_name\",\"employee_first_name\",\"email\"])"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "execution": {
     "iopub.execute_input": "2024-06-21T03:29:22.461906Z",
     "iopub.status.busy": "2024-06-21T03:29:22.461570Z"
    }
   },
   "source": [
    "4.6.3. Store the `dim_employees_df` DataFrame into the `classicmodels_star_schema` schema, table `dim_employees`."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 75,
   "metadata": {
    "execution": {
     "iopub.execute_input": "2025-11-29T18:43:34.372340Z",
     "iopub.status.busy": "2025-11-29T18:43:34.372177Z",
     "iopub.status.idle": "2025-11-29T18:43:34.426276Z",
     "shell.execute_reply": "2025-11-29T18:43:34.425644Z",
     "shell.execute_reply.started": "2025-11-29T18:43:34.372318Z"
    },
    "exercise": [
     "ex15"
    ],
    "tags": [
     "graded"
    ]
   },
   "outputs": [
    {
     "data": {
      "application/vnd.jupyter.widget-view+json": {
       "model_id": "109ad1b50fe74bf3b87dea6b2de8613b",
       "version_major": 2,
       "version_minor": 0
      },
      "text/plain": [
       "VBox()"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "data": {
      "application/vnd.jupyter.widget-view+json": {
       "model_id": "",
       "version_major": 2,
       "version_minor": 0
      },
      "text/plain": [
       "FloatProgress(value=0.0, bar_style='info', description='Progress:', layout=Layout(height='25px', width='50%'),\u2026"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "An error was encountered:\n",
      "name 'dim_employees_df' is not defined\n",
      "Traceback (most recent call last):\n",
      "NameError: name 'dim_employees_df' is not defined\n",
      "\n"
     ]
    }
   ],
   "source": [
    "dim_employees_df.write.jdbc(url=jdbc_url, table=\"classicmodels_star_schema.dim_employees\", mode=\"overwrite\", properties=jdbc_properties)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "4.6.4. Check your work:"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 76,
   "metadata": {
    "execution": {
     "iopub.execute_input": "2025-11-29T18:43:34.427471Z",
     "iopub.status.busy": "2025-11-29T18:43:34.427296Z",
     "iopub.status.idle": "2025-11-29T18:43:34.682716Z",
     "shell.execute_reply": "2025-11-29T18:43:34.681939Z",
     "shell.execute_reply.started": "2025-11-29T18:43:34.427449Z"
    },
    "exercise": [
     "ex16"
    ],
    "tags": [
     "graded"
    ]
   },
   "outputs": [
    {
     "data": {
      "application/vnd.jupyter.widget-view+json": {
       "model_id": "49299f9b1fa243d98442be426ca68675",
       "version_major": 2,
       "version_minor": 0
      },
      "text/plain": [
       "VBox()"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "data": {
      "application/vnd.jupyter.widget-view+json": {
       "model_id": "",
       "version_major": 2,
       "version_minor": 0
      },
      "text/plain": [
       "FloatProgress(value=0.0, bar_style='info', description='Progress:', layout=Layout(height='25px', width='50%'),\u2026"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "An error was encountered:\n",
      "An error occurred while calling o170.jdbc.\n",
      ": org.postgresql.util.PSQLException: The connection attempt failed.\n",
      "\tat org.postgresql.core.v3.ConnectionFactoryImpl.openConnectionImpl(ConnectionFactoryImpl.java:292)\n",
      "\tat org.postgresql.core.ConnectionFactory.openConnection(ConnectionFactory.java:49)\n",
      "\tat org.postgresql.jdbc.PgConnection.<init>(PgConnection.java:195)\n",
      "\tat org.postgresql.Driver.makeConnection(Driver.java:454)\n",
      "\tat org.postgresql.Driver.connect(Driver.java:256)\n",
      "\tat org.apache.spark.sql.execution.datasources.jdbc.connection.BasicConnectionProvider.getConnection(BasicConnectionProvider.scala:49)\n",
      "\tat org.apache.spark.sql.execution.datasources.jdbc.connection.ConnectionProviderBase.create(ConnectionProvider.scala:102)\n",
      "\tat org.apache.spark.sql.jdbc.JdbcDialect.$anonfun$createConnectionFactory$1(JdbcDialects.scala:160)\n",
      "\tat org.apache.spark.sql.jdbc.JdbcDialect.$anonfun$createConnectionFactory$1$adapted(JdbcDialects.scala:156)\n",
      "\tat org.apache.spark.sql.execution.datasources.jdbc.JDBCRDD$.getQueryOutputSchema(JDBCRDD.scala:63)\n",
      "\tat org.apache.spark.sql.execution.datasources.jdbc.JDBCRDD$.resolveTable(JDBCRDD.scala:58)\n",
      "\tat org.apache.spark.sql.execution.datasources.jdbc.JDBCRelation$.getSchema(JDBCRelation.scala:241)\n",
      "\tat org.apache.spark.sql.execution.datasources.jdbc.JdbcRelationProvider.createRelation(JdbcRelationProvider.scala:37)\n",
      "\tat org.apache.spark.sql.execution.datasources.DataSource.resolveRelation(DataSource.scala:346)\n",
      "\tat org.apache.spark.sql.DataFrameReader.loadV1Source(DataFrameReader.scala:229)\n",
      "\tat org.apache.spark.sql.DataFrameReader.$anonfun$load$2(DataFrameReader.scala:211)\n",
      "\tat scala.Option.getOrElse(Option.scala:189)\n",
      "\tat org.apache.spark.sql.DataFrameReader.load(DataFrameReader.scala:211)\n",
      "\tat org.apache.spark.sql.DataFrameReader.load(DataFrameReader.scala:172)\n",
      "\tat org.apache.spark.sql.DataFrameReader.jdbc(DataFrameReader.scala:249)\n",
      "\tat java.base/jdk.internal.reflect.NativeMethodAccessorImpl.invoke0(Native Method)\n",
      "\tat java.base/jdk.internal.reflect.NativeMethodAccessorImpl.invoke(NativeMethodAccessorImpl.java:77)\n",
      "\tat java.base/jdk.internal.reflect.DelegatingMethodAccessorImpl.invoke(DelegatingMethodAccessorImpl.java:43)\n",
      "\tat java.base/java.lang.reflect.Method.invoke(Method.java:569)\n",
      "\tat py4j.reflection.MethodInvoker.invoke(MethodInvoker.java:244)\n",
      "\tat py4j.reflection.ReflectionEngine.invoke(ReflectionEngine.java:374)\n",
      "\tat py4j.Gateway.invoke(Gateway.java:282)\n",
      "\tat py4j.commands.AbstractCommand.invokeMethod(AbstractCommand.java:132)\n",
      "\tat py4j.commands.CallCommand.execute(CallCommand.java:79)\n",
      "\tat py4j.GatewayConnection.run(GatewayConnection.java:238)\n",
      "\tat java.base/java.lang.Thread.run(Thread.java:840)\n",
      "Caused by: java.net.UnknownHostException: <RDS-ENDPOINT>\n",
      "\tat java.base/sun.nio.ch.NioSocketImpl.connect(NioSocketImpl.java:572)\n",
      "\tat java.base/java.net.SocksSocketImpl.connect(SocksSocketImpl.java:327)\n",
      "\tat java.base/java.net.Socket.connect(Socket.java:633)\n",
      "\tat org.postgresql.core.PGStream.<init>(PGStream.java:70)\n",
      "\tat org.postgresql.core.v3.ConnectionFactoryImpl.tryConnect(ConnectionFactoryImpl.java:91)\n",
      "\tat org.postgresql.core.v3.ConnectionFactoryImpl.openConnectionImpl(ConnectionFactoryImpl.java:192)\n",
      "\t... 30 more\n",
      "\n",
      "Traceback (most recent call last):\n",
      "  File \"/mnt1/yarn/usercache/livy/appcache/application_1764439453312_0002/container_1764439453312_0002_01_000001/pyspark.zip/pyspark/sql/readwriter.py\", line 946, in jdbc\n",
      "    return self._df(self._jreader.jdbc(url, table, jprop))\n",
      "  File \"/mnt1/yarn/usercache/livy/appcache/application_1764439453312_0002/container_1764439453312_0002_01_000001/py4j-0.10.9.7-src.zip/py4j/java_gateway.py\", line 1322, in __call__\n",
      "    return_value = get_return_value(\n",
      "  File \"/mnt1/yarn/usercache/livy/appcache/application_1764439453312_0002/container_1764439453312_0002_01_000001/pyspark.zip/pyspark/errors/exceptions/captured.py\", line 179, in deco\n",
      "    return f(*a, **kw)\n",
      "  File \"/mnt1/yarn/usercache/livy/appcache/application_1764439453312_0002/container_1764439453312_0002_01_000001/py4j-0.10.9.7-src.zip/py4j/protocol.py\", line 326, in get_return_value\n",
      "    raise Py4JJavaError(\n",
      "py4j.protocol.Py4JJavaError: An error occurred while calling o170.jdbc.\n",
      ": org.postgresql.util.PSQLException: The connection attempt failed.\n",
      "\tat org.postgresql.core.v3.ConnectionFactoryImpl.openConnectionImpl(ConnectionFactoryImpl.java:292)\n",
      "\tat org.postgresql.core.ConnectionFactory.openConnection(ConnectionFactory.java:49)\n",
      "\tat org.postgresql.jdbc.PgConnection.<init>(PgConnection.java:195)\n",
      "\tat org.postgresql.Driver.makeConnection(Driver.java:454)\n",
      "\tat org.postgresql.Driver.connect(Driver.java:256)\n",
      "\tat org.apache.spark.sql.execution.datasources.jdbc.connection.BasicConnectionProvider.getConnection(BasicConnectionProvider.scala:49)\n",
      "\tat org.apache.spark.sql.execution.datasources.jdbc.connection.ConnectionProviderBase.create(ConnectionProvider.scala:102)\n",
      "\tat org.apache.spark.sql.jdbc.JdbcDialect.$anonfun$createConnectionFactory$1(JdbcDialects.scala:160)\n",
      "\tat org.apache.spark.sql.jdbc.JdbcDialect.$anonfun$createConnectionFactory$1$adapted(JdbcDialects.scala:156)\n",
      "\tat org.apache.spark.sql.execution.datasources.jdbc.JDBCRDD$.getQueryOutputSchema(JDBCRDD.scala:63)\n",
      "\tat org.apache.spark.sql.execution.datasources.jdbc.JDBCRDD$.resolveTable(JDBCRDD.scala:58)\n",
      "\tat org.apache.spark.sql.execution.datasources.jdbc.JDBCRelation$.getSchema(JDBCRelation.scala:241)\n",
      "\tat org.apache.spark.sql.execution.datasources.jdbc.JdbcRelationProvider.createRelation(JdbcRelationProvider.scala:37)\n",
      "\tat org.apache.spark.sql.execution.datasources.DataSource.resolveRelation(DataSource.scala:346)\n",
      "\tat org.apache.spark.sql.DataFrameReader.loadV1Source(DataFrameReader.scala:229)\n",
      "\tat org.apache.spark.sql.DataFrameReader.$anonfun$load$2(DataFrameReader.scala:211)\n",
      "\tat scala.Option.getOrElse(Option.scala:189)\n",
      "\tat org.apache.spark.sql.DataFrameReader.load(DataFrameReader.scala:211)\n",
      "\tat org.apache.spark.sql.DataFrameReader.load(DataFrameReader.scala:172)\n",
      "\tat org.apache.spark.sql.DataFrameReader.jdbc(DataFrameReader.scala:249)\n",
      "\tat java.base/jdk.internal.reflect.NativeMethodAccessorImpl.invoke0(Native Method)\n",
      "\tat java.base/jdk.internal.reflect.NativeMethodAccessorImpl.invoke(NativeMethodAccessorImpl.java:77)\n",
      "\tat java.base/jdk.internal.reflect.DelegatingMethodAccessorImpl.invoke(DelegatingMethodAccessorImpl.java:43)\n",
      "\tat java.base/java.lang.reflect.Method.invoke(Method.java:569)\n",
      "\tat py4j.reflection.MethodInvoker.invoke(MethodInvoker.java:244)\n",
      "\tat py4j.reflection.ReflectionEngine.invoke(ReflectionEngine.java:374)\n",
      "\tat py4j.Gateway.invoke(Gateway.java:282)\n",
      "\tat py4j.commands.AbstractCommand.invokeMethod(AbstractCommand.java:132)\n",
      "\tat py4j.commands.CallCommand.execute(CallCommand.java:79)\n",
      "\tat py4j.GatewayConnection.run(GatewayConnection.java:238)\n",
      "\tat java.base/java.lang.Thread.run(Thread.java:840)\n",
      "Caused by: java.net.UnknownHostException: <RDS-ENDPOINT>\n",
      "\tat java.base/sun.nio.ch.NioSocketImpl.connect(NioSocketImpl.java:572)\n",
      "\tat java.base/java.net.SocksSocketImpl.connect(SocksSocketImpl.java:327)\n",
      "\tat java.base/java.net.Socket.connect(Socket.java:633)\n",
      "\tat org.postgresql.core.PGStream.<init>(PGStream.java:70)\n",
      "\tat org.postgresql.core.v3.ConnectionFactoryImpl.tryConnect(ConnectionFactoryImpl.java:91)\n",
      "\tat org.postgresql.core.v3.ConnectionFactoryImpl.openConnectionImpl(ConnectionFactoryImpl.java:192)\n",
      "\t... 30 more\n",
      "\n",
      "\n"
     ]
    }
   ],
   "source": [
    "dim_employees_df_check = spark.read.jdbc(url=jdbc_url, table=\"classicmodels_star_schema.dim_employees\", properties=jdbc_properties)\n",
    "\n",
    "print(\"dim_employees column names: \", dim_employees_df_check.columns)\n",
    "\n",
    "dim_employees_row_count = dim_employees_df_check.count()\n",
    "print(\"dim_employees number of rows: \", dim_employees_row_count)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "##### __Expected Output__\n",
    "\n",
    "```\n",
    "dim_employees column names:  ['employee_key', 'employee_last_name', 'employee_first_name', 'email']\n",
    "dim_employees number of rows:  23\n",
    "```"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "<a name='4.7'></a>\n",
    "### 4.7 - Date Dimension\n",
    "\n",
    "4.7.1. As in the `dbt` lab, you will limit the date's dimension table to the dates that appear in the `orders` table. You are already provided with the date range required to create your dimension table. In the following cell, you will:\n",
    "\n",
    "- Use the `to_date` function to enclose the `start_date` and `end_date` strings to convert them into actual date types.\n",
    "- Use the `sequence` function from `psypark.sql.functions` to generate a sequence of values from the `start_date` to the `end_date`. Note that the third parameter is the interval, which has been set to `interval 1 day`. The result from the `sequence` function is an array of values.\n",
    "- Finally, enclose the `sequence` function into the `explode` function. This function takes an array and returns one row for each element in the array. Note how the column has been named `date_day`."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 77,
   "metadata": {
    "execution": {
     "iopub.execute_input": "2025-11-29T18:43:34.684112Z",
     "iopub.status.busy": "2025-11-29T18:43:34.683887Z",
     "iopub.status.idle": "2025-11-29T18:43:34.938424Z",
     "shell.execute_reply": "2025-11-29T18:43:34.937769Z",
     "shell.execute_reply.started": "2025-11-29T18:43:34.684077Z"
    },
    "exercise": [
     "ex17"
    ],
    "tags": [
     "graded"
    ]
   },
   "outputs": [
    {
     "data": {
      "application/vnd.jupyter.widget-view+json": {
       "model_id": "512090eb264f4f6f82bebb194bb58679",
       "version_major": 2,
       "version_minor": 0
      },
      "text/plain": [
       "VBox()"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "data": {
      "application/vnd.jupyter.widget-view+json": {
       "model_id": "",
       "version_major": 2,
       "version_minor": 0
      },
      "text/plain": [
       "FloatProgress(value=0.0, bar_style='info', description='Progress:', layout=Layout(height='25px', width='50%'),\u2026"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    }
   ],
   "source": [
    "from pyspark.sql.functions import col, explode, sequence, year, month, dayofweek, dayofmonth, dayofyear, weekofyear, date_format, lit\n",
    "from pyspark.sql.types import DateType\n",
    "\n",
    "# Date range\n",
    "start_date = \"2003-01-01\"\n",
    "end_date = \"2005-12-31\"\n",
    "\n",
    "date_range_df = spark.sql(f\"SELECT explode(sequence(to_date('{start_date}'), to_date('{end_date}'), interval 1 day)) as date_day\")"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "4.7.2. Based on the `date_range_df` DataFrame, you are going to create the following columns by using the `withColumn()`:\n",
    "\n",
    "- Get the day of the week with the `dayofweek()` function; store it at the column `day_of_week`.\n",
    "- Get the day of the month with the `dayofmonth()` function; store it at the column `day_of_month`.\n",
    "- Get the number of the day in the year with the `dayofyear()` function; store it in the column `day_of_year`.\n",
    "- Get the number of the week in the year with the `weekofyear()` function; store it in the column `week_of_year`.\n",
    "- Get the the month with the `month()` function; store it at the column `month_of_year`.\n",
    "- Get the the year with the `year()` function; store it at the column `year_number`.\n",
    "\n",
    "Also, you will be creating a column `month_name`, but the code is already complete for that.\n",
    "\n",
    "In addition, you are going to create the `quarter_of_year` column by creating a new UDF. This time, instead of using the UDF as an SQL function with SparkSQL, you will register it as a Python UDF. \n",
    "\n",
    "- Complete the `get_quarter_of_year()` function by making an integer division between `date.month - 1` and 3 (with the `//` operator). Then, add 1 and return the value.\n",
    "- Call the `udf()` function and pass it as parameters to the function you just created and the `IntegerType()` function."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 78,
   "metadata": {
    "execution": {
     "iopub.execute_input": "2025-11-29T18:43:34.939561Z",
     "iopub.status.busy": "2025-11-29T18:43:34.939384Z",
     "iopub.status.idle": "2025-11-29T18:43:34.994066Z",
     "shell.execute_reply": "2025-11-29T18:43:34.991533Z",
     "shell.execute_reply.started": "2025-11-29T18:43:34.939539Z"
    },
    "tags": []
   },
   "outputs": [
    {
     "data": {
      "application/vnd.jupyter.widget-view+json": {
       "model_id": "00d4fea366e8435e9f8be25e4870e8bd",
       "version_major": 2,
       "version_minor": 0
      },
      "text/plain": [
       "VBox()"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "data": {
      "application/vnd.jupyter.widget-view+json": {
       "model_id": "",
       "version_major": 2,
       "version_minor": 0
      },
      "text/plain": [
       "FloatProgress(value=0.0, bar_style='info', description='Progress:', layout=Layout(height='25px', width='50%'),\u2026"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "An error was encountered:\n",
      "invalid syntax (<stdin>, line 2)\n",
      "  File \"<stdin>\", line 2\n",
      "    return (None.None - None) // None + None\n",
      "                 ^\n",
      "SyntaxError: invalid syntax\n",
      "\n"
     ]
    }
   ],
   "source": [
    "def get_quarter_of_year(date):\n",
    "    return (date.month - 1) // 3 + 1\n",
    "\n",
    "get_quarter_of_year_udf = udf(get_quarter_of_year, IntegerType())"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 79,
   "metadata": {
    "execution": {
     "iopub.execute_input": "2025-11-29T18:43:34.996903Z",
     "iopub.status.busy": "2025-11-29T18:43:34.995676Z",
     "iopub.status.idle": "2025-11-29T18:43:35.251499Z",
     "shell.execute_reply": "2025-11-29T18:43:35.250796Z",
     "shell.execute_reply.started": "2025-11-29T18:43:34.996872Z"
    },
    "exercise": [
     "ex18"
    ],
    "tags": [
     "graded"
    ]
   },
   "outputs": [
    {
     "data": {
      "application/vnd.jupyter.widget-view+json": {
       "model_id": "60974fbb00d74666a921d4dc743883ac",
       "version_major": 2,
       "version_minor": 0
      },
      "text/plain": [
       "VBox()"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "data": {
      "application/vnd.jupyter.widget-view+json": {
       "model_id": "",
       "version_major": 2,
       "version_minor": 0
      },
      "text/plain": [
       "FloatProgress(value=0.0, bar_style='info', description='Progress:', layout=Layout(height='25px', width='50%'),\u2026"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "An error was encountered:\n",
      "'NoneType' object is not callable\n",
      "Traceback (most recent call last):\n",
      "TypeError: 'NoneType' object is not callable\n",
      "\n"
     ]
    }
   ],
   "source": [
    "date_dim_df = date_range_df.withColumn(\"day_of_week\", dayofweek(\"date_day\")) \\\n",
    "    .withColumn(\"day_of_month\", dayofmonth(\"date_day\")) \\\n",
    "    .withColumn(\"day_of_year\", dayofyear(\"date_day\")) \\\n",
    "    .withColumn(\"week_of_year\", weekofyear(\"date_day\")) \\\n",
    "    .withColumn(\"month_of_year\", month(\"date_day\")) \\\n",
    "    .withColumn(\"year_number\", year(\"date_day\")) \\\n",
    "    .withColumn(\"month_name\", date_format(\"date_day\", \"MMMM\")) \\\n",
    "    .withColumn(\"quarter_of_year\", get_quarter_of_year_udf(\"date_day\"))\n",
    "\n",
    "# Show the result\n",
    "date_dim_df.show()"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "4.7.3. Store the `date_dim_df` dataframe into the `classicmodels_star_schema` schema, table `dim_date`."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 80,
   "metadata": {
    "execution": {
     "iopub.execute_input": "2025-11-29T18:43:35.252906Z",
     "iopub.status.busy": "2025-11-29T18:43:35.252659Z",
     "iopub.status.idle": "2025-11-29T18:43:35.310594Z",
     "shell.execute_reply": "2025-11-29T18:43:35.307355Z",
     "shell.execute_reply.started": "2025-11-29T18:43:35.252871Z"
    },
    "exercise": [
     "ex19"
    ],
    "tags": [
     "graded"
    ]
   },
   "outputs": [
    {
     "data": {
      "application/vnd.jupyter.widget-view+json": {
       "model_id": "3380d77cd22d47ccae11921dacee6e5b",
       "version_major": 2,
       "version_minor": 0
      },
      "text/plain": [
       "VBox()"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "data": {
      "application/vnd.jupyter.widget-view+json": {
       "model_id": "",
       "version_major": 2,
       "version_minor": 0
      },
      "text/plain": [
       "FloatProgress(value=0.0, bar_style='info', description='Progress:', layout=Layout(height='25px', width='50%'),\u2026"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "An error was encountered:\n",
      "name 'date_dim_df' is not defined\n",
      "Traceback (most recent call last):\n",
      "NameError: name 'date_dim_df' is not defined\n",
      "\n"
     ]
    }
   ],
   "source": [
    "date_dim_df.write.jdbc(url=jdbc_url, table=\"classicmodels_star_schema.dim_date\", mode=\"overwrite\", properties=jdbc_properties)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "4.7.4. Check that your table was correctly stored in the schema:"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 81,
   "metadata": {
    "execution": {
     "iopub.execute_input": "2025-11-29T18:43:35.313799Z",
     "iopub.status.busy": "2025-11-29T18:43:35.313578Z",
     "iopub.status.idle": "2025-11-29T18:43:35.567140Z",
     "shell.execute_reply": "2025-11-29T18:43:35.566433Z",
     "shell.execute_reply.started": "2025-11-29T18:43:35.313774Z"
    },
    "exercise": [
     "ex20"
    ],
    "tags": [
     "graded"
    ]
   },
   "outputs": [
    {
     "data": {
      "application/vnd.jupyter.widget-view+json": {
       "model_id": "b61ddadb50ff4fa3a36f9da8fd4895ef",
       "version_major": 2,
       "version_minor": 0
      },
      "text/plain": [
       "VBox()"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "data": {
      "application/vnd.jupyter.widget-view+json": {
       "model_id": "",
       "version_major": 2,
       "version_minor": 0
      },
      "text/plain": [
       "FloatProgress(value=0.0, bar_style='info', description='Progress:', layout=Layout(height='25px', width='50%'),\u2026"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "An error was encountered:\n",
      "An error occurred while calling o180.jdbc.\n",
      ": org.postgresql.util.PSQLException: The connection attempt failed.\n",
      "\tat org.postgresql.core.v3.ConnectionFactoryImpl.openConnectionImpl(ConnectionFactoryImpl.java:292)\n",
      "\tat org.postgresql.core.ConnectionFactory.openConnection(ConnectionFactory.java:49)\n",
      "\tat org.postgresql.jdbc.PgConnection.<init>(PgConnection.java:195)\n",
      "\tat org.postgresql.Driver.makeConnection(Driver.java:454)\n",
      "\tat org.postgresql.Driver.connect(Driver.java:256)\n",
      "\tat org.apache.spark.sql.execution.datasources.jdbc.connection.BasicConnectionProvider.getConnection(BasicConnectionProvider.scala:49)\n",
      "\tat org.apache.spark.sql.execution.datasources.jdbc.connection.ConnectionProviderBase.create(ConnectionProvider.scala:102)\n",
      "\tat org.apache.spark.sql.jdbc.JdbcDialect.$anonfun$createConnectionFactory$1(JdbcDialects.scala:160)\n",
      "\tat org.apache.spark.sql.jdbc.JdbcDialect.$anonfun$createConnectionFactory$1$adapted(JdbcDialects.scala:156)\n",
      "\tat org.apache.spark.sql.execution.datasources.jdbc.JDBCRDD$.getQueryOutputSchema(JDBCRDD.scala:63)\n",
      "\tat org.apache.spark.sql.execution.datasources.jdbc.JDBCRDD$.resolveTable(JDBCRDD.scala:58)\n",
      "\tat org.apache.spark.sql.execution.datasources.jdbc.JDBCRelation$.getSchema(JDBCRelation.scala:241)\n",
      "\tat org.apache.spark.sql.execution.datasources.jdbc.JdbcRelationProvider.createRelation(JdbcRelationProvider.scala:37)\n",
      "\tat org.apache.spark.sql.execution.datasources.DataSource.resolveRelation(DataSource.scala:346)\n",
      "\tat org.apache.spark.sql.DataFrameReader.loadV1Source(DataFrameReader.scala:229)\n",
      "\tat org.apache.spark.sql.DataFrameReader.$anonfun$load$2(DataFrameReader.scala:211)\n",
      "\tat scala.Option.getOrElse(Option.scala:189)\n",
      "\tat org.apache.spark.sql.DataFrameReader.load(DataFrameReader.scala:211)\n",
      "\tat org.apache.spark.sql.DataFrameReader.load(DataFrameReader.scala:172)\n",
      "\tat org.apache.spark.sql.DataFrameReader.jdbc(DataFrameReader.scala:249)\n",
      "\tat java.base/jdk.internal.reflect.NativeMethodAccessorImpl.invoke0(Native Method)\n",
      "\tat java.base/jdk.internal.reflect.NativeMethodAccessorImpl.invoke(NativeMethodAccessorImpl.java:77)\n",
      "\tat java.base/jdk.internal.reflect.DelegatingMethodAccessorImpl.invoke(DelegatingMethodAccessorImpl.java:43)\n",
      "\tat java.base/java.lang.reflect.Method.invoke(Method.java:569)\n",
      "\tat py4j.reflection.MethodInvoker.invoke(MethodInvoker.java:244)\n",
      "\tat py4j.reflection.ReflectionEngine.invoke(ReflectionEngine.java:374)\n",
      "\tat py4j.Gateway.invoke(Gateway.java:282)\n",
      "\tat py4j.commands.AbstractCommand.invokeMethod(AbstractCommand.java:132)\n",
      "\tat py4j.commands.CallCommand.execute(CallCommand.java:79)\n",
      "\tat py4j.GatewayConnection.run(GatewayConnection.java:238)\n",
      "\tat java.base/java.lang.Thread.run(Thread.java:840)\n",
      "Caused by: java.net.UnknownHostException: <RDS-ENDPOINT>\n",
      "\tat java.base/sun.nio.ch.NioSocketImpl.connect(NioSocketImpl.java:572)\n",
      "\tat java.base/java.net.SocksSocketImpl.connect(SocksSocketImpl.java:327)\n",
      "\tat java.base/java.net.Socket.connect(Socket.java:633)\n",
      "\tat org.postgresql.core.PGStream.<init>(PGStream.java:70)\n",
      "\tat org.postgresql.core.v3.ConnectionFactoryImpl.tryConnect(ConnectionFactoryImpl.java:91)\n",
      "\tat org.postgresql.core.v3.ConnectionFactoryImpl.openConnectionImpl(ConnectionFactoryImpl.java:192)\n",
      "\t... 30 more\n",
      "\n",
      "Traceback (most recent call last):\n",
      "  File \"/mnt1/yarn/usercache/livy/appcache/application_1764439453312_0002/container_1764439453312_0002_01_000001/pyspark.zip/pyspark/sql/readwriter.py\", line 946, in jdbc\n",
      "    return self._df(self._jreader.jdbc(url, table, jprop))\n",
      "  File \"/mnt1/yarn/usercache/livy/appcache/application_1764439453312_0002/container_1764439453312_0002_01_000001/py4j-0.10.9.7-src.zip/py4j/java_gateway.py\", line 1322, in __call__\n",
      "    return_value = get_return_value(\n",
      "  File \"/mnt1/yarn/usercache/livy/appcache/application_1764439453312_0002/container_1764439453312_0002_01_000001/pyspark.zip/pyspark/errors/exceptions/captured.py\", line 179, in deco\n",
      "    return f(*a, **kw)\n",
      "  File \"/mnt1/yarn/usercache/livy/appcache/application_1764439453312_0002/container_1764439453312_0002_01_000001/py4j-0.10.9.7-src.zip/py4j/protocol.py\", line 326, in get_return_value\n",
      "    raise Py4JJavaError(\n",
      "py4j.protocol.Py4JJavaError: An error occurred while calling o180.jdbc.\n",
      ": org.postgresql.util.PSQLException: The connection attempt failed.\n",
      "\tat org.postgresql.core.v3.ConnectionFactoryImpl.openConnectionImpl(ConnectionFactoryImpl.java:292)\n",
      "\tat org.postgresql.core.ConnectionFactory.openConnection(ConnectionFactory.java:49)\n",
      "\tat org.postgresql.jdbc.PgConnection.<init>(PgConnection.java:195)\n",
      "\tat org.postgresql.Driver.makeConnection(Driver.java:454)\n",
      "\tat org.postgresql.Driver.connect(Driver.java:256)\n",
      "\tat org.apache.spark.sql.execution.datasources.jdbc.connection.BasicConnectionProvider.getConnection(BasicConnectionProvider.scala:49)\n",
      "\tat org.apache.spark.sql.execution.datasources.jdbc.connection.ConnectionProviderBase.create(ConnectionProvider.scala:102)\n",
      "\tat org.apache.spark.sql.jdbc.JdbcDialect.$anonfun$createConnectionFactory$1(JdbcDialects.scala:160)\n",
      "\tat org.apache.spark.sql.jdbc.JdbcDialect.$anonfun$createConnectionFactory$1$adapted(JdbcDialects.scala:156)\n",
      "\tat org.apache.spark.sql.execution.datasources.jdbc.JDBCRDD$.getQueryOutputSchema(JDBCRDD.scala:63)\n",
      "\tat org.apache.spark.sql.execution.datasources.jdbc.JDBCRDD$.resolveTable(JDBCRDD.scala:58)\n",
      "\tat org.apache.spark.sql.execution.datasources.jdbc.JDBCRelation$.getSchema(JDBCRelation.scala:241)\n",
      "\tat org.apache.spark.sql.execution.datasources.jdbc.JdbcRelationProvider.createRelation(JdbcRelationProvider.scala:37)\n",
      "\tat org.apache.spark.sql.execution.datasources.DataSource.resolveRelation(DataSource.scala:346)\n",
      "\tat org.apache.spark.sql.DataFrameReader.loadV1Source(DataFrameReader.scala:229)\n",
      "\tat org.apache.spark.sql.DataFrameReader.$anonfun$load$2(DataFrameReader.scala:211)\n",
      "\tat scala.Option.getOrElse(Option.scala:189)\n",
      "\tat org.apache.spark.sql.DataFrameReader.load(DataFrameReader.scala:211)\n",
      "\tat org.apache.spark.sql.DataFrameReader.load(DataFrameReader.scala:172)\n",
      "\tat org.apache.spark.sql.DataFrameReader.jdbc(DataFrameReader.scala:249)\n",
      "\tat java.base/jdk.internal.reflect.NativeMethodAccessorImpl.invoke0(Native Method)\n",
      "\tat java.base/jdk.internal.reflect.NativeMethodAccessorImpl.invoke(NativeMethodAccessorImpl.java:77)\n",
      "\tat java.base/jdk.internal.reflect.DelegatingMethodAccessorImpl.invoke(DelegatingMethodAccessorImpl.java:43)\n",
      "\tat java.base/java.lang.reflect.Method.invoke(Method.java:569)\n",
      "\tat py4j.reflection.MethodInvoker.invoke(MethodInvoker.java:244)\n",
      "\tat py4j.reflection.ReflectionEngine.invoke(ReflectionEngine.java:374)\n",
      "\tat py4j.Gateway.invoke(Gateway.java:282)\n",
      "\tat py4j.commands.AbstractCommand.invokeMethod(AbstractCommand.java:132)\n",
      "\tat py4j.commands.CallCommand.execute(CallCommand.java:79)\n",
      "\tat py4j.GatewayConnection.run(GatewayConnection.java:238)\n",
      "\tat java.base/java.lang.Thread.run(Thread.java:840)\n",
      "Caused by: java.net.UnknownHostException: <RDS-ENDPOINT>\n",
      "\tat java.base/sun.nio.ch.NioSocketImpl.connect(NioSocketImpl.java:572)\n",
      "\tat java.base/java.net.SocksSocketImpl.connect(SocksSocketImpl.java:327)\n",
      "\tat java.base/java.net.Socket.connect(Socket.java:633)\n",
      "\tat org.postgresql.core.PGStream.<init>(PGStream.java:70)\n",
      "\tat org.postgresql.core.v3.ConnectionFactoryImpl.tryConnect(ConnectionFactoryImpl.java:91)\n",
      "\tat org.postgresql.core.v3.ConnectionFactoryImpl.openConnectionImpl(ConnectionFactoryImpl.java:192)\n",
      "\t... 30 more\n",
      "\n",
      "\n"
     ]
    }
   ],
   "source": [
    "date_dim_df_check = spark.read.jdbc(url=jdbc_url, table=\"classicmodels_star_schema.dim_date\", properties=jdbc_properties)\n",
    "\n",
    "print(\"dim_date column names: \", date_dim_df_check.columns)\n",
    "\n",
    "dim_employees_row_count = date_dim_df_check.count()\n",
    "print(\"dim_date number of rows: \", dim_employees_row_count)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "##### __Expected Output__\n",
    "\n",
    "```\n",
    "dim_date column names:  ['date_day', 'day_of_week', 'day_of_month', 'day_of_year', 'week_of_year', 'month_of_year', 'year_number', 'month_name', 'quarter_of_year']\n",
    "dim_date number of rows:  1096\n",
    "```"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "<a name='4.8'></a>\n",
    "### 4.8 - Fact Table\n",
    "\n",
    "Finally, let's create the orders fact table. Remember that the fact table stores the surrogate keys to the dimensional tables and the numerical facts related to the business process. \n",
    "\n",
    "There has been a change in the model compared to the Week 1 assignment. You will add two new facts:\n",
    "\n",
    "- `profit`: metric calculated by subtracting the price of the product as we sell by the price we bought the product at. \n",
    "- `discount_percentage`: metric calculated by subtracting the MSRP of a product from the selling price, dividing the result by the same MSRP and then multiplying the result by 100. "
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "4.8.1. Here is the statement to bring all the relevant columns and create the data model into the `fact_table_df` dataframe. There are also corresponding operations to add the missing calculated columns."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 82,
   "metadata": {
    "execution": {
     "iopub.execute_input": "2025-11-29T18:43:35.568183Z",
     "iopub.status.busy": "2025-11-29T18:43:35.568018Z",
     "iopub.status.idle": "2025-11-29T18:43:36.325264Z",
     "shell.execute_reply": "2025-11-29T18:43:36.324493Z",
     "shell.execute_reply.started": "2025-11-29T18:43:35.568161Z"
    },
    "exercise": [
     "ex21"
    ],
    "tags": [
     "graded"
    ]
   },
   "outputs": [
    {
     "data": {
      "application/vnd.jupyter.widget-view+json": {
       "model_id": "74dbd4ca75d94f629fdb70afaf5aa1bd",
       "version_major": 2,
       "version_minor": 0
      },
      "text/plain": [
       "VBox()"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "data": {
      "application/vnd.jupyter.widget-view+json": {
       "model_id": "",
       "version_major": 2,
       "version_minor": 0
      },
      "text/plain": [
       "FloatProgress(value=0.0, bar_style='info', description='Progress:', layout=Layout(height='25px', width='50%'),\u2026"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "An error was encountered:\n",
      "[TABLE_OR_VIEW_NOT_FOUND] The table or view `orders` cannot be found. Verify the spelling and correctness of the schema and catalog.\n",
      "If you did not qualify the name with a schema, verify the current_schema() output, or qualify the name with the correct schema and catalog.\n",
      "To tolerate the error on drop use DROP VIEW IF EXISTS or DROP TABLE IF EXISTS.; line 16 pos 5;\n",
      "'Project ['orders.orderNumber, cast('orderdetails.orderLineNumber as string) AS order_line_number#36, cast('orders.customerNumber as string) AS customer_number#37, cast('employees.employeeNumber as string) AS employee_number#38, 'offices.officeCode, 'orderdetails.productCode, 'orders.orderDate AS order_date#39, 'orders.requiredDate AS order_required_date#40, 'orders.shippedDate AS order_shipped_date#41, 'orderdetails.quantityOrdered AS quantity_ordered#42, 'orderdetails.priceEach AS product_price#43, ('orderdetails.priceEach - 'products.buyPrice) AS profit#44, ((('products.msrp - 'orderdetails.priceEach) / 'products.msrp) * 100) AS discount_percentage#45]\n",
      "+- 'Join Inner, ('products.productCode = 'orderdetails.productCode)\n",
      "   :- 'Join Inner, ('employees.officeCode = 'offices.officeCode)\n",
      "   :  :- 'Join Inner, ('customers.salesRepEmployeeNumber = 'employees.employeeNumber)\n",
      "   :  :  :- 'Join Inner, ('orders.customerNumber = 'customers.customerNumber)\n",
      "   :  :  :  :- 'Join Inner, ('orders.orderNumber = 'orderdetails.orderNumber)\n",
      "   :  :  :  :  :- 'UnresolvedRelation [orders], [], false\n",
      "   :  :  :  :  +- 'UnresolvedRelation [orderdetails], [], false\n",
      "   :  :  :  +- 'UnresolvedRelation [customers], [], false\n",
      "   :  :  +- 'UnresolvedRelation [employees], [], false\n",
      "   :  +- 'UnresolvedRelation [offices], [], false\n",
      "   +- 'UnresolvedRelation [products], [], false\n",
      "\n",
      "Traceback (most recent call last):\n",
      "  File \"/mnt1/yarn/usercache/livy/appcache/application_1764439453312_0002/container_1764439453312_0002_01_000001/pyspark.zip/pyspark/sql/session.py\", line 1631, in sql\n",
      "    return DataFrame(self._jsparkSession.sql(sqlQuery, litArgs), self)\n",
      "  File \"/mnt1/yarn/usercache/livy/appcache/application_1764439453312_0002/container_1764439453312_0002_01_000001/py4j-0.10.9.7-src.zip/py4j/java_gateway.py\", line 1322, in __call__\n",
      "    return_value = get_return_value(\n",
      "  File \"/mnt1/yarn/usercache/livy/appcache/application_1764439453312_0002/container_1764439453312_0002_01_000001/pyspark.zip/pyspark/errors/exceptions/captured.py\", line 185, in deco\n",
      "    raise converted from None\n",
      "pyspark.errors.exceptions.captured.AnalysisException: [TABLE_OR_VIEW_NOT_FOUND] The table or view `orders` cannot be found. Verify the spelling and correctness of the schema and catalog.\n",
      "If you did not qualify the name with a schema, verify the current_schema() output, or qualify the name with the correct schema and catalog.\n",
      "To tolerate the error on drop use DROP VIEW IF EXISTS or DROP TABLE IF EXISTS.; line 16 pos 5;\n",
      "'Project ['orders.orderNumber, cast('orderdetails.orderLineNumber as string) AS order_line_number#36, cast('orders.customerNumber as string) AS customer_number#37, cast('employees.employeeNumber as string) AS employee_number#38, 'offices.officeCode, 'orderdetails.productCode, 'orders.orderDate AS order_date#39, 'orders.requiredDate AS order_required_date#40, 'orders.shippedDate AS order_shipped_date#41, 'orderdetails.quantityOrdered AS quantity_ordered#42, 'orderdetails.priceEach AS product_price#43, ('orderdetails.priceEach - 'products.buyPrice) AS profit#44, ((('products.msrp - 'orderdetails.priceEach) / 'products.msrp) * 100) AS discount_percentage#45]\n",
      "+- 'Join Inner, ('products.productCode = 'orderdetails.productCode)\n",
      "   :- 'Join Inner, ('employees.officeCode = 'offices.officeCode)\n",
      "   :  :- 'Join Inner, ('customers.salesRepEmployeeNumber = 'employees.employeeNumber)\n",
      "   :  :  :- 'Join Inner, ('orders.customerNumber = 'customers.customerNumber)\n",
      "   :  :  :  :- 'Join Inner, ('orders.orderNumber = 'orderdetails.orderNumber)\n",
      "   :  :  :  :  :- 'UnresolvedRelation [orders], [], false\n",
      "   :  :  :  :  +- 'UnresolvedRelation [orderdetails], [], false\n",
      "   :  :  :  +- 'UnresolvedRelation [customers], [], false\n",
      "   :  :  +- 'UnresolvedRelation [employees], [], false\n",
      "   :  +- 'UnresolvedRelation [offices], [], false\n",
      "   +- 'UnresolvedRelation [products], [], false\n",
      "\n",
      "\n"
     ]
    }
   ],
   "source": [
    "select_query_fact = \"\"\"\n",
    "SELECT \n",
    "    orders.orderNumber, \n",
    "    cast(orderdetails.orderLineNumber as string) as order_line_number,\n",
    "    cast(orders.customerNumber as string) as customer_number, \n",
    "    cast(employees.employeeNumber as string) as employee_number,\n",
    "    offices.officeCode,\n",
    "    orderdetails.productCode, \n",
    "    orders.orderDate as order_date,\n",
    "    orders.requiredDate as order_required_date, \n",
    "    orders.shippedDate as order_shipped_date,\n",
    "    orderdetails.quantityOrdered as quantity_ordered, \n",
    "    orderdetails.priceEach as product_price,\n",
    "    (orderdetails.priceEach - products.buyPrice) as profit,\n",
    "    (products.msrp - orderdetails.priceEach)/products.msrp * 100 as discount_percentage\n",
    "FROM orders\n",
    "JOIN orderdetails ON orders.orderNumber = orderdetails.orderNumber\n",
    "JOIN customers ON orders.customerNumber = customers.customerNumber\n",
    "JOIN employees ON customers.salesRepEmployeeNumber = employees.employeeNumber\n",
    "JOIN offices ON employees.officeCode = offices.officeCode\n",
    "JOIN products ON products.productCode = orderdetails.productCode\n",
    "\"\"\";\n",
    "\n",
    "fact_table_df = spark.sql(select_query_fact)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "4.8.2. Add the calculated facts and the required surrogate keys. Use function `surrogateUDF()` passing an array based on:\n",
    "- `customer_number` for the `customer_key`,\n",
    "- `employee_number` for the `employee_key`,\n",
    "- `officeCode` for the `office_key`,\n",
    "- `productCode` for the `product_key`."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 83,
   "metadata": {
    "execution": {
     "iopub.execute_input": "2025-11-29T18:43:36.326715Z",
     "iopub.status.busy": "2025-11-29T18:43:36.326464Z",
     "iopub.status.idle": "2025-11-29T18:43:36.386856Z",
     "shell.execute_reply": "2025-11-29T18:43:36.383822Z",
     "shell.execute_reply.started": "2025-11-29T18:43:36.326679Z"
    },
    "exercise": [
     "ex22"
    ],
    "tags": [
     "graded"
    ]
   },
   "outputs": [
    {
     "data": {
      "application/vnd.jupyter.widget-view+json": {
       "model_id": "bdc1cd803d934297bae16b333c0a81c4",
       "version_major": 2,
       "version_minor": 0
      },
      "text/plain": [
       "VBox()"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "data": {
      "application/vnd.jupyter.widget-view+json": {
       "model_id": "",
       "version_major": 2,
       "version_minor": 0
      },
      "text/plain": [
       "FloatProgress(value=0.0, bar_style='info', description='Progress:', layout=Layout(height='25px', width='50%'),\u2026"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "An error was encountered:\n",
      "name 'fact_table_df' is not defined\n",
      "Traceback (most recent call last):\n",
      "NameError: name 'fact_table_df' is not defined\n",
      "\n"
     ]
    }
   ],
   "source": [
    "fact_table_df = fact_table_df.withColumn(\"fact_order_key\", surrogateUDF(array(\"orderNumber\", \"order_line_number\")))\\\n",
    ".withColumn(\"customer_key\", surrogateUDF(array(\"customer_number\")))\\\n",
    ".withColumn(\"employee_key\", surrogateUDF(array(\"employee_number\")))\\\n",
    ".withColumn(\"office_key\", surrogateUDF(array(\"officeCode\")))\\\n",
    ".withColumn(\"product_key\", surrogateUDF(array(\"productCode\")))\\\n",
    ".select([\"fact_order_key\",\"customer_key\",\"employee_key\",\"office_key\",\"product_key\",\"order_date\",\"order_required_date\",\"order_shipped_date\",\"quantity_ordered\",\"product_price\",\"profit\",\"discount_percentage\"])"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "4.8.3. Store the result in the `fact_orders` table in your database `classicmodels_star_schema`."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 84,
   "metadata": {
    "execution": {
     "iopub.execute_input": "2025-11-29T18:43:36.388325Z",
     "iopub.status.busy": "2025-11-29T18:43:36.388080Z",
     "iopub.status.idle": "2025-11-29T18:43:36.436686Z",
     "shell.execute_reply": "2025-11-29T18:43:36.434014Z",
     "shell.execute_reply.started": "2025-11-29T18:43:36.388290Z"
    },
    "exercise": [
     "ex23"
    ],
    "tags": [
     "graded"
    ]
   },
   "outputs": [
    {
     "data": {
      "application/vnd.jupyter.widget-view+json": {
       "model_id": "0c2d4e6df6904af896b80321c85384c0",
       "version_major": 2,
       "version_minor": 0
      },
      "text/plain": [
       "VBox()"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "data": {
      "application/vnd.jupyter.widget-view+json": {
       "model_id": "",
       "version_major": 2,
       "version_minor": 0
      },
      "text/plain": [
       "FloatProgress(value=0.0, bar_style='info', description='Progress:', layout=Layout(height='25px', width='50%'),\u2026"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "An error was encountered:\n",
      "name 'fact_table_df' is not defined\n",
      "Traceback (most recent call last):\n",
      "NameError: name 'fact_table_df' is not defined\n",
      "\n"
     ]
    }
   ],
   "source": [
    "fact_table_df.write.jdbc(url=jdbc_url, table=\"classicmodels_star_schema.fact_orders\", mode=\"overwrite\", properties=jdbc_properties)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "4.8.4. Check that your table was correctly stored in the schema:"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 85,
   "metadata": {
    "execution": {
     "iopub.execute_input": "2025-11-29T18:43:36.440453Z",
     "iopub.status.busy": "2025-11-29T18:43:36.439457Z",
     "iopub.status.idle": "2025-11-29T18:43:36.710432Z",
     "shell.execute_reply": "2025-11-29T18:43:36.709553Z",
     "shell.execute_reply.started": "2025-11-29T18:43:36.440427Z"
    },
    "exercise": [
     "ex24"
    ],
    "tags": [
     "graded"
    ]
   },
   "outputs": [
    {
     "data": {
      "application/vnd.jupyter.widget-view+json": {
       "model_id": "d0d26e7d31b74a9d9c95fda83db22ede",
       "version_major": 2,
       "version_minor": 0
      },
      "text/plain": [
       "VBox()"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "data": {
      "application/vnd.jupyter.widget-view+json": {
       "model_id": "",
       "version_major": 2,
       "version_minor": 0
      },
      "text/plain": [
       "FloatProgress(value=0.0, bar_style='info', description='Progress:', layout=Layout(height='25px', width='50%'),\u2026"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "An error was encountered:\n",
      "An error occurred while calling o190.jdbc.\n",
      ": org.postgresql.util.PSQLException: The connection attempt failed.\n",
      "\tat org.postgresql.core.v3.ConnectionFactoryImpl.openConnectionImpl(ConnectionFactoryImpl.java:292)\n",
      "\tat org.postgresql.core.ConnectionFactory.openConnection(ConnectionFactory.java:49)\n",
      "\tat org.postgresql.jdbc.PgConnection.<init>(PgConnection.java:195)\n",
      "\tat org.postgresql.Driver.makeConnection(Driver.java:454)\n",
      "\tat org.postgresql.Driver.connect(Driver.java:256)\n",
      "\tat org.apache.spark.sql.execution.datasources.jdbc.connection.BasicConnectionProvider.getConnection(BasicConnectionProvider.scala:49)\n",
      "\tat org.apache.spark.sql.execution.datasources.jdbc.connection.ConnectionProviderBase.create(ConnectionProvider.scala:102)\n",
      "\tat org.apache.spark.sql.jdbc.JdbcDialect.$anonfun$createConnectionFactory$1(JdbcDialects.scala:160)\n",
      "\tat org.apache.spark.sql.jdbc.JdbcDialect.$anonfun$createConnectionFactory$1$adapted(JdbcDialects.scala:156)\n",
      "\tat org.apache.spark.sql.execution.datasources.jdbc.JDBCRDD$.getQueryOutputSchema(JDBCRDD.scala:63)\n",
      "\tat org.apache.spark.sql.execution.datasources.jdbc.JDBCRDD$.resolveTable(JDBCRDD.scala:58)\n",
      "\tat org.apache.spark.sql.execution.datasources.jdbc.JDBCRelation$.getSchema(JDBCRelation.scala:241)\n",
      "\tat org.apache.spark.sql.execution.datasources.jdbc.JdbcRelationProvider.createRelation(JdbcRelationProvider.scala:37)\n",
      "\tat org.apache.spark.sql.execution.datasources.DataSource.resolveRelation(DataSource.scala:346)\n",
      "\tat org.apache.spark.sql.DataFrameReader.loadV1Source(DataFrameReader.scala:229)\n",
      "\tat org.apache.spark.sql.DataFrameReader.$anonfun$load$2(DataFrameReader.scala:211)\n",
      "\tat scala.Option.getOrElse(Option.scala:189)\n",
      "\tat org.apache.spark.sql.DataFrameReader.load(DataFrameReader.scala:211)\n",
      "\tat org.apache.spark.sql.DataFrameReader.load(DataFrameReader.scala:172)\n",
      "\tat org.apache.spark.sql.DataFrameReader.jdbc(DataFrameReader.scala:249)\n",
      "\tat java.base/jdk.internal.reflect.NativeMethodAccessorImpl.invoke0(Native Method)\n",
      "\tat java.base/jdk.internal.reflect.NativeMethodAccessorImpl.invoke(NativeMethodAccessorImpl.java:77)\n",
      "\tat java.base/jdk.internal.reflect.DelegatingMethodAccessorImpl.invoke(DelegatingMethodAccessorImpl.java:43)\n",
      "\tat java.base/java.lang.reflect.Method.invoke(Method.java:569)\n",
      "\tat py4j.reflection.MethodInvoker.invoke(MethodInvoker.java:244)\n",
      "\tat py4j.reflection.ReflectionEngine.invoke(ReflectionEngine.java:374)\n",
      "\tat py4j.Gateway.invoke(Gateway.java:282)\n",
      "\tat py4j.commands.AbstractCommand.invokeMethod(AbstractCommand.java:132)\n",
      "\tat py4j.commands.CallCommand.execute(CallCommand.java:79)\n",
      "\tat py4j.GatewayConnection.run(GatewayConnection.java:238)\n",
      "\tat java.base/java.lang.Thread.run(Thread.java:840)\n",
      "Caused by: java.net.UnknownHostException: <RDS-ENDPOINT>\n",
      "\tat java.base/sun.nio.ch.NioSocketImpl.connect(NioSocketImpl.java:572)\n",
      "\tat java.base/java.net.SocksSocketImpl.connect(SocksSocketImpl.java:327)\n",
      "\tat java.base/java.net.Socket.connect(Socket.java:633)\n",
      "\tat org.postgresql.core.PGStream.<init>(PGStream.java:70)\n",
      "\tat org.postgresql.core.v3.ConnectionFactoryImpl.tryConnect(ConnectionFactoryImpl.java:91)\n",
      "\tat org.postgresql.core.v3.ConnectionFactoryImpl.openConnectionImpl(ConnectionFactoryImpl.java:192)\n",
      "\t... 30 more\n",
      "\n",
      "Traceback (most recent call last):\n",
      "  File \"/mnt1/yarn/usercache/livy/appcache/application_1764439453312_0002/container_1764439453312_0002_01_000001/pyspark.zip/pyspark/sql/readwriter.py\", line 946, in jdbc\n",
      "    return self._df(self._jreader.jdbc(url, table, jprop))\n",
      "  File \"/mnt1/yarn/usercache/livy/appcache/application_1764439453312_0002/container_1764439453312_0002_01_000001/py4j-0.10.9.7-src.zip/py4j/java_gateway.py\", line 1322, in __call__\n",
      "    return_value = get_return_value(\n",
      "  File \"/mnt1/yarn/usercache/livy/appcache/application_1764439453312_0002/container_1764439453312_0002_01_000001/pyspark.zip/pyspark/errors/exceptions/captured.py\", line 179, in deco\n",
      "    return f(*a, **kw)\n",
      "  File \"/mnt1/yarn/usercache/livy/appcache/application_1764439453312_0002/container_1764439453312_0002_01_000001/py4j-0.10.9.7-src.zip/py4j/protocol.py\", line 326, in get_return_value\n",
      "    raise Py4JJavaError(\n",
      "py4j.protocol.Py4JJavaError: An error occurred while calling o190.jdbc.\n",
      ": org.postgresql.util.PSQLException: The connection attempt failed.\n",
      "\tat org.postgresql.core.v3.ConnectionFactoryImpl.openConnectionImpl(ConnectionFactoryImpl.java:292)\n",
      "\tat org.postgresql.core.ConnectionFactory.openConnection(ConnectionFactory.java:49)\n",
      "\tat org.postgresql.jdbc.PgConnection.<init>(PgConnection.java:195)\n",
      "\tat org.postgresql.Driver.makeConnection(Driver.java:454)\n",
      "\tat org.postgresql.Driver.connect(Driver.java:256)\n",
      "\tat org.apache.spark.sql.execution.datasources.jdbc.connection.BasicConnectionProvider.getConnection(BasicConnectionProvider.scala:49)\n",
      "\tat org.apache.spark.sql.execution.datasources.jdbc.connection.ConnectionProviderBase.create(ConnectionProvider.scala:102)\n",
      "\tat org.apache.spark.sql.jdbc.JdbcDialect.$anonfun$createConnectionFactory$1(JdbcDialects.scala:160)\n",
      "\tat org.apache.spark.sql.jdbc.JdbcDialect.$anonfun$createConnectionFactory$1$adapted(JdbcDialects.scala:156)\n",
      "\tat org.apache.spark.sql.execution.datasources.jdbc.JDBCRDD$.getQueryOutputSchema(JDBCRDD.scala:63)\n",
      "\tat org.apache.spark.sql.execution.datasources.jdbc.JDBCRDD$.resolveTable(JDBCRDD.scala:58)\n",
      "\tat org.apache.spark.sql.execution.datasources.jdbc.JDBCRelation$.getSchema(JDBCRelation.scala:241)\n",
      "\tat org.apache.spark.sql.execution.datasources.jdbc.JdbcRelationProvider.createRelation(JdbcRelationProvider.scala:37)\n",
      "\tat org.apache.spark.sql.execution.datasources.DataSource.resolveRelation(DataSource.scala:346)\n",
      "\tat org.apache.spark.sql.DataFrameReader.loadV1Source(DataFrameReader.scala:229)\n",
      "\tat org.apache.spark.sql.DataFrameReader.$anonfun$load$2(DataFrameReader.scala:211)\n",
      "\tat scala.Option.getOrElse(Option.scala:189)\n",
      "\tat org.apache.spark.sql.DataFrameReader.load(DataFrameReader.scala:211)\n",
      "\tat org.apache.spark.sql.DataFrameReader.load(DataFrameReader.scala:172)\n",
      "\tat org.apache.spark.sql.DataFrameReader.jdbc(DataFrameReader.scala:249)\n",
      "\tat java.base/jdk.internal.reflect.NativeMethodAccessorImpl.invoke0(Native Method)\n",
      "\tat java.base/jdk.internal.reflect.NativeMethodAccessorImpl.invoke(NativeMethodAccessorImpl.java:77)\n",
      "\tat java.base/jdk.internal.reflect.DelegatingMethodAccessorImpl.invoke(DelegatingMethodAccessorImpl.java:43)\n",
      "\tat java.base/java.lang.reflect.Method.invoke(Method.java:569)\n",
      "\tat py4j.reflection.MethodInvoker.invoke(MethodInvoker.java:244)\n",
      "\tat py4j.reflection.ReflectionEngine.invoke(ReflectionEngine.java:374)\n",
      "\tat py4j.Gateway.invoke(Gateway.java:282)\n",
      "\tat py4j.commands.AbstractCommand.invokeMethod(AbstractCommand.java:132)\n",
      "\tat py4j.commands.CallCommand.execute(CallCommand.java:79)\n",
      "\tat py4j.GatewayConnection.run(GatewayConnection.java:238)\n",
      "\tat java.base/java.lang.Thread.run(Thread.java:840)\n",
      "Caused by: java.net.UnknownHostException: <RDS-ENDPOINT>\n",
      "\tat java.base/sun.nio.ch.NioSocketImpl.connect(NioSocketImpl.java:572)\n",
      "\tat java.base/java.net.SocksSocketImpl.connect(SocksSocketImpl.java:327)\n",
      "\tat java.base/java.net.Socket.connect(Socket.java:633)\n",
      "\tat org.postgresql.core.PGStream.<init>(PGStream.java:70)\n",
      "\tat org.postgresql.core.v3.ConnectionFactoryImpl.tryConnect(ConnectionFactoryImpl.java:91)\n",
      "\tat org.postgresql.core.v3.ConnectionFactoryImpl.openConnectionImpl(ConnectionFactoryImpl.java:192)\n",
      "\t... 30 more\n",
      "\n",
      "\n"
     ]
    }
   ],
   "source": [
    "fact_table_df_check = spark.read.jdbc(url=jdbc_url, table=\"classicmodels_star_schema.fact_orders\", properties=jdbc_properties)\n",
    "\n",
    "print(\"fact_orders column names: \", fact_table_df_check.columns)\n",
    "\n",
    "fact_table_row_count = fact_table_df_check.count()\n",
    "print(\"fact_orders number of rows: \", fact_table_row_count)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "##### __Expected Output__\n",
    "\n",
    "```\n",
    "fact_orders column names:  ['fact_order_key', 'customer_key', 'employee_key', 'office_key', 'product_key', 'order_date', 'order_required_date', 'order_shipped_date', 'quantity_ordered', 'product_price', 'profit', 'discount_percentage']\n",
    "fact_orders number of rows:  2996\n",
    "```"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "4.8.5. Finally, print out your schema:"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 86,
   "metadata": {
    "execution": {
     "iopub.execute_input": "2025-11-29T18:43:36.711934Z",
     "iopub.status.busy": "2025-11-29T18:43:36.711687Z",
     "iopub.status.idle": "2025-11-29T18:43:36.760766Z",
     "shell.execute_reply": "2025-11-29T18:43:36.759889Z",
     "shell.execute_reply.started": "2025-11-29T18:43:36.711899Z"
    },
    "exercise": [
     "ex25"
    ],
    "tags": [
     "graded"
    ]
   },
   "outputs": [
    {
     "data": {
      "application/vnd.jupyter.widget-view+json": {
       "model_id": "eaba4868a8e940c5a0982bcdcdea00e9",
       "version_major": 2,
       "version_minor": 0
      },
      "text/plain": [
       "VBox()"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "data": {
      "application/vnd.jupyter.widget-view+json": {
       "model_id": "",
       "version_major": 2,
       "version_minor": 0
      },
      "text/plain": [
       "FloatProgress(value=0.0, bar_style='info', description='Progress:', layout=Layout(height='25px', width='50%'),\u2026"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "An error was encountered:\n",
      "name 'fact_table_df' is not defined\n",
      "Traceback (most recent call last):\n",
      "NameError: name 'fact_table_df' is not defined\n",
      "\n"
     ]
    }
   ],
   "source": [
    "fact_table_df.printSchema()"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "##### __Expected Output__\n",
    "\n",
    "```\n",
    "root\n",
    " |-- fact_order_key: string (nullable = true)\n",
    " |-- customer_key: string (nullable = true)\n",
    " |-- employee_key: string (nullable = true)\n",
    " |-- office_key: string (nullable = true)\n",
    " |-- product_key: string (nullable = true)\n",
    " |-- order_date: timestamp (nullable = true)\n",
    " |-- order_required_date: timestamp (nullable = true)\n",
    " |-- order_shipped_date: timestamp (nullable = true)\n",
    " |-- quantity_ordered: integer (nullable = true)\n",
    " |-- product_price: decimal(38,18) (nullable = true)\n",
    " |-- profit: decimal(38,17) (nullable = true)\n",
    " |-- discount_percentage: decimal(38,6) (nullable = true)\n",
    "```"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "In this lab, you have explored basic data transformation using Apache Spark, focusing on the capabilities of their Python API (PySpark) and Spark SQL. These tools are essential for data engineers, offering powerful and efficient methods for manipulating large datasets. Spark offers rich APIs and tools for data transformations, one of them being Spark SQL, which enables querying of structured data with SQL-like syntax. Although this dataset isn't particularly large, the same principles apply to larger data sources due to data parallelism."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "<a name='5'></a>\n",
    "## 5 - Upload Files for Grading\n",
    "\n",
    "Upload the notebook into S3 bucket for grading purposes.\n",
    "\n",
    "*Note*: you may need to click **Save** button before the upload.\n",
    "\n",
    "In your AWS console, search again for **CloudShell** and click on it. Once the terminal is ready, execute the following two commands to upload your notebook:\n",
    "\n",
    "```bash\n",
    "export ACCOUNT_ID=$(aws sts get-caller-identity --query Account --output text)\n",
    "\n",
    "aws s3 cp s3://de-c4w3a1-$ACCOUNT_ID-us-east-1-emr-bucket/emr-studio/$(aws s3 ls s3://de-c4w3a1-$ACCOUNT_ID-us-east-1-emr-bucket/emr-studio --recursive | grep -o \"e-[^/]*\" | head -n 1)/C4_W3_Assignment.ipynb s3://de-c4w3a1-$ACCOUNT_ID-us-east-1-submission/C4_W3_Assignment_Learner.ipynb\n",
    "```"
   ]
  }
 ],
 "metadata": {
  "grader_version": "1",
  "kernelspec": {
   "display_name": "PySpark",
   "language": "python",
   "name": "pysparkkernel"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "python",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "pyspark",
   "pygments_lexer": "python3"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 4
}