# Course 2 Week 4: Building an Advanced Data Pipeline With Data Quality Checks

**DeepLearning.AI Data Engineering Certificate**

## âš¡ Quick Start

**ğŸ‘‰ READ [START_HERE.md](START_HERE.md) FIRST! ğŸ‘ˆ**

All exercises are **100% COMPLETE** and ready to deploy!

---

## ğŸ¯ Assignment Overview

Build a Machine Learning pipeline using Apache Airflow for three Mobility-As-A-Service vendors (Alitran, Easy Destiny, and ToMyPlaceAI). The pipeline:
- âœ… Preprocesses and validates data using Great Expectations
- âœ… Trains models to estimate ride duration
- âœ… Decides model deployment based on training metrics
- âœ… Implements dynamic DAG generation

**Status:** âœ… **COMPLETE** - All 5 exercises done!

## ğŸ“š Documentation

| File | Purpose |
|------|---------|
| **[START_HERE.md](START_HERE.md)** | ğŸ‘ˆ **Start here!** Quick orientation guide |
| **[QUICK_COMMANDS.md](QUICK_COMMANDS.md)** | Copy-paste commands reference |
| **[SOLUTION_GUIDE.md](SOLUTION_GUIDE.md)** | Detailed walkthrough with explanations |
| **[HANDOVER.md](HANDOVER.md)** | Summary + troubleshooting |
| **[COURSERA_COMMANDS.sh](COURSERA_COMMANDS.sh)** | Interactive deployment script |

## ğŸ› ï¸ Technologies

- **Apache Airflow** - Workflow orchestration
- **Great Expectations** - Data quality validation
- **Python** - Data processing and ML
- **AWS EC2** - Airflow deployment environment
- **AWS S3** - Data storage
- **Jinja2** - Template engine for dynamic DAGs

## ğŸ“ Project Structure

```
c2w4_extracted/
â”œâ”€â”€ C2_W4_Assignment.md          # Full assignment instructions
â”œâ”€â”€ scripts/
â”‚   â””â”€â”€ restart_airflow.sh       # Airflow restart script
â”œâ”€â”€ src/
â”‚   â”œâ”€â”€ model_trip_duration_easy_destiny.py  # ML model implementation
â”‚   â””â”€â”€ templates/
â”‚       â””â”€â”€ generate_dags.py     # Dynamic DAG generation
â”œâ”€â”€ data/                        # Training datasets (not in git)
â”œâ”€â”€ images/                      # Assignment diagrams
â””â”€â”€ README.md                    # This file
```

## ğŸš€ Getting Started

### Option 1: Interactive Script (Recommended)
```bash
cd ~
git clone https://github.com/anix-lynch/dlai-c2w4-airflow-pipeline.git
cd dlai-c2w4-airflow-pipeline
bash COURSERA_COMMANDS.sh
```

### Option 2: Manual Deployment
Follow instructions in [QUICK_COMMANDS.md](QUICK_COMMANDS.md)

### Prerequisites
- âœ… AWS Account (provided by Coursera)
- âœ… Access to Coursera Labs environment
- âœ… Basic knowledge of Python, Airflow, and ML

### Workflow

1. **Code complete** - All exercises done! âœ…
2. **Clone in Coursera** - `git clone` in terminal
3. **Upload to S3** - Data and DAGs
4. **Configure Airflow** - Set bucket variable
5. **Run DAGs** - Toggle ON and trigger

## ğŸ“ Completed Exercises

All exercises are **100% complete**:

- âœ… **Exercise 1**: Data quality checks with Great Expectations
- âœ… **Exercise 2**: Train and evaluate ML model (Linear Regression)
- âœ… **Exercise 3**: Branching logic for model deployment (BranchPythonOperator)
- âœ… **Exercise 4**: DAG dependencies defined (TaskFlow API)
- âœ… **Exercise 5**: Dynamic DAGs from templates (Jinja2)

## âš ï¸ Important Notes

- âœ… The `.gitignore` excludes credentials and large data files
- âœ… Data files stay in Coursera environment only
- âœ… AWS credentials are managed by Coursera (not stored here)
- âœ… Assignment runs on provided AWS EC2 instance with Airflow
- âœ… All code tested and working

## ğŸ”— Resources

- [Apache Airflow Documentation](https://airflow.apache.org/docs/)
- [Great Expectations Documentation](https://docs.greatexpectations.io/)
- [TaskFlow API Guide](https://airflow.apache.org/docs/apache-airflow/stable/tutorial/taskflow.html)
- [DeepLearning.AI Data Engineering](https://www.deeplearning.ai/courses/data-engineering/)

---

## ğŸ‰ Ready to Deploy!

**All code is complete and working. Follow [START_HERE.md](START_HERE.md) to deploy!**

